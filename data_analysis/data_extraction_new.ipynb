{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csavelli/chatIMPACT/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub.utils import logging\n",
    "\n",
    "from tags import * # tags.py\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "default_path = \"/home/csavelli/database/HF entries/hf extracted json/\"\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?\n",
    "\n",
    "tag_language = set(tag_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)')\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri\n",
    "\n",
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')\n",
    "\n",
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>version</th>\n",
       "      <th>numberOfParameters</th>\n",
       "      <th>quantization</th>\n",
       "      <th>architecture</th>\n",
       "      <th>languages</th>\n",
       "      <th>modelCreator</th>\n",
       "      <th>licenseToUse</th>\n",
       "      <th>libraryFramework</th>\n",
       "      <th>...</th>\n",
       "      <th>uri</th>\n",
       "      <th>fineTuned</th>\n",
       "      <th>carbonEmission [CO2eq tons]</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>likes</th>\n",
       "      <th>downloads_all_time</th>\n",
       "      <th>downloads</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>age</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>all-mpnet-base-v2</td>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[sentence-transformers, pytorch, onnx, safeten...</td>\n",
       "      <td>...</td>\n",
       "      <td>1904.06472</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>903</td>\n",
       "      <td>771404860</td>\n",
       "      <td>382681384</td>\n",
       "      <td>2022-03-02 23:29:05</td>\n",
       "      <td>2.742466</td>\n",
       "      <td>30527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert-large-uncased-whole-word-masking-finetune...</td>\n",
       "      <td>google-bert/bert-large-uncased-whole-word-mask...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf, jax, safetensors]</td>\n",
       "      <td>...</td>\n",
       "      <td>1810.04805</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>172</td>\n",
       "      <td>33050939</td>\n",
       "      <td>115466</td>\n",
       "      <td>2022-03-02 23:29:04</td>\n",
       "      <td>2.742466</td>\n",
       "      <td>30522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303304</th>\n",
       "      <td>opus-mt-gl-pt</td>\n",
       "      <td>Helsinki-NLP/opus-mt-gl-pt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[gl, pt]</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>23829</td>\n",
       "      <td>14942</td>\n",
       "      <td>2022-03-02 23:29:04</td>\n",
       "      <td>2.742466</td>\n",
       "      <td>5835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303305</th>\n",
       "      <td>opus-mt-gmq-en</td>\n",
       "      <td>Helsinki-NLP/opus-mt-gmq-en</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[da, nb, sv, is, nn, fo, en]</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>200329</td>\n",
       "      <td>12278</td>\n",
       "      <td>2022-03-02 23:29:04</td>\n",
       "      <td>2.742466</td>\n",
       "      <td>57388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303306</th>\n",
       "      <td>opus-mt-gmw-gmw</td>\n",
       "      <td>Helsinki-NLP/opus-mt-gmw-gmw</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[nl, en, lb, af, de, fy, yi]</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>225652</td>\n",
       "      <td>11301</td>\n",
       "      <td>2022-03-02 23:29:04</td>\n",
       "      <td>2.742466</td>\n",
       "      <td>35464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303674</th>\n",
       "      <td>gpt2-law</td>\n",
       "      <td>naresh810/gpt2-law</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, safetensors]</td>\n",
       "      <td>...</td>\n",
       "      <td>1910.09700</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-11-27 00:53:01</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303675</th>\n",
       "      <td>Wisedom-8B-EmbeddingReordering</td>\n",
       "      <td>wisenut-nlp-team/Wisedom-8B-EmbeddingReordering</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, safetensors]</td>\n",
       "      <td>...</td>\n",
       "      <td>1910.09700</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-11-27 00:54:16</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>128256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303676</th>\n",
       "      <td>Qwen-2.5-3b-Text_to_SQL-GGUF</td>\n",
       "      <td>Ellbendls/Qwen-2.5-3b-Text_to_SQL-GGUF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Qwen/Qwen2.5-3B-Instruct]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, gguf]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>2024-11-27 00:55:35</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303677</th>\n",
       "      <td>Qwen-2.5-72B-Instruct-abliterated-v2-Q6_K.gguf</td>\n",
       "      <td>blotfaba/Qwen-2.5-72B-Instruct-abliterated-v2-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>zetasepic/Qwen2.5-72B-Instruct-abliterated-v2</td>\n",
       "      <td>[en]</td>\n",
       "      <td>zetasepic</td>\n",
       "      <td>other</td>\n",
       "      <td>[transformers, gguf]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>2024-11-27 01:10:08</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303678</th>\n",
       "      <td>llama-3-8b</td>\n",
       "      <td>dimasik87/llama-3-8b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unsloth/llama-3-8b-Instruct-bnb-4bit</td>\n",
       "      <td>[en]</td>\n",
       "      <td>unsloth</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, safetensors]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-11-27 01:13:58</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>128256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303843 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "62                                      all-mpnet-base-v2   \n",
       "7       bert-large-uncased-whole-word-masking-finetune...   \n",
       "303304                                      opus-mt-gl-pt   \n",
       "303305                                     opus-mt-gmq-en   \n",
       "303306                                    opus-mt-gmw-gmw   \n",
       "...                                                   ...   \n",
       "303674                                           gpt2-law   \n",
       "303675                     Wisedom-8B-EmbeddingReordering   \n",
       "303676                       Qwen-2.5-3b-Text_to_SQL-GGUF   \n",
       "303677     Qwen-2.5-72B-Instruct-abliterated-v2-Q6_K.gguf   \n",
       "303678                                         llama-3-8b   \n",
       "\n",
       "                                                       id version  \\\n",
       "62                sentence-transformers/all-mpnet-base-v2    None   \n",
       "7       google-bert/bert-large-uncased-whole-word-mask...    None   \n",
       "303304                         Helsinki-NLP/opus-mt-gl-pt    None   \n",
       "303305                        Helsinki-NLP/opus-mt-gmq-en    None   \n",
       "303306                       Helsinki-NLP/opus-mt-gmw-gmw    None   \n",
       "...                                                   ...     ...   \n",
       "303674                                 naresh810/gpt2-law    None   \n",
       "303675    wisenut-nlp-team/Wisedom-8B-EmbeddingReordering    None   \n",
       "303676             Ellbendls/Qwen-2.5-3b-Text_to_SQL-GGUF    None   \n",
       "303677  blotfaba/Qwen-2.5-72B-Instruct-abliterated-v2-...    None   \n",
       "303678                               dimasik87/llama-3-8b    None   \n",
       "\n",
       "       numberOfParameters quantization  \\\n",
       "62                   None         None   \n",
       "7                    None         None   \n",
       "303304               None         None   \n",
       "303305               None         None   \n",
       "303306               None         None   \n",
       "...                   ...          ...   \n",
       "303674               None         None   \n",
       "303675               None         None   \n",
       "303676               None         None   \n",
       "303677               None         None   \n",
       "303678               None         None   \n",
       "\n",
       "                                         architecture  \\\n",
       "62                                               None   \n",
       "7                                                None   \n",
       "303304                                           None   \n",
       "303305                                           None   \n",
       "303306                                           None   \n",
       "...                                               ...   \n",
       "303674                                           None   \n",
       "303675                                           None   \n",
       "303676                     [Qwen/Qwen2.5-3B-Instruct]   \n",
       "303677  zetasepic/Qwen2.5-72B-Instruct-abliterated-v2   \n",
       "303678           unsloth/llama-3-8b-Instruct-bnb-4bit   \n",
       "\n",
       "                           languages modelCreator licenseToUse  \\\n",
       "62                              [en]         None   apache-2.0   \n",
       "7                               [en]         None   apache-2.0   \n",
       "303304                      [gl, pt]         None   apache-2.0   \n",
       "303305  [da, nb, sv, is, nn, fo, en]         None   apache-2.0   \n",
       "303306  [nl, en, lb, af, de, fy, yi]         None   apache-2.0   \n",
       "...                              ...          ...          ...   \n",
       "303674                            []         None         None   \n",
       "303675                            []         None         None   \n",
       "303676                            []         None          mit   \n",
       "303677                          [en]    zetasepic        other   \n",
       "303678                          [en]      unsloth   apache-2.0   \n",
       "\n",
       "                                         libraryFramework  ...         uri  \\\n",
       "62      [sentence-transformers, pytorch, onnx, safeten...  ...  1904.06472   \n",
       "7           [transformers, pytorch, tf, jax, safetensors]  ...  1810.04805   \n",
       "303304                        [transformers, pytorch, tf]  ...        None   \n",
       "303305                        [transformers, pytorch, tf]  ...        None   \n",
       "303306                        [transformers, pytorch, tf]  ...        None   \n",
       "...                                                   ...  ...         ...   \n",
       "303674                        [transformers, safetensors]  ...  1910.09700   \n",
       "303675                        [transformers, safetensors]  ...  1910.09700   \n",
       "303676                               [transformers, gguf]  ...        None   \n",
       "303677                               [transformers, gguf]  ...        None   \n",
       "303678                        [transformers, safetensors]  ...        None   \n",
       "\n",
       "       fineTuned  carbonEmission [CO2eq tons] tokenizer likes  \\\n",
       "62          None                         None      None   903   \n",
       "7           None                         None      None   172   \n",
       "303304      None                         None      None     0   \n",
       "303305      None                         None      None     1   \n",
       "303306      None                         None      None     1   \n",
       "...          ...                          ...       ...   ...   \n",
       "303674      None                         None      None     0   \n",
       "303675      None                         None      None     0   \n",
       "303676      True                         None      None     0   \n",
       "303677      True                         None      None     0   \n",
       "303678      True                         None      None     0   \n",
       "\n",
       "       downloads_all_time  downloads        creation_date       age  \\\n",
       "62              771404860  382681384  2022-03-02 23:29:05  2.742466   \n",
       "7                33050939     115466  2022-03-02 23:29:04  2.742466   \n",
       "303304              23829      14942  2022-03-02 23:29:04  2.742466   \n",
       "303305             200329      12278  2022-03-02 23:29:04  2.742466   \n",
       "303306             225652      11301  2022-03-02 23:29:04  2.742466   \n",
       "...                   ...        ...                  ...       ...   \n",
       "303674                  7          7  2024-11-27 00:53:01  0.002740   \n",
       "303675                  2          2  2024-11-27 00:54:16  0.002740   \n",
       "303676                 18         18  2024-11-27 00:55:35  0.002740   \n",
       "303677                 26         26  2024-11-27 01:10:08  0.002740   \n",
       "303678                  2          2  2024-11-27 01:13:58  0.002740   \n",
       "\n",
       "        vocab_size  \n",
       "62           30527  \n",
       "7            30522  \n",
       "303304        5835  \n",
       "303305       57388  \n",
       "303306       35464  \n",
       "...            ...  \n",
       "303674       16000  \n",
       "303675      128256  \n",
       "303676        None  \n",
       "303677        None  \n",
       "303678      128256  \n",
       "\n",
       "[303843 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load llm json \n",
    "\n",
    "with open('/home/csavelli/chatIMPACT/database/database/HF entries/hf extracted json/models_new.json', 'r') as f:\n",
    "    llm = json.load(f)\n",
    "\n",
    "models_df = pd.DataFrame(llm)\n",
    "\n",
    "models_df.sort_values(by='age', inplace=True, ascending=False)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the model info given the id from HF \n",
    "\n",
    "id = \"Helsinki-NLP/opus-mt-gmq-en\"\n",
    "\n",
    "creation_date = api.model_info(id).created_at\n",
    "creation_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all models\n",
    "\n",
    "# models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 1000 models\n",
    "\n",
    "# model = itertools.islice(models, 0, 1000)\n",
    "# models_df = pd.DataFrame(model)\n",
    "# models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model):\n",
    "\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['id'] = model.id\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # extracted in a postprocessing step\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] \n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\tmodel_attributes['likes'] = model.likes\n",
    "\n",
    "\tinfo = api.model_info(repo_id=model.id, expand=\"downloadsAllTime\")\n",
    "\tmodel_attributes['downloads_all_time'] = info.downloads_all_time\n",
    "\n",
    "\tmodel_attributes['downloads'] = model.downloads\n",
    "\n",
    "\tmodel_attributes['creation_date'] = model.created_at.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\t# Convert both datetimes to timezone-naive\n",
    "\tstarting_datetime = pd.to_datetime(model.created_at).tz_localize(None)\n",
    "\tcurrent_datetime = pd.to_datetime('today').tz_localize(None)\n",
    "\n",
    "\t# evaluate how many years have passed since the creation \n",
    "\tmodel_attributes[\"age\"] = (current_datetime - starting_datetime).days / 365\n",
    "\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(result_path, 'models_duplicates_no_modelCreator.json')\n",
    "\n",
    "# Total: 697,162 models\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} models...')\n",
    "    models = api.list_models(filter=task, full=True, cardData=True)\n",
    "    for model in models:\n",
    "        model_attributes = extract_model_attributes(model)\n",
    "        add_to_json_file(model_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} models...')\n",
    "    models = api.list_models(filter=task, full=True, cardData=True)\n",
    "    for model in models:\n",
    "        model_attributes = extract_model_attributes(model)\n",
    "        add_to_json_file(model_attributes, \"text\")\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "with open(default_path + \"models_duplicates_no_modelCreator.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "models_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before removing duplicates: 335254\n",
      "len after removing duplicates: 303843\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(f'len before removing duplicates: {  len(models_df) }')\n",
    "models_df = models_df.loc[models_df.astype(str).drop_duplicates().index]\n",
    "print(f'len after removing duplicates: {  len(models_df) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 rows processed (1.3250649281814808 %), elapsed time: 17.53973937034607 seconds, estimated time remaining: 1306.1494179582596 seconds\n",
      "2000 rows processed (2.6501298563629616 %), elapsed time: 34.99690842628479 seconds, estimated time remaining: 1285.576477921486 seconds\n",
      "3000 rows processed (3.9751947845444424 %), elapsed time: 52.463114976882935 seconds, estimated time remaining: 1267.2990341777802 seconds\n",
      "4000 rows processed (5.300259712725923 %), elapsed time: 69.906982421875 seconds, estimated time remaining: 1249.0280847504139 seconds\n",
      "5000 rows processed (6.6253246409074045 %), elapsed time: 87.37666869163513 seconds, estimated time remaining: 1231.4518380334855 seconds\n",
      "6000 rows processed (7.950389569088885 %), elapsed time: 104.87480330467224 seconds, estimated time remaining: 1214.240489223957 seconds\n",
      "7000 rows processed (9.275454497270367 %), elapsed time: 122.38737726211548 seconds, estimated time remaining: 1197.0884395678386 seconds\n",
      "8000 rows processed (10.600519425451846 %), elapsed time: 139.8876302242279 seconds, estimated time remaining: 1179.7423375388385 seconds\n",
      "9000 rows processed (11.925584353633328 %), elapsed time: 157.3668863773346 seconds, estimated time remaining: 1162.2069185686112 seconds\n",
      "10000 rows processed (13.250649281814809 %), elapsed time: 174.8170735836029 seconds, estimated time remaining: 1144.4924282632828 seconds\n",
      "11000 rows processed (14.57571420999629 %), elapsed time: 192.34505128860474 seconds, estimated time remaining: 1127.2818948477832 seconds\n",
      "12000 rows processed (15.90077913817777 %), elapsed time: 209.83013677597046 seconds, estimated time remaining: 1109.791600974083 seconds\n",
      "13000 rows processed (17.22584406635925 %), elapsed time: 227.37095308303833 seconds, estimated time remaining: 1092.5699078035354 seconds\n",
      "14000 rows processed (18.550908994540734 %), elapsed time: 244.86945509910583 seconds, estimated time remaining: 1075.1168374751637 seconds\n",
      "15000 rows processed (19.875973922722213 %), elapsed time: 262.38776445388794 seconds, estimated time remaining: 1057.7375599109648 seconds\n",
      "16000 rows processed (21.201038850903693 %), elapsed time: 279.85948610305786 seconds, estimated time remaining: 1040.167751176536 seconds\n",
      "17000 rows processed (22.526103779085176 %), elapsed time: 297.38353204727173 seconds, estimated time remaining: 1022.7894496751112 seconds\n",
      "18000 rows processed (23.851168707266655 %), elapsed time: 314.84545063972473 seconds, estimated time remaining: 1005.196581498623 seconds\n",
      "19000 rows processed (25.176233635448135 %), elapsed time: 332.3013873100281 seconds, estimated time remaining: 987.5997266283035 seconds\n",
      "20000 rows processed (26.501298563629618 %), elapsed time: 349.761741399765 seconds, estimated time remaining: 970.0292169042586 seconds\n",
      "21000 rows processed (27.8263634918111 %), elapsed time: 367.18311858177185 seconds, estimated time remaining: 952.3681032306126 seconds\n",
      "22000 rows processed (29.15142841999258 %), elapsed time: 384.61825609207153 seconds, estimated time remaining: 934.7622263849865 seconds\n",
      "23000 rows processed (30.47649334817406 %), elapsed time: 402.09678649902344 seconds, estimated time remaining: 917.2701867002405 seconds\n",
      "24000 rows processed (31.80155827635554 %), elapsed time: 419.57384991645813 seconds, estimated time remaining: 899.7761252361537 seconds\n",
      "25000 rows processed (33.12662320453702 %), elapsed time: 437.07960081100464 seconds, estimated time remaining: 882.3413346369935 seconds\n",
      "26000 rows processed (34.4516881327185 %), elapsed time: 454.5445201396942 seconds, estimated time remaining: 864.823399278494 seconds\n",
      "27000 rows processed (35.776753060899985 %), elapsed time: 472.065349817276 seconds, estimated time remaining: 847.4097571954728 seconds\n",
      "28000 rows processed (37.10181798908147 %), elapsed time: 489.5736999511719 seconds, estimated time remaining: 829.9673016381604 seconds\n",
      "29000 rows processed (38.426882917262944 %), elapsed time: 507.0854344367981 seconds, estimated time remaining: 812.5257256883094 seconds\n",
      "30000 rows processed (39.75194784544443 %), elapsed time: 524.5673418045044 seconds, estimated time remaining: 795.0342657683371 seconds\n",
      "31000 rows processed (41.07701277362591 %), elapsed time: 542.0675756931305 seconds, estimated time remaining: 777.5697106495519 seconds\n",
      "32000 rows processed (42.402077701807386 %), elapsed time: 559.5526895523071 seconds, estimated time remaining: 760.0823859660626 seconds\n",
      "33000 rows processed (43.72714262998887 %), elapsed time: 577.0402591228485 seconds, estimated time remaining: 742.5983571266695 seconds\n",
      "34000 rows processed (45.05220755817035 %), elapsed time: 594.5492265224457 seconds, estimated time remaining: 725.1402166170514 seconds\n",
      "35000 rows processed (46.377272486351835 %), elapsed time: 612.0351588726044 seconds, estimated time remaining: 707.6525390613283 seconds\n",
      "36000 rows processed (47.70233741453331 %), elapsed time: 629.5246596336365 seconds, estimated time remaining: 690.1688695572217 seconds\n",
      "37000 rows processed (49.027402342714794 %), elapsed time: 646.9750866889954 seconds, estimated time remaining: 672.6442618859782 seconds\n",
      "38000 rows processed (50.35246727089627 %), elapsed time: 664.454000711441 seconds, estimated time remaining: 655.1516461119651 seconds\n",
      "39000 rows processed (51.67753219907776 %), elapsed time: 681.9229378700256 seconds, estimated time remaining: 637.6504040796573 seconds\n",
      "40000 rows processed (53.002597127259236 %), elapsed time: 699.3929851055145 seconds, estimated time remaining: 620.1517607386827 seconds\n",
      "41000 rows processed (54.32766205544072 %), elapsed time: 716.8481931686401 seconds, estimated time remaining: 602.6420379270228 seconds\n",
      "42000 rows processed (55.6527269836222 %), elapsed time: 734.3302478790283 seconds, estimated time remaining: 585.1563045683589 seconds\n",
      "43000 rows processed (56.97779191180368 %), elapsed time: 751.7823467254639 seconds, estimated time remaining: 567.6481226090054 seconds\n",
      "44000 rows processed (58.30285683998516 %), elapsed time: 769.2980198860168 seconds, estimated time remaining: 550.1879576088298 seconds\n",
      "45000 rows processed (59.62792176816664 %), elapsed time: 786.8147571086884 seconds, estimated time remaining: 532.7260455254237 seconds\n",
      "46000 rows processed (60.95298669634812 %), elapsed time: 804.3011615276337 seconds, estimated time remaining: 515.242318914144 seconds\n",
      "47000 rows processed (62.27805162452961 %), elapsed time: 821.8421154022217 seconds, estimated time remaining: 497.791518765835 seconds\n",
      "48000 rows processed (63.60311655271108 %), elapsed time: 839.3450498580933 seconds, estimated time remaining: 480.31520546346906 seconds\n",
      "49000 rows processed (64.92818148089256 %), elapsed time: 856.8133232593536 seconds, estimated time remaining: 462.81908322227247 seconds\n",
      "50000 rows processed (66.25324640907404 %), elapsed time: 874.2805118560791 seconds, estimated time remaining: 445.32352224765776 seconds\n",
      "51000 rows processed (67.57831133725553 %), elapsed time: 891.6973867416382 seconds, estimated time remaining: 427.804934943704 seconds\n",
      "52000 rows processed (68.903376265437 %), elapsed time: 909.1499528884888 seconds, estimated time remaining: 410.30636784535193 seconds\n",
      "53000 rows processed (70.22844119361848 %), elapsed time: 926.5479307174683 seconds, estimated time remaining: 392.7863950849209 seconds\n",
      "54000 rows processed (71.55350612179997 %), elapsed time: 943.9690186977386 seconds, estimated time remaining: 375.2801283191045 seconds\n",
      "55000 rows processed (72.87857104998145 %), elapsed time: 961.4017186164856 seconds, estimated time remaining: 357.7812801076716 seconds\n",
      "56000 rows processed (74.20363597816294 %), elapsed time: 978.8181567192078 seconds, estimated time remaining: 340.2791410395929 seconds\n",
      "57000 rows processed (75.52870090634441 %), elapsed time: 996.270574092865 seconds, estimated time remaining: 322.79166639232636 seconds\n",
      "58000 rows processed (76.85376583452589 %), elapsed time: 1013.7243778705597 seconds, estimated time remaining: 305.3058181632141 seconds\n",
      "59000 rows processed (78.17883076270738 %), elapsed time: 1031.137690782547 seconds, estimated time remaining: 287.8097544977059 seconds\n",
      "60000 rows processed (79.50389569088885 %), elapsed time: 1048.5637929439545 seconds, estimated time remaining: 270.3197461897373 seconds\n",
      "61000 rows processed (80.82896061907033 %), elapsed time: 1065.9948778152466 seconds, estimated time remaining: 252.8330148529303 seconds\n",
      "62000 rows processed (82.15402554725182 %), elapsed time: 1083.4368300437927 seconds, estimated time remaining: 235.3504395081766 seconds\n",
      "63000 rows processed (83.4790904754333 %), elapsed time: 1100.8574843406677 seconds, estimated time remaining: 217.86493861262005 seconds\n",
      "64000 rows processed (84.80415540361477 %), elapsed time: 1118.3326017856598 seconds, estimated time remaining: 200.39122333879772 seconds\n",
      "65000 rows processed (86.12922033179626 %), elapsed time: 1135.8003442287445 seconds, estimated time remaining: 182.91627716715888 seconds\n",
      "66000 rows processed (87.45428525997774 %), elapsed time: 1153.267250776291 seconds, estimated time remaining: 165.44142941873724 seconds\n",
      "67000 rows processed (88.77935018815923 %), elapsed time: 1170.7389914989471 seconds, estimated time remaining: 147.96742976336694 seconds\n",
      "68000 rows processed (90.1044151163407 %), elapsed time: 1188.2022302150726 seconds, estimated time remaining: 130.49256273425326 seconds\n",
      "69000 rows processed (91.42948004452218 %), elapsed time: 1205.7466759681702 seconds, estimated time remaining: 113.0256450416316 seconds\n",
      "70000 rows processed (92.75454497270367 %), elapsed time: 1223.2672944068909 seconds, estimated time remaining: 95.5546510706084 seconds\n",
      "71000 rows processed (94.07960990088515 %), elapsed time: 1240.7246611118317 seconds, estimated time remaining: 78.0782787939394 seconds\n",
      "72000 rows processed (95.40467482906662 %), elapsed time: 1258.2264139652252 seconds, estimated time remaining: 60.60457236452898 seconds\n",
      "73000 rows processed (96.7297397572481 %), elapsed time: 1275.7420935630798 seconds, estimated time remaining: 43.130568362249086 seconds\n",
      "74000 rows processed (98.05480468542959 %), elapsed time: 1293.2590715885162 seconds, estimated time remaining: 25.655463772863953 seconds\n",
      "75000 rows processed (99.37986961361108 %), elapsed time: 1310.761152267456 seconds, estimated time remaining: 8.179149596099853 seconds\n"
     ]
    }
   ],
   "source": [
    "# Postprocessing: find the modelCreator\n",
    "\n",
    "df_filtered = models_df[models_df['architecture'].notna()]\n",
    "\n",
    "# Process each row\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for index, row in df_filtered.iterrows():\n",
    "    # Find the row where 'id' matches the 'architecture' of the current row\n",
    "    try:\n",
    "        matching_row = models_df[models_df['id'].astype(str) == str(row['architecture'])]\n",
    "    except ValueError:\n",
    "        break\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        # Get the first developer from the 'developers' list\n",
    "        first_developer = matching_row['developers'].iloc[0][0] if matching_row['developers'].iloc[0] else None\n",
    "        # Set the 'modelCreator' attribute of the original row\n",
    "        models_df.at[index, 'modelCreator'] = first_developer\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} rows processed ({count/len(df_filtered)*100} %), elapsed time: {time.time() - start_time} seconds, estimated time remaining: {(time.time() - start_time) / count * (len(df_filtered) - count)} seconds')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = models_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join('/home/csavelli/chatIMPACT/database/database/HF entries/hf extracted json/models.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "    json.dump(models_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all datasets\n",
    "\n",
    "# datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 1000 models\n",
    "\n",
    "# datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "# datasets_df = pd.DataFrame(datasets)\n",
    "# datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset):\n",
    "\n",
    "\tdataset_tags = dataset.tags\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset.id)\n",
    "\tdataset_attributes['size [GB]'] = match_size(dataset_tags)\n",
    "\n",
    "\t# url = \"https://huggingface.co/datasets/\" + dataset.id\n",
    "\t# file_size_str = extract_file_size(url)\n",
    "\t# if file_size_str:\n",
    "\t# \tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t# \tif file_size_gb:\n",
    "\t# \t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(result_path, 'datasets_duplicates_new.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} datasets...')\n",
    "    datasets = api.list_datasets(task_categories=task, full=True)\n",
    "    for dataset in datasets:\n",
    "        dataset_attributes = extract_datasets_attributes(dataset)\n",
    "        add_to_json_file(dataset_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} datasets processed, {time.time() - start_time} seconds elapsed, estimated time remaining: {(time.time() - start_time) / count:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'datasets_duplicates_new.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "datasets_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(f'len before removing duplicates: {len(datasets_df)}')\n",
    "datasets_df = datasets_df.loc[datasets_df.astype(str).drop_duplicates().index]\n",
    "print(f'len after removing duplicates: {len(datasets_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = datasets_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'datasets.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "    json.dump(datasets_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description,\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "    \n",
    "    file_path = os.path.join(result_path, 'downstreamtasks.json')\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tasks_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "def extract_metrics():\n",
    "\tmetrics = api.list_metrics()\n",
    "\n",
    "\tmetrics_names = [metric.id for metric in metrics]\n",
    "\tmetrics_descriptions = [metric.description for metric in metrics]\n",
    "\n",
    "\t# url_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "\t# # Remove from the list the metrics withoud description (not useful for our purpose)\n",
    "\t# metrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "\t# metrics.remove('Aye10032/loss_metric')\n",
    "\t# metrics.remove('giulio98/code_eval_outputs')\n",
    "\t# metrics.remove('maysonma/lingo_judge_metric')\n",
    "\t# metrics.remove('lvwerra/test')\n",
    "\t# metrics.remove('sma2023/wil')\n",
    "\n",
    "\t# From the lists, replace the description 'TODO: add a description here' with None\n",
    "\n",
    "\tfor i, description in enumerate(metrics_descriptions):\n",
    "\t\tif type(description) is not str or 'TODO: add a description here' in description:\n",
    "\t\t\tmetrics_descriptions[i] = None\n",
    "\t\n",
    "\treturn metrics_names, metrics_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json():\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    metrics, descriptions = extract_metrics()\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['trained'] = None\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "        metrics_data.append(metric_attributes)\n",
    "    \n",
    "    file_path = os.path.join(result_path, 'metrics.json')\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship():\n",
    "\n",
    "    file_path = os.path.join(result_path, 'train_duplicates.json')\n",
    "\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        print(f'Processing {task} models...')\n",
    "        models = api.list_models(filter=task, full=True)\n",
    "        for model in models:\n",
    "            model_tags = model.tags\n",
    "            datasets = match_dataset(model_tags)\n",
    "            if len(datasets) != 0:\n",
    "                train_relationship = dict()\n",
    "                train_relationship[\"Models\"] = extract_name(model.id)\n",
    "                train_relationship[\"Datasets\"] = [extract_name(dataset) for dataset in datasets]\n",
    "                add_to_json_file(train_relationship, file_path)\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'train_duplicates.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "\tdata = json.load(file)\n",
    "train_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "print(f'len before removing duplicates: {len(train_df)}')\n",
    "train_df = train_df.loc[train_df.astype(str).drop_duplicates().index]\n",
    "print(f'len after removing duplicates: {len(train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'train.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "\tjson.dump(train_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship():\n",
    "\n",
    "    file_path = os.path.join(result_path, 'suited_for_duplicates.json')\n",
    "\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        print(f'Processing {task} models...')\n",
    "        models = api.list_models(filter=task, full=True)\n",
    "        for model in models:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model.id)\n",
    "            suited_for_relationship['DownstreamTask'] = task\n",
    "            add_to_json_file(suited_for_relationship, file_path)\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'suited_for_duplicates.json')\n",
    "with open(file_path, 'r') as file:\n",
    "\tdata = json.load(file)\n",
    "suited_for_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge duplicates\n",
    "\n",
    "print(f'len before removing duplicates: {len(suited_for_df)}')\n",
    "suited_for_df = suited_for_df.groupby('LargeLanguageModel')['DownstreamTask'].apply(list).reset_index()\n",
    "print(f'len after removing duplicates: {len(suited_for_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'suited_for.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "\tjson.dump(suited_for_df.to_dict(orient='records'), json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship():\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'enable_duplicates.json')\n",
    "\n",
    "\tcount = 0\n",
    "\tstart_time = time.time()\n",
    "\tfor task in TAG_DOWNSTREAM_TASK:\n",
    "\t\tprint(f'Processing {task} datasets...')\n",
    "\t\tdatasets = api.list_datasets(filter=task, full=True)\n",
    "\t\tfor dataset in datasets:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset.id)\n",
    "\t\t\tenable_relationship['DownstreamTask'] = task\n",
    "\t\t\tadd_to_json_file(enable_relationship, file_path)\n",
    "\t\t\tcount += 1\n",
    "\t\t\tif count % 1000 == 0:\n",
    "\t\t\t\tprint(f'{count} datasets processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enable_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'enable_duplicates.json')\n",
    "with open(file_path, 'r') as file:\n",
    "\tdata = json.load(file)\n",
    "enable_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge duplicates\n",
    "\n",
    "print(f'len before removing duplicates: {len(enable_df)}')\n",
    "enable_df = enable_df.groupby('Dataset')['DownstreamTask'].apply(list).reset_index()\n",
    "print(f'len after removing duplicates: {len(enable_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'enable.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "\tjson.dump(enable_df.to_dict(orient='records'), json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "\n",
    "    return assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'assess.json')\n",
    "\n",
    "\twith open(file_path, 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def create_evaluate_relationship():\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'evaluate_duplicates.json')\n",
    "\n",
    "\tcount = 0\n",
    "\tstart_time = time.time()\n",
    "\tfor task in TAG_DOWNSTREAM_TASK:\n",
    "\t\tprint(f'Processing {task} models...')\n",
    "\t\tmodels = api.list_models(filter=task, full=True, cardData=True)\n",
    "\t\tfor model in models:\n",
    "\t\t\tif model.card_data is not None:\n",
    "\t\t\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\t\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\t\tevaluate_relationship = dict()\n",
    "\t\t\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model.id)\n",
    "\t\t\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\t\t\tadd_to_json_file(evaluate_relationship, file_path)\n",
    "\t\t\tcount += 1\n",
    "\t\t\tif count % 10000 == 0:\n",
    "\t\t\t\tprint(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'evaluate_duplicates.json')\n",
    "with open(file_path, 'r') as file:\n",
    "\tdata = json.load(file)\n",
    "evaluate_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "print(f'len before removing duplicates: {len(evaluate_df)}')\n",
    "evaluate_df = evaluate_df.loc[evaluate_df.astype(str).drop_duplicates().index]\n",
    "print(f'len after removing duplicates: {len(evaluate_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_list = evaluate_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'evaluate.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "\tjson.dump(evaluate_list, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub.utils import logging\n",
    "\n",
    "from tags import * # tags.py\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = itertools.islice(models, 0, 1000)\n",
    "models_df = pd.DataFrame(model)\n",
    "models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag examples\n",
    "models_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# richer tag information example\n",
    "example_model = api.list_models(model_name='albert_bert_summarization_cnn_dailymail')\n",
    "example_df = pd.DataFrame(example_model)\n",
    "example_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'zh', 'fr', 'es', 'ru', 'de', 'ja', 'pt', 'ko', 'ar', 'it', 'vi', 'tr', 'hi', 'id', 'pl', 'nl', 'th', 'cs', 'bn', 'fa', 'sv', 'ro', 'ca', 'fi', 'ta', 'da', 'hu', 'uk', 'ind', 'el', 'te', 'ur', 'bg', 'he', 'ml', 'ms', 'sl', 'mr', 'sw', 'sk', 'et', 'eu', 'kn', 'gu', 'sr', 'hr', 'no', 'lt', 'lv', 'pa', 'is', 'yo', 'am', 'vie', 'ne', 'az', 'af', 'ga', 'mt', 'si', 'gl', 'sq', 'or', 'kk', 'cy', 'tl', 'ceb', 'tha', 'as', 'mk', 'ha', 'hy', 'ka', 'my', 'uz', 'eng', 'ig', 'eo', 'be', 'nb', 'km', 'mn', 'ky', 'la', 'so', 'zu', 'min', 'jav', 'xh', 'ps', 'nn', 'rw', 'jv', 'yue', 'mya', 'br', 'tt', 'bs', 'ckb', 'lg', 'sa', 'wo', 'lo', 'ku', 'ug', 'sd', 'ilo', 'tw', 'ast', 'sun', 'tg', 'ace', 'lb', 'nso', 'gd', 'war', 'fil', 'oc', 'su', 'tk', 'fy', 'bug', 'tgl', 'gn', 'bjn', 'khm', 'sn', 'yi', 'ht', 'mai', 'ba', 'bo', 'tn', 'zlm', 'ban', 'fo', 'dv', 'kab', 'ln', 'bm', 'hin', 'ny', 'cv', 'ti', 'aa', 'shn', 'mg', 'sat', 'mi', 'arz', 'sah', 'lao', 'st', 'ee', 'mar', 'ia', 'pag', 'qu', 'hsb', 'ab', 'azb', 'mhr', 'vec', 'lij', 'mni', 'por', 'ak', 'om', 'sc', 'als', 'ts', 'tpi', 'crh', 'li', 'scn', 'tam', 'rn', 'rm', 'nds', 'spa', 'mad', 'fur', 'fra', 'tet', 'pam', 'luo', 'rus', 'pap', 'ltg', 'fon', 'szl', 'kmr', 'lmo', 'ch', 'hau', 'swh', 'sm', 'ben', 'ary', 'cmn', 'jpn', 'ita', 'deu', 'ss', 'ks', 'io', 'urd', 'mal', 'tum', 'os', 'an', 'nld', 'myv', 'pcm', 'yor', 'gom', 'kan', 'guj', 'bho', 'pes', 'pan', 'kor', 'amh', 'pol', 'wa', 'cbk', 'ron', 'swe', 'kg', 'srp', 'hun', 'kam', 'dan', 'zsm', 'dz', 'haw', 'lug', 'tel', 'ki', 'kin', 'kbp', 'ce', 'npi', 'afr', 'vo', 'udm', 'kw', 'kea', 'xho', 'ory', 'lit', 'ibo', 'pnb', 'bcl', 'tur', 'lus', 'wuu', 'fj', 'jbo', 'umb', 'dyu', 'som', 'ukr', 'mrj', 'awa', 'nya', 'quy', 'krc', 'ie', 'co', 'mdf', 'heb', 'zul', 'zho', 'ces', 'pms', 'tdt', 'bem', 'mos', 'cnh', 'sg', 'fin', 'sna', 'ay', 'csb', 'to', 'sco', 'kbd', 'tyv', 'nap', 'hrv', 'bar', 'dsb', 'ang', 've', 'zgh', 'tgk', 'gv', 'new', 'bxr', 'azj', 'hil', 'ell', 'wol', 'nan', 'bul', 'sot', 'tzm', 'kac', 'bel', 'bh', 'vep', 'kir', 'cym', 'av', 'mzn', 'rue', 'srn', 'xmf', 'cat', 'gle', 'asm', 'hne', 'bpy', 'ff', 'slk', 'isl', 'hak', 'glk', 'ara', 'hat', 'gor', 'vls', 'slv', 'mkd', 'plt', 'taq', 'prs', 'jam', 'chr', 'se', 'diq', 'dik', 'ksh', 'kaa', 'xal', 'frr', 'glg', 'mwl', 'lin', 'mnw', 'hye', 'san', 'lad', 'nob', 'lua', 'lez', 'brx', 'lzh', 'kat', 'rmy', 'mlt', 'koi', 'lvs', 'snd', 'pbt', 'bi', 'ltz', 'mri', 'got', 'lfn', 'guc', 'skr', 'ydd', 'epo', 'orm', 'apc', 'kaz', 'twi', 'kv', 'gaz', 'bos', 'ext', 'mag', 'nv', 'arb', 'kmb', 'rup', 'msa', 'nov', 'bbc', 'cjk', 'ayr', 'quc', 'gan', 'frp', 'pcd', 'fas', 'uzn', 'kl', 'fuv', 'gag', 'tsn', 'pdc', 'oci', 'gsw', 'mak', 'aeb', 'lrc', 'zea', 'knc', 'gla', 'tso', 'est', 'hif', 'dty', 'ty', 'lld', 'eus', 'kek', 'cdo', 'khk', 'nus', 'iu', 'ars', 'acq', 'arn', 'kik', 'cak', 'mam', 'uzb', 'stq', 'inh', 'acm', 'grc', 'cu', 'olo', 'tok', 'swa', 'za', 'bew', 'cho', 'pfl', 'sqi', 'lat', 'uig', 'ewe', 'shi', 'msb', 'nij', 'nno', 'aka', 'vot', 'mus', 'bam', 'lbe', 'vro', 'bas', 'sin', 'pus', 'ady', 'dtp', 'mqj', 'kur', 'cgc', 'dag', 'tig', 'chy', 'nia', 'nep', 'ae', 'tcy', 'sgs', 'bod', 'din', 'ksw', 'taj', 'nhi', 'llg', 'mon', 'pi', 'pih', 'atj', 'arq', 'cbr', 'cr', 'aii', 'hus', 'mh', 'ik', 'pnt', 'qub', 'kas', 'mkn', 'yid', 'quz', 'tir', 'lav', 'nyu', 'jiv', 'ven', 'bre', 'bkx', 'myk', 'hyw', 'agr', 'nyn', 'ii', 'fuh', 'ote', 'quh', 'nrf', 'kmu', 'tca', 'jra', 'fuf', 'smo', 'rug', 'nas', 'fat', 'ng', 'cni', 'sxb', 'rej', 'ori', 'fao', 'acu', 'bef', 'ame', 'ton', 'ho', 'ami', 'djk', 'chd', 'amu', 'chk', 'maz', 'kok', 'kbq', 'tzo', 'bzi', 'mui', 'hla', 'alt', 'nqo', 'ssw', 'mic', 'cab', 'top', 'shp', 'pon', 'toj', 'sps', 'cbs', 'boa', 'yap', 'kr', 'zam', 'tbz', 'hnj', 'hlt', 'snn', 'snk', 'mle', 'mlg', 'kde', 'krl', 'nfa', 'arl', 'kyq', 'nwi', 'cbi', 'mcd', 'ahk', 'tat', 'kmg', 'sme', 'ino', 'qvh', 'quf', 'aze', 'hmn', 'bjr', 'nor', 'mbb', 'tnp', 'kne', 'pwg', 'srm', 'lbk', 'hch', 'cop', 'kms', 'nsn', 'csy', 'bis', 'ful', 'qvm', 'pmf', 'wbp', 'kj', 'cha', 'sml', 'omw', 'qxn', 'tnn', 'sag', 'hvn', 'wal', 'tuk', 'roo', 'ctu', 'xog', 'ngu', 'bpr', 'tod', 'mbs', 'ese', 'zaw', 'enm', 'aia', 'doi', 'att', 'smk', 'agt', 'meu', 'cmo', 'ach', 'bzd', 'hbo', 'gyr', 'obo', 'cof', 'trv', 'nhe', 'msm', 'qvc', 'alp', 'na', 'teo', 'gym', 'ixl', 'heg', 'blz', 'fry', 'nuj', 'mbt', 'myu', 'agn', 'vmw', 'cbu', 'atd', 'kri', 'dgc', 'kmk', 'bba', 'cbt', 'huu', 'blw', 'lhu', 'dje', 'iba', 'bps', 'msk', 'lex', 'ada', 'ptu', 'bss', 'qvn', 'lsi', 'nch', 'wrs', 'bgs', 'auc', 'ken', 'yad', 'qve', 'bkd', 'atb', 'tlh', 'sus', 'not', 'rmc', 'hns', 'yva', 'anp', 'awb', 'kwd', 'ded', 'sey', 'mgh', 'lgg', 'qul', 'cot', 'smn', 'yml', 'qvw', 'und', 'ddg', 'con', 'kak', 'liv', 'mhx', 'crx', 'mph', 'amr', 'cao', 'bdd', 'nys', 'car', 'kje', 'tcz', 'pls', 'gvc', 'abx', 'myx', 'bbj', 'abt', 'yao', 'gul', 'gum', 'fij', 'bhg', 'suz', 'acf', 'bqc', 'tzj', 'qvs', 'mto', 'gaa', 'aoj', 'acr', 'kpx', 'usp', 'srq', 'clu', 'cub', 'pib', 'hbs', 'nod', 'ura', 'ksd', 'ncj', 'dzo', 'nhw', 'zai', 'yle', 'hui', 'knv', 'mau', 'bvr', 'tbl', 'hmo', 'aoi', 'bzj', 'ikk', 'prf', 'ata', 'bgc', 'rop', 'seh', 'pis', 'amp', 'alq', 'mps', 'piu', 'nbl', 'poi', 'nuy', 'run', 'dhg', 'mpm', 'yaq', 'mgm', 'gcr', 'ake', 'snc', 'aaz', 'cuk', 'myw', 'laj', 'bon', 'poh', 'mwf', 'bmu', 'zyp', 'gun', 'yka', 'udu', 'kwi', 'dob', 'opm', 'tuc', 'apu', 'ruf', 'bcc', 'amk', 'kos', 'mxb', 'kqc', 'zap', 'azz', 'nhg', 'csw', 'mcf', 'hub', 'nst', 'kbh', 'dgz', 'yaa', 'gwi', 'sgb', 'brv', 'kau', 'qwh', 'tbg', 'mjc', 'rif', 'ebk', 'upv', 'zpm', 'fro', 'sea', 'crn', 'aau', 'bal', 'ncu', 'cax', 'fai', 'gui', 'gfk', 'gvf', 'tku', 'etr', 'yet', 'mek', 'crs', 'dwr', 'dww', 'kdl', 'ghs', 'wed', 'snl', 'pwn', 'emp', 'yua', 'agu', 'sim', 'ttc', 'zia', 'khz', 'bus', 'avt', 'tkd', 'xla', 'zpq', 'cta', 'kgk', 'bsn', 'loz', 'kwf', 'lcm', 'apb', 'sgd', 'adz', 'buk', 'esk', 'swp', 'tbo', 'ots', 'tpu', 'tos', 'qxh', 'yal', 'sny', 'mie', 'mpj', 'maa', 'pad', 'geb', 'zac', 'zpu', 'mbc', 'pce', 'cfm', 'kup', 'ape', 'ssx', 'gah', 'sue', 'spp', 'myy', 'djr', 'kbm', 'snp', 'ubr', 'tgp', 'bch', 'sid', 'ign', 'tke', 'bla', 'xbi', 'cap', 'kqf', 'zpo', 'tna', 'yon', 'nlg', 'tfr', 'bjp', 'yss', 'cac', 'kpw', 'tlf', 'bxh', 'ksr', 'xtm', 'agd', 'krr', 'arp', 'wnc', 'mca', 'poy', 'kyc', 'gur', 'wms', 'anv', 'ziw', 'kqw', 'wsk', 'yuj', 'lwl', 'big', 'med', 'rwo', 'jac', 'wuv', 'bsp', 'mfe', 'kyz', 'bnp', 'caf', 'gvn', 'wmw', 'kjh', 'nii', 'gaw', 'guw', 'kaq', 'gai', 'apz', 'tnk', 'nxa', 'kkc', 'cwe', 'ian', 'dop', 'tue', 'tvk', 'azg', 'nbq', 'gdn', 'amm', 'mkz', 'msy', 'ffm', 'kkl', 'kmh', 'mwp', 'soq', 'tnc', 'cav', 'gnn', 'dnw', 'kdc', 'mdr', 'gng', 'brb', 'mti', 'pjt', 'ppo', 'niu', 'dmg', 'hot', 'kha', 'amf', 'zaj', 'ntu', 'xtd', 'wbi', 'zad', 'cjv', 'lgl', 'pau', 'aom', 'kjs', 'nak', 'tcs', 'ewo', 'npl', 'tte', 'miq', 'tuf', 'mvn', 'viv', 'gmv', 'for', 'tbc', 'apr', 'rro', 'dad', 'mpx', 'nrm', 'mdy', 'xav', 'ood', 'kew', 'hto', 'wmt', 'tgo', 'sbk', 'ong', 'leu', 'ktm', 'dgr', 'gup', 'bhl', 'kjb', 'kbc', 'xmm', 'row', 'zos', 'lac', 'pma', 'mcq', 'mwe', 'khb', 'kcg', 'abs', 'bbr', 'kpg', 'ntp', 'caa', 'plu', 'mva', 'sbe', 'tiy', 'kvg', 'nna', 'mco', 'mbh', 'kwj', 'mxt', 'poe', 'sja', 'beu', 'kpr', 'chq', 'spm', 'aln', 'dga', 'tif', 'aey', 'nin', 'ikw', 'gdr', 'tmd', 'trc', 'okv', 'nhr', 'usa', 'pao', 'faa', 'orv', 'kpj', 'bbb', 'zao', 'mmx', 'sab', 'enq', 'lif', 'aly', 'gub', 'wat', 'gux', 'kgp', 'zav', 'iws', 'avk', 'txu', 'too', 'xon', 'tay', 'wim', 'wap', 'mwc', 'aui', 'bya', 'klv', 'cpu', 'ubu', 'auy', 'aso', 'oji', 'guz', 'cut', 'rgu', 'cya', 'jic', 'nop', 'kud', 'blk', 'bmk', 'xed', 'gil', 'sbs', 'zar', 'sll', 'awx', 'gnw', 'gvs', 'pir', 'tew', 'wnu', 'urt', 'jni', 'tbf', 'bgt', 'lid', 'ncl', 'zca', 'wiu', 'uvh', 'kyg', 'zpl', 'ntj', 'waj', 'kal', 'maj', 'roh', 'eve', 'tpa', 'tah', 'kyf', 'fue', 'otm', 'yre', 'vmy', 'bax', 'kql', 'cuc', 'mig', 'agq', 'gjn', 'imo', 'hre', 'cjo', 'mlh', 'nab', 'btx', 'kpf', 'txq', 'mzz', 'nca', 'mcp', 'ido', 'tar', 'mbj', 'ota', 'gcf', 'nho', 'fub', 'mgw', 'emi', 'eri', 'cek', 'daa', 'nif', 'mop', 'ina', 'uri', 'yuw', 'wos', 'ctp', 'tly', 'ssd', 'tee', 'mux', 'msc', 'oki', 'mih', 'kze', 'inb', 'gug', 'nnq', 'ipi', 'far', 'lns', 'aer', 'gof', 'mmo', 'zab', 'bmr', 'nhu', 'bkq', 'mkl', 'bea', 'agm', 'qup', 'tsw', 'klt', 'apw', 'mna', 'naf', 'jvn', 'kiw', 'ter', 'ksj', 'mib', 'mbl', 'toc', 'mpt', 'mox', 'guo', 'nav', 'moh', 'mxp', 'jae', 'bao', 'lbb', 'quw', 'knj', 'bzh', 'cco', 'aon', 'tiw', 'guh', 'bjk', 'eko', 'nyo', 'cnr', 'ekk', 'cme', 'mcr', 'zpz', 'wbm', 'beo', 'zpv', 'hop', 'jid', 'nko', 'bco', 'stp', 'kqa', 'zpc', 'otq', 'mpp', 'chf', 'rai', 'zas', 'cpb', 'mir', 'pot', 'nou', 'kpv', 'yrb', 'tav', 'kue', 'srd', 'qug', 'mav', 'agg', 'ndj', 'fli', 'box', 'cso', 'muy', 'khs', 'jao', 'sgz', 'ann', 'spl', 'cpa', 'zza', 'ssg', 'cbv', 'zat', 'ngp', 'mil', 'yut', 'bjv', 'lww', 'wrk', 'rmn', 'wln', 'iou', 'cnl', 'pio', 'cle', 'tsz', 'tac', 'are', 'aby', 'huv', 'bmh', 'kmo', 'nhy', 'nss', 'lim', 'bvd', 'nde', 'ccp', 'tuo', 'lue', 'kto', 'spy', 'aai', 'mks', 'dah', 'pab', 'cux', 'hix', 'ycn', 'oss', 'urb', 'shj', 'uvl', 'pah', 'cpc', 'taw', 'ajz', 'toi', 'hni', 'apn', 'pri', 'noa', 'cnt', 'pdu', 'qvi', 'knf', 'bak', 'amn', 'thd', 'bjz', 'yby', 'mit', 'tom', 'tof', 'mxq', 'gam', 'byr', 'sua', 'mgc', 'byx', 'xnn', 'pbb', 'boj', 'mio', 'zaa', 'met', 'wro', 'cbc', 'ptp', 'hsn', 'qxo', 'glv', 'cui', 'kvn', 'amo', 'ctd', 'ztq', 'tnt', 'rkb', 'rom', 'maq', 'kkh', 'ybh', 'ybb', 'ckt', 'tim', 'zdj', 'sms', 'mah', 'blt', 'non', 'gbm', 'stk', 'sri', 'boz', 'prg', 'dwy', 'duu', 'suk', 'loh', 'gwr', 'mfy', 'aoz', 'bkm', 'swg', 'abk', 'lmp', 'rmq', 'chp', 'aak', 'chz', 'kiu', 'syc', 'nmw', 'fkv', 'nhx', 'saq', 'bze', 'vif', 'swc', 'ndo', 'szy', 'bri', 'chv', 'bag', 'ivv', 'koo', 'amx', 'tvl', 'mcb', 'nvm', 'cpy', 'urw', 'bfz', 'ktu', 'tpz', 'lun', 'isn', 'miz', 'ons', 'vid', 'qvz', 'nzi', 'hbb', 'srr', 'bhd', 'yom', 'grn', 'meq', 'xsm', 'tbj', 'kby', 'hig', 'men', 'ses', 'ozm', 'ppl', 'ile', 'kgf', 'tpt', 'ivb', 'swb', 'nr', 'zty', 'mqb', 'sdh', 'atg', 'soy', 'thk', 'vol', 'xsi', 'nhn', 'div', 'mee', 'cos', 'naq', 'sma', 'unr', 'che', 'jmx', 'ase', 'tuz', 'kqn', 'mlp', 'hna', 'nog', 'bqp', 'clo', 'mup', 'yin', 'kon', 'thl', 'tuv', 'ojb', 'wer', 'keo', 'mfj', 'kfm', 'mfq', 'lgr', 'apy', 'lia', 'wbr', 'lsm', 'yrk', 'shk', 'mxv', 'alz', 'gbi', 'brh', 'arg', 'kbo', 'pex', 'sas', 'adq', 'bxa', 'vai', 'sxn', 'the', 'gos', 'mas', 'bts', 'hup', 'pov', 'mot', 'rel', 'oj', 'ktz', 'bob', 'tnl', 'mve', 'nza', 'bbk', 'kbx', 'isu', 'wes', 'kua', 'dua', 'lan', 'kij', 'frm', 'kdj', 'mxu', 'mog', 'bci', 'tzh', 'tpl', 'nlv', 'rcf', 'xmg', 'efi', 'yea', 'baw', 'gmh', 'dak', 'bin', 'nco', 'zyb', 'dug', 'fuc', 'gue', 'lkt', 'bum', 'mrw', 'bud', 'kvt', 'rhg', 'ium', 'ify', 'mrn', 'otn', 'syw', 'tvs', 'plw', 'idu', 'zsr', 'ain', 'tzl', 'lhm', 'rnl', 'mhl', 'yij', 'mlk', 'nnd', 'mgo', 'nlc', 'cor', 'bgf', 'luc', 'buo', 'nge', 'enb', 'bfm', 'dig', 'kca', 'crk', 'bec', 'tdg', 'akb', 'bwx', 'tsb', 'akh', 'mlw', 'ngn', 'mfh', 'isd', 'tbk', 'dov', 'evn', 'pkb', 'syr', 'aaa', 'kwu', 'lbr', 'sbd', 'tkr', 'ppk', 'dyo', 'bwt', 'tob', 'mnf', 'adh', 'mww', 'tio', 'bkc', 'hoj', 'mzm', 'dao', 'mne', 'rar', 'nnb', 'ike', 'ibb', 'tiv', 'nym', 'cuv', 'wni', 'xkg', 'oro', 'qvo', 'bfd', 'que', 'ags', 'kxv', 'rap', 'bra', 'max', 'nd', 'omb', 'orh', 'gou', 'otk', 'tdx', 'ybi', 'gpe', 'kxp', 'ryu', 'nyq', 'lob', 'dwu', 'tlb', 'guu', 'nla', 'zom', 'bxk', 'qxu', 'enx', 'thy', 'shs', 'iii', 'kck', 'kdi', 'tab', 'wwa', 'npy', 'due', 'kwn', 'byn', 'ifu', 'zro', 'ena', 'kdh', 'gqr', 'tdb', 'pnz', 'mhw', 'mor', 'nba', 'slr', 'mwn', 'knn', 'bhs', 'kwx', 'aeu', 'syb', 'sei', 'ddn', 'bgr', 'ngl', 'dts', 'dgo', 'src', 'miy', 'mvp', 'kky', 'bgg', 'nyy', 'pbu', 'nku', 'cri', 'akk', 'lzz', 'mtg', 'rjs', 'hms', 'btd', 'abz', 'kru', 'nlx', 'tby', 'kfo', 'lew', 'vun', 'gia', 'tem', 'mbf', 'mbi', 'iku', 'bua', 'lhi', 'udg', 'kng', 'zne', 'nbe', 'ssn', 'odk', 'kum', 'mnb', 'xcl', 'bru', 'czt', 'mhi', 'mzh', 'afb', 'sda', 'njo', 'mgg', 'mej', 'sdk', 'bqi', 'ifk', 'rtm', 'xsr', 'fvr', 'gaq', 'ajg', 'bfa', 'pui', 'chj', 'kln', 'aar', 'mmn', 'bht', 'rub', 'bwo', 'pdt', 'cku', 'ceg', 'shy', 'aym', 'krj', 'peg', 'plg', 'bgn', 'hnn', 'csa', 'lki', 'twu', 'bgz', 'cjs', 'sga', 'xbr', 'ifa', 'rki', 'pck', 'bik', 'mnk', 'akl', 'mwv', 'ljp', 'kqs', 'mxx', 'lah', 'muv', 'nag', 'moc', 'cag', 'mzi', 'hac', 'tgj', 'ijs', 'smj', 'ykg', 'mer', 'kqe', 'guk', 'hwc', 'mqy', 'yas', 'her', 'dip', 'rmb', 'mwm', 'btt', 'hrx', 'itv', 'myb', 'acn', 'bhp', 'mbd', 'lot', 'amc', 'syl', 'sbl', 'xty', 'ahr', 'kyu', 'mjw', 'hoc', 'gkp', 'nci', 'gdg', 'mta', 'ttj', 'ava', 'nce', 'pcg', 'hz', 'nmz', 'rim', 'lbm', 'cla', 'ker', 'trn', 'tsi', 'bpx', 'xsb', 'coe', 'mjl', 'mns', 'mif', 'yak', 'bhw', 'sbp', 'mmy', 'lbj', 'hdn', 'wno', 'tap', 'icr', 'mur', 'nyh', 'guq', 'lnd', 'kit', 'ndc', 'cyb', 'aqt', 'ifb', 'xmv', 'ldn', 'xdy', 'nmc', 'klb', 'bjg', 'mtp', 'zin', 'gbk', 'ocu', 'lui', 'ayo', 'frc', 'laa', 'ale', 'sgw', 'lut', 'btm', 'mfm', 'zpi', 'nnp', 'pse', 'nmm', 'bdq', 'mrz', 'gby', 'bou', 'shb', 'avu', 'aro', 'kfc', 'end', 'sef', 'pne', 'jmc', 'tek', 'lud', 'mgu', 'tsg', 'ngj', 'nio', 'dar', 'yrl', 'haq', 'mvv', 'won', 'pqm', 'zag', 'tlj', 'esu', 'muh', 'crj', 'mjg', 'tao', 'ogc', 'meo', 'ggw', 'shu', 'adj', 'sby', 'jya', 'kgo', 'nnw', 'khw', 'mzj', 'ort', 'cnw', 'jit', 'lev', 'qva', 'dhv', 'pem', 'jaa', 'ntk', 'bni', 'kod', 'pmy', 'rwk', 'kzf', 'mde', 'moz', 'snf', 'bim', 'wca', 'kqi', 'mzp', 'shg', 'bvz', 'bzt', 'dav', 'nbc', 'ctz', 'yur', 'raw', 'ikx', 'bkw', 'tsc', 'kyh', 'old', 'bqt', 'aim', 'abq', 'wau', 'cce', 'hea', 'atq', 'ksb', 'dma', 'nph', 'mix', 'sac', 'waw', 'thq', 'bva', 'enl', 'mla', 'dcr', 'did', 'see', 'odu', 'ldi', 'mrh', 'bot', 'ebu', 'ksp', 'lai', 'kxj', 'chm', 'sil', 'bdh', 'ssp', 'tmf', 'mov', 'bbq', 'kjc', 'trf', 'emk', 'nit', 'rwr', 'shr', 'kus', 'anu', 'thf', 'fit', 'bfj', 'tih', 'bnj', 'arr', 'unk', 'wsg', 'aol', 'hmr', 'buw', 'gww', 'nih', 'goh', 'adi', 'lmk', 'suc', 'ute', 'cay', 'lis', 'ikt', 'tls', 'rin', 'ttq', 'kjd', 'bcj', 'sck', 'zun', 'gnd', 'maw', 'iry', 'lme', 'bkl', 'cgg', 'yae', 'xkv', 'kix', 'trp', 'msi', 'ahg', 'mat', 'soz', 'biu', 'eip', 'ktb', 'sdc', 'mfi', 'yns', 'dbq', 'irk', 'kfs', 'mim', 'bfw', 'sro', 'tnr', 'otd', 'ury', 'kwv', 'bje', 'bej', 'njm', 'nau', 'uzs', 'mqu', 'ciw', 'rmo', 'srb', 'bdv', 'suj', 'heh', 'trq', 'qus', 'kks', 'tfn', 'alh', 'lmn', 'bgj', 'tqo', 'skg', 'sch', 'chu', 'aqz', 'dru', 'tll', 'agw', 'njn', 'nbu', 'wic', 'xte', 'rag', 'ksc', 'nyk', 'sop', 'wdd', 'njz', 'lag', 'rui', 'qxs', 'zak', 'oym', 'tts', 'urh', 'leh', 'ttv', 'kun', 'pny', 'hra', 'haz', 'gyd', 'spn', 'zyg', 'djm', 'hag', 'nim', 'sgc', 'way', 'mnx', 'nsa', 'pum', 'kye', 'hoy', 'dnj', 'kap', 'kbr', 'kpz', 'dis', 'chn', 'esi', 'hdy', 'isk', 'chb', 'egl', 'gwd', 'szb', 'mrq', 'dng', 'sgh', 'bku', 'zoh', 'mip', 'sjn', 'nid', 'xnr', 'msn', 'dyi', 'trs', 'urk', 'kfx', 'mbq', 'mck', 'duo', 'ghl', 'khg', 'shh', 'mye', 'tvt', 'nfu', 'lwg', 'gej', 'bmv', 'ndh', 'bsk', 'pst', 'lea', 'bno', 'jup', 'kmw', 'bbw', 'kuj', 'crt', 'tog', 'gyn', 'lub', 'rml', 'yli', 'asa', 'sba', 'ibg', 'snw', 'nut', 'pak', 'alj', 'mua', 'miu', 'bji', 'hay', 'mul', 'sux', 'one', 'any', 'dws', 'nmf', 'lti', 'tli', 'tdc', 'kgr', 'xtc', 'bom', 'ngt', 'qxl', 'cas', 'ihp', 'mgr', 'acw', 'pmq', 'kff', 'cnk', 'nzm', 'mwq', 'tdj', 'yan', 'dib', 'gqa', 'wlv', 'ekg', 'tpn', 'unx', 'iso', 'nmn', 'lam', 'bez', 'kxm', 'kpo', 'kmm', 'erk', 'klr', 'tui', 'pai', 'kjp', 'skt', 'apt', 'izh', 'nyb', 'biy', 'rad', 'chw', 'koh', 'tvu', 'bwd', 'gow', 'xrw', 'ldm', 'alu', 'sdo', 'mrt', 'mmm', 'mwa', 'emb', 'wti', 'bjt', 'ver', 'wad', 'lee', 'aji', 'yun', 'mpd', 'bau', 'kbj', 'jmr', 'ite', 'kvu', 'kdu', 'nga', 'bzw', 'skd', 'bwe', 'kph', 'keb', 'nzy', 'xkn', 'dbn', 'kga', 'jbj', 'sjg', 'mjx', 'izz', 'eot', 'sui', 'cto', 'oia', 'mke', 'bfs', 'brq', 'lga', 'ogb', 'mfv', 'glo', 'sek', 'sns', 'blq', 'cdz', 'fla', 'weh', 'zmp', 'poc', 'ndz', 'kss', 'bvi', 'mae', 'ynq', 'mtk', 'azm', 'tds', 'tcf', 'kom', 'niw', 'nil', 'ksn', 'ums', 'bek', 'sgy', 'afo', 'kvq', 'hvv', 'ksg', 'pta', 'wut', 'lol', 'keu', 'eyo', 'anw', 'kno', 'myl', 'onp', 'uar', 'kcx', 'irn', 'pkg', 'bde', 'ula', 'bev', 'avi', 'bcn', 'sig', 'khy', 'tpx', 'aif', 'fal', 'prq', 'dcc', 'qya', 'dij', 'saj', 'mgd', 'add', 'aul', 'lrl', 'ckl', 'kxn', 'stv', 'mfc', 'bsh', 'msj', 'kvm', 'kaj', 'khr', 'byp', 'hbn', 'mzq', 'pbn', 'gnb', 'jru', 'fmp', 'abu', 'abn', 'gnk', 'hav', 'jpa', 'dhi', 'ksf', 'kee', 'ets', 'nnu', 'cog', 'ato', 'qui', 'iar', 'iko', 'jmb', 'lic', 'apj', 'saf', 'cry', 'bor', 'law', 'job', 'kxf', 'yog', 'dai', 'ekp', 'nbv', 'swj', 'wme', 'anx', 'auk', 'scl', 'yig', 'twm', 'tmc', 'eja', 'txy', 'smq', 'ccg', 'akp', 'bzy', 'dbm', 'bzx', 'lmx', 'nnh', 'gju', 'ert', 'bjh', 'mcc', 'vnk', 'akg', 'ica', 'nuk', 'log', 'pca', 'chx', 'bdu', 'lou', 'swo', 'sax', 'myh', 'tuq', 'vaj', 'tsa', 'kmy', 'ish', 'bqg', 'naw', 'tlr', 'ldb', 'nti', 'zbc', 'ass', 'apd', 'aqm', 'soe', 'kfa', 'hlb', 'ral', 'sde', 'dem', 'xmz', 'nkx', 'spo', 'mef', 'tmq', 'tau', 'kma', 'nir', 'lna', 'dof', 'wan', 'mfb', 'ndr', 'kio', 'muk', 'kle', 'kpq', 'bey', 'iyx', 'kcl', 'drs', 'pkh', 'bqr', 'tsu', 'aio', 'mrf', 'gvl', 'bft', 'pga', 'xkl', 'kib', 'mgp', 'uba', 'mgl', 'scg', 'kqo', 'lom', 'vay', 'cly', 'sts', 'krh', 'zgb', 'lsr', 'dta', 'lkh', 'nxg', 'ycl', 'txn', 'drd', 'emg', 'knw', 'flr', 'bcp', 'led', 'tei', 'kel', 'ysn', 'mea', 'ilu', 'crc', 'asi', 'bja', 'zts', 'quv', 'bex', 'kxw', 'wmo', 'wsi', 'bpu', 'kkk', 'sky', 'gol', 'sgi', 'slz', 'bif', 'scu', 'kji', 'nun', 'kpe', 'moy', 'piv', 'ruk', 'tdl', 'blm', 'sbr', 'mtl', 'aac', 'jig', 'mrl', 'blh', 'ztp', 'sdg', 'pym', 'ram', 'bza', 'dor', 'dgg', 'tmy', 'epi', 'vkl', 'gmm', 'bcr', 'lje', 'bys', 'bcs', 'mfd', 'hwo', 'pbp', 'puo', 'kfe', 'rbb', 'bbf', 'frd', 'dow', 'mzr', 'kqy', 'owi', 'plv', 'liq', 'diu', 'hts', 'ngi', 'mjt', 'acv', 'bcw', 'dil', 'huf', 'kkj', 'qxa', 'wyy', 'ore', 'dhd', 'ahs', 'kzr', 'dsn', 'umu', 'zng', 'bxl', 'bzz', 'lle', 'mda', 'fan', 'mfg', 'gdf', 'ril', 'hgm', 'was', 'sym', 'biv', 'jmd', 'kjl', 'twx', 'jma', 'cae', 'pru', 'mwg', 'vaa', 'duq', 'mmg', 'sen', 'mlv', 'lht', 'tty', 'bid', 'mvz', 'nkh', 'sep', 'van', 'zkr', 'ayg', 'bfh', 'jax', 'gae', 'hum', 'kjq', 'ayb', 'mey', 'kza', 'bzv', 'oke', 'jna', 'vmp', 'klq', 'bil', 'anc', 'pac', 'lvk', 'prc', 'sdp', 'aal', 'tqu', 'daq', 'skx', 'msg', 'sho', 'iai', 'ntm', 'kdr', 'tgs', 'wog', 'msw', 'yot', 'how', 'mnz', 'iqw', 'ndy', 'ems', 'mdt', 'bli', 'sry', 'bby', 'ymm', 'cin', 'cvg', 'sbb', 'wrm', 'adl', 'kys', 'ukp', 'pay', 'akc', 'sgr', 'ibl', 'zim', 'itd', 'cyo', 'lkr', 'shq', 'jiu', 'dos', 'dsq', 'mue', 'gaf', 'jio', 'knt', 'dya', 'kol', 'mse', 'koe', 'mji', 'fqs', 'tsr', 'yev', 'ndx', 'knp', 'hru', 'kvf', 'doz', 'elm', 'mzv', 'mtu', 'brp', 'pww', 'iqu', 'pwo', 'raf', 'bqj', 'ksi', 'sif', 'akf', 'tus', 'aup', 'mhc', 'skj', 'tro', 'nbi', 'khj', 'jdt', 'tal', 'tgc', 'yba', 'pci', 'tmr', 'fak', 'bmd', 'shw', 'tdy', 'vah', 'nsm', 'kul', 'juo', 'dva', 'jms', 'dhm', 'nbn', 'byv', 'jle', 'lir', 'dni', 'oka', 'blc', 'bpw', 'iwm', 'tis', 'byd', 'kcj', 'wja', 'afu', 'bst', 'abm', 'xod', 'mkf', 'mrm', 'hol', 'pss', 'pcb', 'kqb', 'bhq', 'bkk', 'ndm', 'dgi', 'kof', 'kht', 'mbo', 'rah', 'tik', 'rao', 'jmi', 'age', 'gno', 'jqr', 'kwe', 'tcc', 'kis', 'zay', 'osi', 'tgw', 'jib', 'oks', 'gax', 'tpj', 'daw', 'twb', 'bgq', 'cfd', 'sku', 'bda', 'sbu', 'lra', 'orc', 'mtd', 'vaf', 'abi', 'ncg', 'gar', 'mym', 'mgk', 'plr', 'suq', 'aiw', 'bsy', 'liu', 'nsu', 'rkm', 'ria', 'kni', 'bbv', 'nat', 'plk', 'res', 'zaf', 'orz', 'wli', 'xns', 'bjc', 'srl', 'nmb', 'sok', 'esh', 'pof', 'yay', 'xta', 'ito', 'kpk', 'bep', 'mgi', 'llp', 'lbq', 'tan', 'yuy', 'ndd', 'niy', 'ted', 'sie', 'kui', 'lcc', 'mhs', 'xwg', 'bsi', 'ckh', 'ung', 'bol', 'puc', 'txo', 'mep', 'lae', 'bnv', 'ccj', 'djn', 'myp', 'wrp', 'lpa', 'yui', 'byz', 'swl', 'bth', 'mpe', 'giw', 'mql', 'bib', 'mwi', 'dks', 'pbi', 'eit', 'tsx', 'zpw', 'kad', 'jei', 'bub', 'loa', 'aty', 'ggb', 'kny', 'bmi', 'zpr', 'ttr', 'kps', 'bxb', 'kci', 'lup', 'cua', 'elk', 'foi', 'ibd', 'bzf', 'gdl', 'sge', 'kfr', 'llc', 'kwg', 'bks', 'kxz', 'mfa', 'qxr', 'can', 'emn', 'bqv', 'yaw', 'buh', 'bsb', 'bux', 'mhz', 'ael', 'swv', 'tic', 'crl', 'krp', 'hno', 'had', 'hoa', 'ngz', 'ssy', 'yuz', 'ktp', 'tdv', 'fir', 'cli', 'tdo', 'lax', 'xks', 'yoy', 'tww', 'eza', 'mkc', 'nfd', 'phl', 'axk', 'lyn', 'psw', 'njb', 'kbv', 'nja', 'tva', 'fut', 'brf', 'jbu', 'hut', 'kws', 'mpg', 'szv', 'afi', 'slu', 'nhp', 'avd', 'aog', 'kna', 'onn', 'kid', 'yaf', 'xra', 'tra', 'oyd', 'kls', 'kkz', 'sss', 'jge', 'wbl', 'dir', 'kgj', 'pdo', 'bab', 'bee', 'jeh', 'tba', 'grt', 'pip', 'dax', 'yyu', 'cnb', 'kvo', 'bkr', 'ior', 'igb', 'hit', 'haa', 'wlc', 'mgv', 'nre', 'ghc', 'msl', 'avn', 'ble', 'hio', 'xnz', 'rnd', 'hca', 'bhh', 'gwn', 'ida', 'duv', 'krx', 'tji', 'gog', 'taz', 'cik', 'nhb', 'dre', 'bnx', 'gcd', 'mro', 'ayn', 'nac', 'bws', 'khc', 'ywn', 'sbg', 'mnv', 'pwa', 'gwt', 'gek', 'aot', 'bmf', 'lum', 'gri', 'kwl', 'ekr', 'kcr', 'nru', 'ebr', 'dwa', 'gbz', 'kfz', 'irx', 'muo', 'ulu', 'sya', 'tdd', 'niz', 'jaf', 'lul', 'mxn', 'kzc', 'mxj', 'gcn', 'rwa', 'ugo', 'mfz', 'xer', 'mza', 'los', 'pre', 'mkg', 'kia', 'awn', 'eky', 'orx', 'prm', 'ygw', 'cbj', 'naz', 'ncf', 'tiq', 'tbp', 'all', 'wwo', 'nri', 'ykm', 'mxm', 'mhk', 'sti', 'lgt', 'ssk', 'psa', 'gna', 'ega', 'okr', 'mdj', 'llu', 'svs', 'neq', 'tkq', 'eme', 'kst', 'arv', 'gvr', 'ldk', 'nli', 'sru', 'wbf', 'mbv', 'oma', 'nhd', 'kbl', 'aba', 'vap', 'plc', 'tsj', 'mxe', 'asb', 'dms', 'has', 'klw', 'ige', 'rey', 'set', 'twf', 'nzb', 'har', 'mnj', 'kbz', 'zlj', 'sur', 'brt', 'tkp', 'iby', 'kwa', 'nnm', 'rav', 'mmz', 'cbd', 'kvj', 'pfe', 'bpa', 'mls', 'sqq', 'hid', 'tye', 'vmm', 'mpn', 'cpx', 'tsv', 'aao', 'lek', 'mlm', 'iki', 'wod', 'tyn', 'aun', 'sav', 'sip', 'lch', 'zaz', 'bcv', 'kcv', 'mpr', 'mev', 'pil', 'psn', 'kpl', 'lbw', 'cfg', 'gis', 'gdx', 'agl', 'skv', 'tdn', 'tpe', 'udl', 'chl', 'mcn', 'stj', 'ndi', 'zhi', 'pcc', 'var', 'ilb', 'kdd', 'moj', 'nmk', 'nnj', 'cje', 'yim', 'kvr', 'gde', 'ctg', 'khu', 'jul', 'gwa', 'kge', 'mdd', 'kmd', 'mdm', 'kmc', 'nto', 'kie', 'mvo', 'pmi', 'wlx', 'vig', 'rir', 'atk', 'pbo', 'giz', 'moa', 'lig', 'kuy', 'jnj', 'kdt', 'grs', 'ler', 'brr', 'xom', 'azd', 'nup', 'bfy', 'sys', 'cro', 'tjg', 'beq', 'txt', 'kdm', 'sce', 'gop', 'mln', 'kdz', 'baa', 'adn', 'bfq', 'hal', 'bgd', 'qux', 'xmt', 'kwk', 'bwi', 'buf', 'sug', 'nkk', 'haj', 'ybj', 'alw', 'bcq', 'kil', 'dnn', 'tcx', 'bnn', 'beh', 'oyb', 'wbb', 'zpn', 'bbp', 'bzk', 'mbx', 'tcn', 'wlw', 'lln', 'afe', 'ttw', 'dgd', 'srz', 'kml', 'sjl', 'afh', 'ckx', 'dtm', 'dur', 'ghn', 'nar', 'clc', 'bfg', 'mhu', 'fun', 'saw', 'bxq', 'mvf', 'kdp', 'tgy', 'agc', 'cja', 'geg', 'lyg', 'ywq', 'dio', 'qvj', 'fab', 'boh', 'bca', 'int', 'tlq', 'hmb', 'lbo', 'mqz', 'mhy', 'alk', 'yup', 'jns', 'gid', 'wdj', 'saz', 'ctl', 'cod', 'hei', 'kfp', 'sgp', 'ttk', 'wbk', 'gga', 'pia', 'awe', 'kyo', 'acd', 'meh', 'ema', 'tmn', 'igl', 'smw', 'tol', 'etx', 'teq', 'ahb', 'pbs', 'yra', 'onj', 'zln', 'pek', 'dgh', 'rog', 'anf', 'mzb', 'dee', 'nen', 'ott', 'svc', 'lur', 'dot', 'nyj', 'nmh', 'utr', 'scp', 'wgb', 'jku', 'wof', 'syk', 'gox', 'hia', 'pwm', 'yer', 'ogo', 'akw', 'alf', 'sha', 'hae', 'gut', 'tbw', 'mlu', 'moi', 'aki', 'aix', 'skb', 'dia', 'ekl', 'szg', 'fpe', 'kfk', 'siy', 'bye', 'lar', 'fng', 'dei', 'mbz', 'iyo', 'kdq', 'jum', 'kvb', 'ywa', 'sev', 'mbm', 'tgd', 'xmh', 'mij', 'ppq', 'mrg', 'knd', 'gra', 'tbt', 'mgb', 'aof', 'ayu', 'lbu', 'loy', 'win', 'bsc', 'khn', 'tma', 'clk', 'huh', 'lec', 'itr', 'mpq', 'bpn', 'tdf', 'fuy', 'nbb', 'oni', 'etu', 'mtt', 'mxd', 'phk', 'der', 'bdb', 'tkl', 'lbn', 'pua', 'nuo', 'tix', 'vas', 'kcf', 'zps', 'fie', 'noe', 'tri', 'ktn', 'siw', 'vmk', 'thr', 'mnp', 'lkn', 'bmb', 'tft', 'doy', 'nyi', 'yam', 'kgq', 'aaw', 'mjs', 'muz', 'aad', 'tlp', 'iru', 'snv', 'blf', 'zyn', 'alx', 'xub', 'wow', 'kxh', 'nnc', 'ndp', 'wew', 'zzj', 'kvd', 'txa', 'bvw', 'gew', 'mgf', 'dih', 'tyz', 'xac', 'kef', 'bgp', 'sjr', 'caq', 'kdx', 'nev', 'kmz', 'aks', 'knm', 'lem', 'adx', 'cfa', 'qwa', 'crq', 'ank', 'ats', 'dmw', 'ipo', 'mqx', 'bwm', 'gvj', 'dbb', 'kic', 'tti', 'isi', 'mqn', 'cra', 'thz', 'yuf', 'cmr', 'nqg', 'mzw', 'amt', 'bgi', 'dmo', 'loe', 'pbg', 'ged', 'rau', 'kfb', 'nez', 'sel', 'kex', 'yaz', 'dus', 'kzm', 'kfd', 'nka', 'sng', 'bwu', 'bsq', 'ifm', 'arw', 'pnu', 'xok', 'bte', 'sad', 'bpz', 'ati', 'gni', 'hed', 'mtr', 'stf', 'jeb', 'pbv', 'bbu', 'bse', 'lva', 'sob', 'yis', 'tld', 'awi', 'gru', 'hue', 'wib', 'jab', 'klg', 'lla', 'gbr', 'lmu', 'psi', 'doo', 'umm', 'duc', 'kgy', 'lil', 'tlx', 'tug', 'btu', 'czn', 'kpc', 'arx', 'sir', 'ife', 'tex', 'grx', 'psh', 'asr', 'nzk', 'lbf', 'dge', 'njh', 'kvy', 'ppm', 'sxw', 'mmh', 'kwb', 'bhf', 'zae', 'ybe', 'cqd', 'otr', 'zoc', 'cdm', 'prx', 'uki', 'jkp', 'tpr', 'agh', 'las', 'oku', 'dtb', 'blr', 'niq', 'ijn', 'bdi', 'pko', 'uta', 'szp', 'akr', 'klu', 'hux', 'ydg', 'naj', 'sst', 'grd', 'ado', 'vra', 'sbc', 'dzg', 'yah', 'nkw', 'mbu', 'nlo', 'yes', 'xmw', 'she', 'bpp', 'erg', 'agx', 'jow', 'vut', 'fud', 'pav', 'nma', 'duw', 'gel', 'bdm', 'wmd', 'gmb', 'njj', 'ade', 'goa', 'kqj', 'wob', 'pmx', 'bww', 'coz', 'pqa', 'mcw', 'izr', 'tuy', 'kmq', 'ngc', 'kot', 'qud', 'lwo', 'yix', 'cdn', 'twe', 'pmm', 'tru', 'kpm', 'pht', 'kcd', 'kay', 'pha', 'wmb', 'sau', 'ilk', 'ner', 'cul', 'ney', 'tpp', 'khq', 'glh', 'diw', 'lsh', 'mkw', 'noz', 'nux', 'wbj', 'bkv', 'gab', 'kli', 'dry', 'sos', 'ijj', 'mct', 'kai', 'bsf', 'nbh', 'pcj', 'bky', 'mxy', 'mtj', 'dun', 'dnd', 'tyr', 'vor', 'pyu', 'say', 'dna', 'cob', 'mmd', 'ncr', 'whk', 'kmn', 'tcd', 'kxc', 'byo', 'aha', 'inj', 'mma', 'dmr', 'jml', 'kfq', 'cjp', 'mnl', 'nke', 'sao', 'trd', 'xvi', 'sly', 'dbj', 'bcy', 'les', 'nkb', 'svb', 'snq', 'hnd', 'kao', 'gud', 'gxx', 'peb', 'ijc', 'cbn', 'cvn', 'sse', 'abo', 'ebo', 'slc', 'neb', 'sso', 'zns', 'qxq', 'rji', 'uya', 'ngb', 'ora', 'eka', 'rab', 'wle', 'gkn', 'luj', 'juk', 'bhb', 'bof', 'pow', 'fod', 'bei', 'cch', 'ala', 'mmp', 'gvp', 'uiv', 'bpv', 'nwb', 'csh', 'brg', 'nps', 'pku', 'msh', 'hkk', 'nfl', 'nlu', 'lcp', 'akq', 'ggu', 'nyf', 'klz', 'bhy', 'ppi', 'bqs', 'pid', 'leq', 'cou', 'mte', 'cbo', 'bwq', 'cns', 'app', 'kvv', 'nud', 'kfi', 'twp', 'kmi', 'ndu', 'doa', 'crw', 'asc', 'abr', 'ich', 'cld', 'kxb', 'suy', 'ego', 'bhz', 'jaq', 'lma', 'bta', 'mbp', 'lok', 'lor', 'mmc', 'mme', 'ngs', 'soj', 'nlj', 'dme', 'nyd', 'ghk', 'aik', 'cko', 'knk', 'ofu', 'iow', 'tdk', 'gvo', 'toh', 'pos', 'ano', 'kih', 'bvc', 'suv', 'zpx', 'ztg', 'ksv', 'thm', 'ayi', 'mfl', 'wry', 'zwa', 'diz', 'uuu', 'enn', 'gdu', 'bvm', 'boq', 'kez', 'mtf', 'ahl', 'coc', 'anm', 'ths', 'twy', 'xem', 'glj', 'sbh', 'ncb', 'ssb', 'scw', 'aab', 'png', 'gua', 'khe', 'kjg', 'hmd', 'mcs', 'wsa', 'bcz', 'cdr', 'lap', 'kgb', 'mpc', 'mki', 'dbi', 'dim', 'lml', 'ldl', 'zmb', 'wnp', 'esg', 'nms', 'blb', 'coj', 'ald', 'ndv', 'lbx', 'mrp', 'fip', 'mdh', 'aoa', 'gea', 'coh', 'nxr', 'xri', 'krf', 'akd', 'bun', 'erh', 'ess', 'lep', 'aap', 'kmt', 'vem', 'wls', 'sam', 'xpe', 'kei', 'dby', 'iti', 'gya', 'mds', 'mrr', 'ldj', 'hve', 'lef', 'nda', 'bbt', 'opa', 'bio', 'mcu', 'gnu', 'bit', 'zkd', 'liz', 'mng', 'wji', 'wom', 'twh', 'zrs', 'qxp', 'scs', 'nuf', 'aqg', 'bov', 'nbp', 'tpm', 'fay', 'sou', 'qun', 'aee', 'gim', 'xtn', 'uhn', 'brl', 'mlx', 'sor', 'kog', 'arh', 'lip', 'agf', 'nal', 'sol', 'kzq', 'bly', 'pcn', 'nxq', 'bmm', 'lky', 'sne', 'raa', 'fuu', 'asu', 'sto', 'cia', 'bxg', 'jgk', 'glw', 'git', 'drg', 'lu', 'tii', 'kvw', 'bet', 'dsh', 'ymb', 'mzk', 'apm', 'hml', 'yll', 'gbg', 'itz', 'ola', 'mnm', 'aec', 'wtm', 'bwr', 'mdb', 'jub', 'mch', 'moe', 'pll', 'bwf', 'pei', 'ppt', 'bdw', 'wci', 'pkt', 'otw', 'lop', 'dny', 'xuu', 'hoo', 'cjm', 'wgi', 'cdh', 'gnm', 'aca', 'mug', 'sjo', 'grh', 'kbb', 'idi', 'vam', 'weo', 'nng', 'hul', 'siu', 'bdl', 'bns', 'kyk', 'cwt', 'jer', 'anj', 'ybl', 'kwo', 'cdf', 'maf', 'deg', 'lmy', 'sed', 'zyj', 'yey', 'bcg', 'cdj', 'khl', 'hah', 'jun', 'oru', 'qum', 'bnm', 'fap', 'poo', 'org', 'lgu', 'soc', 'swi', 'bhr', 'gdb', 'gro', 'tkg', 'kem', 'puu', 'biz', 'mdw', 'smy', 'eto', 'lnu', 'rhp', 'mfn', 'sjm', 'kzi', 'nnz', 'kfy', 'gbe', 'ayz', 'huc', 'scv', 'nrg', 'cte', 'kjr', 'mtq', 'mwt', 'ygr', 'pic', 'ask', 'mfo', 'ktv', 'bfu', 'dox', 'mga', 'nbr', 'tvd', 'lse', 'rol', 'hoe', 'djc', 'jen', 'lmg', 'igs', 'yum', 'cbg', 'zik', 'gaj', 'nhv', 'raj', 'bhj', 'slp', 'pnq', 'tce', 'nbm', 'bqw', 'une', 'lih', 'ncx', 'mho', 'ttm', 'kub', 'auu', 'oub', 'tdh', 'afz', 'lro', 'bav', 'guf', 'ncm', 'xkb', 'kzs', 'xky', 'czh', 'vrs', 'bgv', 'gpa', 'cll', 'hmt', 'tes', 'fll', 'pln', 'sew', 'prk', 'sdq', 'etn', 'rgs', 'xsu', 'krs', 'nfr', 'com', 'dri', 'thv', 'mez', 'kla', 'tow', 'tnm', 'caz', 'wlo', 'kwy', 'bbi', 'ono', 'brd', 'hur', 'ndb', 'prn', 'xes', 'rmt', 'asy', 'bzu', 'lgq', 'vag', 'aez', 'ksm', 'yki', 'tul', 'mlf', 'agb', 'mlq', 'knx', 'vmz', 'bap', 'bcf', 'dbd', 'thp', 'iri', 'ilp', 'spu', 'gad', 'klx', 'tja', 'jat', 'lhl', 'xuj', 'rdb', 'dub', 'nlk', 'kyv', 'aaf', 'dde', 'pgg', 'ruy', 'kqk', 'zrg', 'nki', 'hld', 'tef', 'zpp', 'ysp', 'zeh', 'gso', 'dyg', 'liw', 'kft', 'tyy', 'mxa', 'itt', 'krv', 'bmq', 'ywl', 'deh', 'nao', 'djo', 'xgu', 'gbl', 'dkx', 'zpg', 'tqb', 'lri', 'ktc', 'aoe', 'rng', 'ogg', 'kpa', 'nkf', 'ldo', 'cnq', 'otx', 'sjb', 'yuq', 'nos', 'glr', 'dbv', 'clt', 'twr', 'lmi', 'okx', 'wem', 'nwm', 'yat', 'mku', 'zpk', 'bvh', 'ctt', 'blo', 'kjo', 'lpo', 'mii', 'sbn', 'oso', 'wba', 'ont', 'mrd', 'npb', 'tla', 'abh', 'mab', 'cde', 'zpd', 'knz', 'its', 'azt', 'bgw', 'ghr', 'acz', 'klk', 'sfw', 'dez', 'krn', 'pbl', 'buu', 'sdr', 'krw', 'mhp', 'vmc', 'lhp', 'byj', 'def', 'bxs', 'kvl', 'lal', 'url', 'lmd', 'bhi', 'kdy', 'zpj', 'tks', 'ahp', 'bfb', 'das', 'kvi', 'jdg', 'aug', 'kxx', 'kyb', 'ttb', 'cky', 'fuq', 'rat', 'xtl', 'byc', 'gmz', 'mqh', 'bac', 'tvn', 'tto', 'zkn', 'snm', 'saa', 'ccl', 'nmi', 'dza', 'jnl', 'kfg', 'nni', 'gig', 'loq', 'skq', 'cna', 'kif', 'nqy', 'pnc', 'tkb', 'yiu', 'fwe', 'kcc', 'bfo', 'qws', 'xtj', 'bsl', 'npo', 'tng', 'bha', 'pez', 'soa', 'nqt', 'soo', 'dhw', 'ate', 'mny', 'nnl', 'jwi', 'dwz', 'xrb', 'nct', 'dka', 'pud', 'jiy', 'piy', 'etc', 'pps', 'pwb', 'kej', 'ldg', 'usi', 'prt', 'vmx', 'nxd', 'ykk', 'kqp', 'ntr', 'jrt', 'ngw', 'kfv', 'ggg', 'auq', 'gau', 'gbn', 'mtb', 'luz', 'vav', 'ghe', 'bfr', 'smt', 'tml', 'jkr', 'ckm', 'ayt', 'atp', 'itl', 'ncq', 'bjj', 'yiq', 'bjo', 'nse', 'xti', 'mqg', 'sgj', 'boo', 'cib', 'slx', 'vkn', 'sct', 'grj', 'env', 'csk', 'sci', 'pbm', 'wkd', 'tak', 'lie', 'mnu', 'pug', 'drt', 'cok', 'xkz', 'tsp', 'bhu', 'atu', 'xkk', 'cma', 'gok', 'kce', 'kep', 'ktj', 'rmz', 'ibm', 'pcf', 'xkf', 'kfh', 'lik', 'smh', 'bip', 'kcs', 'zms', 'yif', 'gas', 'xwl', 'kty', 'nof', 'gez', 'cov', 'hmw', 'tga', 'phq', 'xkt', 'srx', 'tcu', 'nmo', 'cwd', 'spt', 'njx', 'uis', 'zpa', 'dln', 'zpt', 'pmj', 'bpe', 'vmj', 'zpe', 'mpu', 'rme', 'zmq', 'nsy', 'nuq', 'cnc', 'lrm', 'gba', 'gyz', 'skn', 'vum', 'kfu', 'mrv', 'agi', 'sbx', 'kch', 'nuz', 'bzs', 'tfi', 'sez', 'sjp', 'whg', 'pwr', 'mjv', 'duh', 'mdk', 'lrk', 'xuo', 'jbm', 'mzl', 'mik', 'bbo', 'bge', 'zte', 'awu', 'gbh', 'mum', 'hii', 'grv', 'hmg', 'ukw', 'kpb', 'soi', 'vmh', 'uss', 'bui', 'xkj', 'olu', 'bvy', 'nyw', 'asg', 'nix', 'jad', 'zpf', 'bhx', 'bqx', 'zuy', 'xmc', 'key', 'pcw', 'sld', 'clj', 'kkf', 'kjt', 'bqh', 'mml', 'mut', 'pom', 'kra', 'bqa', 'yde', 'mel', 'gbv', 'xum', 'pch', 'kmj', 'yiz', 'tkx', 'bqo', 'shc', 'nhz', 'kuh', 'smf', 'pcl', 'pbc', 'hgw', 'mxh', 'seg', 'mdu', 'ssi', 'gbo', 'cmi', 'irr', 'bgx', 'kow', 'kki', 'jog', 'des', 'amb', 'dho', 'tny', 'kev', 'hrm', 'hmj', 'fad', 'xwe', 'bka', 'kcq', 'tpq', 'hmz', 'tnv', 'ztl', 'ktf', 'wbq', 'jda', 'ldq', 'kku', 'toq', 'jmn', 'xsn', 'sju', 'cih', 'bix', 'ity', 'ztx', 'uth', 'dzl', 'ymk', 'nxk', 'tgt', 'cox', 'lpn', 'sle', 'kqm', 'bfe', 'mjz', 'jnd', 'ruz', 'yax', 'wss', 'kwt', 'ost', 'kzn', 'xsq', 'lel', 'ors', 'zpy', 'bmj', 'xtt', 'dgx', 'mkb', 'ldp', 'zcd', 'gjk', 'ksu', 'anl', 'smu', 'stn', 'bjx', 'tth', 'lgm', 'kmp', 'aww', 'mxs', 'gec', 'goj', 'bvu', 'stt', 'aoc', 'dhn', 'xkc', 'bma', 'eze', 'sre', 'mdn', 'zph', 'klo', 'cdi', 'zbu', 'knu', 'akt', 'cgk', 'tov', 'swk', 'mvg', 'njs', 'sbz', 'okh', 'gry', 'pxm', 'mkk', 'sfm', 'kyy', 'btg', 'nes', 'anr', 'wud', 'ayp', 'pdn', 'yeu', 'mxl', 'mfk', 'crm', 'buj', 'xdo', 'bga', 'kkd', 'kip', 'nyg', 'tou', 'buz', 'yno', 'swr', 'ojs', 'tnb', 'goz', 'crv', 'agy', 'bah', 'shm', 'zau', 'bkg', 'knl', 'kvx', 'tkt', 'kkn', 'kwc', 'tcp', 'rei', 'yhd', 'avl', 'phr', 'csg', 'aed', 'prl', 'vsl', 'ymr', 'mfs', 'fse', 'csn', 'zxx']\n"
     ]
    }
   ],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "print(tag_language)\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)')\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model_idx):\n",
    "\n",
    "\tmodel = models_df.loc[model_idx]\n",
    "\tmodel_tags = models_df.loc[model_idx]['tags']\n",
    "\tmodel_card_data = None\n",
    "\ttry:\n",
    "\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\texcept AttributeError:\n",
    "\t\tprint('No card data available for this model')\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model['id'])\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No architecture data available for this model')\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tbase_model = model_card_data['base_model']\n",
    "\t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model['author']]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No emission data available for this model')\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_json(model_idx):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    model_attributes = extract_model_attributes(model_idx)\n",
    "    \n",
    "    with open(os.path.join(result_path, 'test_models.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_attributes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_json(models_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    for model_idx in range(models_df.shape[0]):\n",
    "        output.append(extract_model_attributes(model_idx))\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.LargeLanguageModel.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_json(models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes_optimized(model):\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['id'] = model.id\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\t# try:\n",
    "\t# \tif model_card_data is not None:\n",
    "\t# \t\tif 'base_model' in model_card_data:\n",
    "\t# \t\t\tbase_model = model_card_data['base_model']\n",
    "\t# \t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t# \t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\t# except KeyError:\n",
    "\t# \tpass\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] \n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'models_data.json')\n",
    "\n",
    "# tot 697,162 models\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} models...')\n",
    "    models = api.list_models(filter=task, full=True, cardData=True)\n",
    "    for model in models:\n",
    "        model_attributes = extract_model_attributes_optimized(model)\n",
    "        add_to_json_file(model_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/alessiobuda/Documents/GitHub/chatIMPACT/database/hf_extracted_json/models_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "models_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>version</th>\n",
       "      <th>numberOfParameters</th>\n",
       "      <th>quantization</th>\n",
       "      <th>architecture</th>\n",
       "      <th>languages</th>\n",
       "      <th>modelCreator</th>\n",
       "      <th>licenseToUse</th>\n",
       "      <th>libraryFramework</th>\n",
       "      <th>contextLength</th>\n",
       "      <th>developers</th>\n",
       "      <th>openSource</th>\n",
       "      <th>uri</th>\n",
       "      <th>fineTuned</th>\n",
       "      <th>carbonEmission [CO2eq tons]</th>\n",
       "      <th>tokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>Salesforce/ctrl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>bsd-3-clause</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Salesforce]</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.05858</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, safeten...</td>\n",
       "      <td>None</td>\n",
       "      <td>[distilbert]</td>\n",
       "      <td>True</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>None</td>\n",
       "      <td>149200</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>openai-community/gpt2-large</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, onnx, s...</td>\n",
       "      <td>None</td>\n",
       "      <td>[openai-community]</td>\n",
       "      <td>True</td>\n",
       "      <td>1910.09700</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>openai-community/gpt2-medium</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, onnx, s...</td>\n",
       "      <td>None</td>\n",
       "      <td>[openai-community]</td>\n",
       "      <td>True</td>\n",
       "      <td>1910.09700</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>openai-community/gpt2-xl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, safeten...</td>\n",
       "      <td>None</td>\n",
       "      <td>[openai-community]</td>\n",
       "      <td>True</td>\n",
       "      <td>1910.09700</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>openai-community/gpt2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, onnx, s...</td>\n",
       "      <td>None</td>\n",
       "      <td>[openai-community]</td>\n",
       "      <td>True</td>\n",
       "      <td>10.57967/hf/0039</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>openai-gpt</td>\n",
       "      <td>openai-community/openai-gpt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf, rust, safetensors]</td>\n",
       "      <td>None</td>\n",
       "      <td>[openai-community]</td>\n",
       "      <td>True</td>\n",
       "      <td>1705.11168</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transfo-xl-wt103</td>\n",
       "      <td>transfo-xl/transfo-xl-wt103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>None</td>\n",
       "      <td>[transfo-xl]</td>\n",
       "      <td>True</td>\n",
       "      <td>1901.02860</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xlnet-base-cased</td>\n",
       "      <td>xlnet/xlnet-base-cased</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf, rust]</td>\n",
       "      <td>None</td>\n",
       "      <td>[xlnet]</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.08237</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xlnet-large-cased</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>None</td>\n",
       "      <td>mit</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>None</td>\n",
       "      <td>[xlnet]</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.08237</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                            id version numberOfParameters  \\\n",
       "0               ctrl               Salesforce/ctrl    None               None   \n",
       "1         distilgpt2         distilbert/distilgpt2    None               None   \n",
       "2         gpt2-large   openai-community/gpt2-large    None               None   \n",
       "3        gpt2-medium  openai-community/gpt2-medium    None               None   \n",
       "4            gpt2-xl      openai-community/gpt2-xl    None               None   \n",
       "5               gpt2         openai-community/gpt2    None               None   \n",
       "6         openai-gpt   openai-community/openai-gpt    None               None   \n",
       "7   transfo-xl-wt103   transfo-xl/transfo-xl-wt103    None               None   \n",
       "8   xlnet-base-cased        xlnet/xlnet-base-cased    None               None   \n",
       "9  xlnet-large-cased       xlnet/xlnet-large-cased    None               None   \n",
       "\n",
       "  quantization architecture languages modelCreator  licenseToUse  \\\n",
       "0         None         None      [en]         None  bsd-3-clause   \n",
       "1         None         None      [en]         None    apache-2.0   \n",
       "2         None         None      [en]         None           mit   \n",
       "3         None         None      [en]         None           mit   \n",
       "4         None         None      [en]         None           mit   \n",
       "5         None         None      [en]         None           mit   \n",
       "6         None         None      [en]         None           mit   \n",
       "7         None         None      [en]         None          None   \n",
       "8         None         None      [en]         None           mit   \n",
       "9         None         None      [en]         None           mit   \n",
       "\n",
       "                                    libraryFramework contextLength  \\\n",
       "0                        [transformers, pytorch, tf]          None   \n",
       "1  [transformers, pytorch, tf, jax, rust, safeten...          None   \n",
       "2  [transformers, pytorch, tf, jax, rust, onnx, s...          None   \n",
       "3  [transformers, pytorch, tf, jax, rust, onnx, s...          None   \n",
       "4  [transformers, pytorch, tf, jax, rust, safeten...          None   \n",
       "5  [transformers, pytorch, tf, jax, rust, onnx, s...          None   \n",
       "6     [transformers, pytorch, tf, rust, safetensors]          None   \n",
       "7                        [transformers, pytorch, tf]          None   \n",
       "8                  [transformers, pytorch, tf, rust]          None   \n",
       "9                        [transformers, pytorch, tf]          None   \n",
       "\n",
       "           developers  openSource               uri fineTuned  \\\n",
       "0        [Salesforce]        True        1909.05858      None   \n",
       "1        [distilbert]        True        1910.01108      None   \n",
       "2  [openai-community]        True        1910.09700      None   \n",
       "3  [openai-community]        True        1910.09700      None   \n",
       "4  [openai-community]        True        1910.09700      None   \n",
       "5  [openai-community]        True  10.57967/hf/0039      None   \n",
       "6  [openai-community]        True        1705.11168      None   \n",
       "7        [transfo-xl]        True        1901.02860      None   \n",
       "8             [xlnet]        True        1906.08237      None   \n",
       "9             [xlnet]        True        1906.08237      None   \n",
       "\n",
       "  carbonEmission [CO2eq tons] tokenizer  \n",
       "0                        None      None  \n",
       "1                      149200      None  \n",
       "2                        None      None  \n",
       "3                        None      None  \n",
       "4                        None      None  \n",
       "5                        None      None  \n",
       "6                        None      None  \n",
       "7                        None      None  \n",
       "8                        None      None  \n",
       "9                        None      None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 rows processed (1.826951183864367 %), elapsed time: 24.251325130462646 seconds, estimated time remaining: 1303.1692712688446 seconds\n",
      "2000 rows processed (3.653902367728734 %), elapsed time: 48.823071241378784 seconds, estimated time remaining: 1287.3668745117186 seconds\n",
      "3000 rows processed (5.480853551593102 %), elapsed time: 74.07481813430786 seconds, estimated time remaining: 1277.4449837830862 seconds\n",
      "4000 rows processed (7.307804735457468 %), elapsed time: 98.7923891544342 seconds, estimated time remaining: 1253.0826761312485 seconds\n",
      "5000 rows processed (9.134755919321835 %), elapsed time: 123.23447012901306 seconds, estimated time remaining: 1225.8379331253052 seconds\n",
      "6000 rows processed (10.961707103186203 %), elapsed time: 148.04533123970032 seconds, estimated time remaining: 1202.5229740460713 seconds\n",
      "7000 rows processed (12.78865828705057 %), elapsed time: 172.9973590373993 seconds, estimated time remaining: 1179.743168770654 seconds\n",
      "8000 rows processed (14.615609470914936 %), elapsed time: 197.33480024337769 seconds, estimated time remaining: 1152.8299085931778 seconds\n",
      "9000 rows processed (16.442560654779303 %), elapsed time: 222.79837822914124 seconds, estimated time remaining: 1132.211917682012 seconds\n",
      "10000 rows processed (18.26951183864367 %), elapsed time: 248.04521703720093 seconds, estimated time remaining: 1109.6551245346068 seconds\n",
      "11000 rows processed (20.096463022508036 %), elapsed time: 273.43541502952576 seconds, estimated time remaining: 1087.1792385959625 seconds\n",
      "12000 rows processed (21.923414206372406 %), elapsed time: 298.2346181869507 seconds, estimated time remaining: 1062.1129013376235 seconds\n",
      "13000 rows processed (23.750365390236773 %), elapsed time: 322.8593351840973 seconds, estimated time remaining: 1036.5274810035412 seconds\n",
      "14000 rows processed (25.57731657410114 %), elapsed time: 347.21837306022644 seconds, estimated time remaining: 1010.3062665993828 seconds\n",
      "15000 rows processed (27.404267757965506 %), elapsed time: 371.59530425071716 seconds, estimated time remaining: 984.3807365067801 seconds\n",
      "16000 rows processed (29.231218941829873 %), elapsed time: 395.9761381149292 seconds, estimated time remaining: 958.6582373027801 seconds\n",
      "17000 rows processed (31.05817012569424 %), elapsed time: 420.79441022872925 seconds, estimated time remaining: 934.0645866091111 seconds\n",
      "18000 rows processed (32.885121309558606 %), elapsed time: 445.19485116004944 seconds, estimated time remaining: 908.5932270694308 seconds\n",
      "19000 rows processed (34.71207249342297 %), elapsed time: 469.55399918556213 seconds, estimated time remaining: 883.1569341566186 seconds\n",
      "20000 rows processed (36.53902367728734 %), elapsed time: 494.23703718185425 seconds, estimated time remaining: 858.3908932168961 seconds\n",
      "21000 rows processed (38.365974861151706 %), elapsed time: 519.1907811164856 seconds, estimated time remaining: 834.0676327745347 seconds\n",
      "22000 rows processed (40.19292604501607 %), elapsed time: 543.7399632930756 seconds, estimated time remaining: 809.0850667991638 seconds\n",
      "23000 rows processed (42.01987722888044 %), elapsed time: 571.140792131424 seconds, estimated time remaining: 788.0749712164505 seconds\n",
      "24000 rows processed (43.84682841274481 %), elapsed time: 598.2420501708984 seconds, estimated time remaining: 766.1486614122391 seconds\n",
      "25000 rows processed (45.67377959660918 %), elapsed time: 624.6321053504944 seconds, estimated time remaining: 742.9624218807221 seconds\n",
      "26000 rows processed (47.500730780473546 %), elapsed time: 651.7379491329193 seconds, estimated time remaining: 720.3208390731812 seconds\n",
      "27000 rows processed (49.32768196433791 %), elapsed time: 677.1085870265961 seconds, estimated time remaining: 695.566070935214 seconds\n",
      "28000 rows processed (51.15463314820228 %), elapsed time: 702.0202150344849 seconds, estimated time remaining: 670.3290217642102 seconds\n",
      "29000 rows processed (52.981584332066646 %), elapsed time: 727.4977161884308 seconds, estimated time remaining: 645.6165983682829 seconds\n",
      "30000 rows processed (54.80853551593101 %), elapsed time: 753.6905353069305 seconds, estimated time remaining: 621.4429733100892 seconds\n",
      "31000 rows processed (56.63548669979538 %), elapsed time: 778.9787302017212 seconds, estimated time remaining: 596.4464300799216 seconds\n",
      "32000 rows processed (58.462437883659746 %), elapsed time: 804.3121421337128 seconds, estimated time remaining: 571.4637791881561 seconds\n",
      "33000 rows processed (60.28938906752411 %), elapsed time: 829.8796169757843 seconds, estimated time remaining: 546.614047015508 seconds\n",
      "34000 rows processed (62.11634025138848 %), elapsed time: 854.4730451107025 seconds, estimated time remaining: 521.1280324401855 seconds\n",
      "35000 rows processed (63.943291435252846 %), elapsed time: 879.8072981834412 seconds, estimated time remaining: 496.11077120650157 seconds\n",
      "36000 rows processed (65.77024261911721 %), elapsed time: 904.3149192333221 seconds, estimated time remaining: 470.6456762395435 seconds\n",
      "37000 rows processed (67.59719380298158 %), elapsed time: 929.1521122455597 seconds, estimated time remaining: 445.3903248444634 seconds\n",
      "38000 rows processed (69.42414498684595 %), elapsed time: 953.6653423309326 seconds, estimated time remaining: 420.01429434766266 seconds\n",
      "39000 rows processed (71.25109617071031 %), elapsed time: 977.8514432907104 seconds, estimated time remaining: 394.5505236006028 seconds\n",
      "40000 rows processed (73.07804735457468 %), elapsed time: 1002.9102010726929 seconds, estimated time remaining: 369.47212000751495 seconds\n",
      "41000 rows processed (74.90499853843905 %), elapsed time: 1027.4539849758148 seconds, estimated time remaining: 344.2221452195702 seconds\n",
      "42000 rows processed (76.73194972230341 %), elapsed time: 1051.747708082199 seconds, estimated time remaining: 318.9299720313663 seconds\n",
      "43000 rows processed (78.55890090616778 %), elapsed time: 1076.010056257248 seconds, estimated time remaining: 293.67567567004716 seconds\n",
      "44000 rows processed (80.38585209003215 %), elapsed time: 1100.2072041034698 seconds, estimated time remaining: 268.4505583248138 seconds\n",
      "45000 rows processed (82.21280327389651 %), elapsed time: 1124.4573559761047 seconds, estimated time remaining: 243.28259620865717 seconds\n",
      "46000 rows processed (84.03975445776088 %), elapsed time: 1149.2110073566437 seconds, estimated time remaining: 218.2501609113942 seconds\n",
      "47000 rows processed (85.86670564162524 %), elapsed time: 1173.5806262493134 seconds, estimated time remaining: 193.166377277496 seconds\n",
      "48000 rows processed (87.69365682548963 %), elapsed time: 1198.7126290798187 seconds, estimated time remaining: 168.21933965015413 seconds\n",
      "49000 rows processed (89.52060800935399 %), elapsed time: 1223.7243921756744 seconds, estimated time remaining: 143.25067603730182 seconds\n",
      "50000 rows processed (91.34755919321836 %), elapsed time: 1248.3740260601044 seconds, estimated time remaining: 118.24598822265625 seconds\n",
      "51000 rows processed (93.17451037708273 %), elapsed time: 1272.847924232483 seconds, estimated time remaining: 93.24234995303435 seconds\n",
      "52000 rows processed (95.00146156094709 %), elapsed time: 1297.180907011032 seconds, estimated time remaining: 68.25167245101929 seconds\n",
      "53000 rows processed (96.82841274481146 %), elapsed time: 1321.4178211688995 seconds, estimated time remaining: 43.282666785258165 seconds\n",
      "54000 rows processed (98.65536392867583 %), elapsed time: 1345.6734211444855 seconds, estimated time remaining: 18.341030345634177 seconds\n"
     ]
    }
   ],
   "source": [
    "df_filtered = models_df[models_df['architecture'].notna()]\n",
    "\n",
    "# Process each row\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for index, row in df_filtered.iterrows():\n",
    "    # Find the row where 'id' matches the 'architecture' of the current row\n",
    "    try:\n",
    "        matching_row = models_df[models_df['id'].astype(str) == str(row['architecture'])]\n",
    "    except ValueError:\n",
    "        print(\"architecture_value: \", row['architecture'])\n",
    "        print(row)\n",
    "        print(\"len(matching_rows): \", len(matching_row))\n",
    "        print(\"matching_rows.empty: \", matching_row.empty)\n",
    "        break\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        # Get the first developer from the 'developers' list\n",
    "        first_developer = matching_row['developers'].iloc[0][0] if matching_row['developers'].iloc[0] else None\n",
    "        \n",
    "        # Set the 'modelCreator' attribute of the original row\n",
    "        models_df.at[index, 'modelCreator'] = first_developer\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} rows processed ({count/len(df_filtered)*100} %), elapsed time: {time.time() - start_time} seconds, estimated time remaining: {(time.time() - start_time) / count * (len(df_filtered) - count)} seconds')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = models_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/alessiobuda/Documents/GitHub/chatIMPACT/database/hf_extracted_json/models_data_with_creator.json\", \"w\") as json_file:\n",
    "    json.dump(models_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'id' field if it exists\n",
    "for model in models_list:\n",
    "    model.pop('id', None)\n",
    "\n",
    "# Write to JSON file\n",
    "with open(\"/Users/alessiobuda/Documents/GitHub/chatIMPACT/database/hf_extracted_json/models_data_with_creator_no_id.json\", \"w\") as json_file:\n",
    "    json.dump(models_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def models_statistics(file_name):\n",
    "\n",
    "\tname_count = 0\n",
    "\tversion_count = 0\n",
    "\tnumber_of_parameters_count = 0\n",
    "\tquantization_count = 0\n",
    "\tarchitecture_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tmodel_creator_count = 0\n",
    "\tlicense_count = 0\n",
    "\tlibrary_count = 0\n",
    "\tcontext_length_count = 0\n",
    "\tdevelopers_count = 0\n",
    "\topen_source_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuned_count = 0\n",
    "\tcarbon_emission_count = 0\n",
    "\ttokenizer_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "\n",
    "\tmodels_json = open(os.path.join(result_path, file_name))\n",
    "\tmodels_data_json = json.load(models_json)\n",
    "\n",
    "\tmodels_df = pd.DataFrame(models_data_json) \n",
    "\n",
    "\t# TODO: add more attributes (?)\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['version'] is not None:\n",
    "\t\t\tversion_count += 1\n",
    "\t\tif item['numberOfParameters'] is not None:\n",
    "\t\t\tnumber_of_parameters_count += 1\n",
    "\t\tif item['quantization'] is not None:\n",
    "\t\t\tquantization_count += 1\n",
    "\t\tif item['architecture'] is not None:\n",
    "\t\t\tarchitecture_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\n",
    "\t\tif item['modelCreator'] is not None:\n",
    "\t\t\tmodel_creator_count += 1\n",
    "\t\tif item['licenseToUse'] is not None:\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['libraryFramework']) > 0:\n",
    "\t\t\tlibrary_count += 1\n",
    "\t\tif item['contextLength'] is not None:\n",
    "\t\t\tcontext_length_count += 1\n",
    "\t\tif len(item['developers']) > 0:\n",
    "\t\t\tdevelopers_count += 1\n",
    "\t\tif item['openSource'] is not None:\n",
    "\t\t\topen_source_count += 1\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuned'] is not None:\n",
    "\t\t\tfinetuned_count += 1\n",
    "\t\tif item['carbonEmission [CO2eq tons]'] is not None:\n",
    "\t\t\tcarbon_emission_count += 1\n",
    "\t\tif item['tokenizer'] is not None:\n",
    "\t\t\ttokenizer_count += 1\n",
    "\t\n",
    "\ttotal_models = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed models: {total_models}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Version: {version_count} ({(version_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Number of Parameters: {number_of_parameters_count} ({(number_of_parameters_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Quantization: {quantization_count} ({(quantization_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Architecture: {architecture_count} ({(architecture_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Model creator: {model_creator_count} ({(model_creator_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Library: {library_count} ({(library_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Context Length: {context_length_count} ({(context_length_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Developers: {developers_count} ({(developers_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Open Source: {open_source_count} ({(open_source_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuned: {finetuned_count} ({(finetuned_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Carbon emission: {carbon_emission_count} ({(carbon_emission_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Tokenizer: {tokenizer_count} ({(tokenizer_count / total_models) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tllm_attributes = models_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tmodel_name = item['name']\n",
    "\t\tfor attr in llm_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed models: 276660\n",
      "    Name: 276660 (100.00%)\n",
      "    Version: 0 (0.00%)\n",
      "    Number of Parameters: 0 (0.00%)\n",
      "    Quantization: 14843 (5.37%)\n",
      "    Architecture: 54736 (19.78%)\n",
      "    Languages: 59988 (21.68%)\n",
      "    Model creator: 30901 (11.17%)\n",
      "    License to use: 116065 (41.95%)\n",
      "    Library: 274520 (99.23%)\n",
      "    Context Length: 0 (0.00%)\n",
      "    Developers: 276660 (100.00%)\n",
      "    Open Source: 276660 (100.00%)\n",
      "    URI: 50638 (18.30%)\n",
      "    Fine-tuned: 54736 (19.78%)\n",
      "    Carbon emission: 1621 (0.59%)\n",
      "    Tokenizer: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = models_statistics('models_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>paperswithcode_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>card_data</th>\n",
       "      <th>siblings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amirveyseh/acronym_identification</td>\n",
       "      <td>amirveyseh</td>\n",
       "      <td>15ef643450d589d5883e289ffadeb03563e80a9e</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:39:57+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>206</td>\n",
       "      <td>19</td>\n",
       "      <td>acronym-identification</td>\n",
       "      <td>[task_categories:token-classification, annotat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ade-benchmark-corpus/ade_corpus_v2</td>\n",
       "      <td>ade-benchmark-corpus</td>\n",
       "      <td>4ba01c71687dd7c996597042449448ea312126cf</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:42:58+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>244</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_cat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLNLP/adversarial_qa</td>\n",
       "      <td>UCLNLP</td>\n",
       "      <td>c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 14:20:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>135</td>\n",
       "      <td>32</td>\n",
       "      <td>adversarialqa</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yale-LILY/aeslc</td>\n",
       "      <td>Yale-LILY</td>\n",
       "      <td>2305f2e63b68056f9b9037a3805c8c196e0d5581</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:49:13+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>132</td>\n",
       "      <td>12</td>\n",
       "      <td>aeslc</td>\n",
       "      <td>[task_categories:summarization, annotations_cr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nwu-ctext/afrikaans_ner_corpus</td>\n",
       "      <td>nwu-ctext</td>\n",
       "      <td>445834a997dce8b40e1d108638064381de80c497</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:51:47+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>103</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:token-classification, task_id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fancyzhx/ag_news</td>\n",
       "      <td>fancyzhx</td>\n",
       "      <td>eb185aade064a813bc0b7f42de02595523103ca4</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 12:02:37+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6975</td>\n",
       "      <td>124</td>\n",
       "      <td>ag-news</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>allenai</td>\n",
       "      <td>210d026faf9955653af8916fad021475a3f00453</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 15:09:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>836988</td>\n",
       "      <td>116</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google/air_dialogue</td>\n",
       "      <td>google</td>\n",
       "      <td>dbdbe7bcef8d344bc3c68a05600f3d95917d6898</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 15:22:15+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-generation, task_categor...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>komari6/ajgt_twitter_ar</td>\n",
       "      <td>komari6</td>\n",
       "      <td>af3f2fa5462ac461b696cb300d66e07ad366057f</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:58:01+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>135</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>legacy-datasets/allegro_reviews</td>\n",
       "      <td>legacy-datasets</td>\n",
       "      <td>71593d1379934286885c53d147bc863ffe830745</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:59:39+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>allegro-reviews</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                author  \\\n",
       "0   amirveyseh/acronym_identification            amirveyseh   \n",
       "1  ade-benchmark-corpus/ade_corpus_v2  ade-benchmark-corpus   \n",
       "2               UCLNLP/adversarial_qa                UCLNLP   \n",
       "3                     Yale-LILY/aeslc             Yale-LILY   \n",
       "4      nwu-ctext/afrikaans_ner_corpus             nwu-ctext   \n",
       "5                    fancyzhx/ag_news              fancyzhx   \n",
       "6                     allenai/ai2_arc               allenai   \n",
       "7                 google/air_dialogue                google   \n",
       "8             komari6/ajgt_twitter_ar               komari6   \n",
       "9     legacy-datasets/allegro_reviews       legacy-datasets   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  15ef643450d589d5883e289ffadeb03563e80a9e 2022-03-02 23:29:22+00:00   \n",
       "1  4ba01c71687dd7c996597042449448ea312126cf 2022-03-02 23:29:22+00:00   \n",
       "2  c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9 2022-03-02 23:29:22+00:00   \n",
       "3  2305f2e63b68056f9b9037a3805c8c196e0d5581 2022-03-02 23:29:22+00:00   \n",
       "4  445834a997dce8b40e1d108638064381de80c497 2022-03-02 23:29:22+00:00   \n",
       "5  eb185aade064a813bc0b7f42de02595523103ca4 2022-03-02 23:29:22+00:00   \n",
       "6  210d026faf9955653af8916fad021475a3f00453 2022-03-02 23:29:22+00:00   \n",
       "7  dbdbe7bcef8d344bc3c68a05600f3d95917d6898 2022-03-02 23:29:22+00:00   \n",
       "8  af3f2fa5462ac461b696cb300d66e07ad366057f 2022-03-02 23:29:22+00:00   \n",
       "9  71593d1379934286885c53d147bc863ffe830745 2022-03-02 23:29:22+00:00   \n",
       "\n",
       "              last_modified  private  gated  disabled  downloads  likes  \\\n",
       "0 2024-01-09 11:39:57+00:00    False  False     False        206     19   \n",
       "1 2024-01-09 11:42:58+00:00    False  False     False        244     25   \n",
       "2 2023-12-21 14:20:00+00:00    False  False     False        135     32   \n",
       "3 2024-01-09 11:49:13+00:00    False  False     False        132     12   \n",
       "4 2024-01-09 11:51:47+00:00    False  False     False        103      6   \n",
       "5 2024-03-07 12:02:37+00:00    False  False     False       6975    124   \n",
       "6 2023-12-21 15:09:48+00:00    False  False     False     836988    116   \n",
       "7 2024-03-07 15:22:15+00:00    False  False     False         77     15   \n",
       "8 2024-01-09 11:58:01+00:00    False  False     False        135      4   \n",
       "9 2024-01-09 11:59:39+00:00    False  False     False        124      4   \n",
       "\n",
       "        paperswithcode_id                                               tags  \\\n",
       "0  acronym-identification  [task_categories:token-classification, annotat...   \n",
       "1                    None  [task_categories:text-classification, task_cat...   \n",
       "2           adversarialqa  [task_categories:question-answering, task_ids:...   \n",
       "3                   aeslc  [task_categories:summarization, annotations_cr...   \n",
       "4                    None  [task_categories:token-classification, task_id...   \n",
       "5                 ag-news  [task_categories:text-classification, task_ids...   \n",
       "6                    None  [task_categories:question-answering, task_ids:...   \n",
       "7                    None  [task_categories:text-generation, task_categor...   \n",
       "8                    None  [task_categories:text-classification, task_ids...   \n",
       "9         allegro-reviews  [task_categories:text-classification, task_ids...   \n",
       "\n",
       "  card_data siblings  \n",
       "0        {}     None  \n",
       "1        {}     None  \n",
       "2        {}     None  \n",
       "3        {}     None  \n",
       "4        {}     None  \n",
       "5        {}     None  \n",
       "6        {}     None  \n",
       "7        {}     None  \n",
       "8        {}     None  \n",
       "9        {}     None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO run for all datasets\n",
    "datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "datasets_df = pd.DataFrame(datasets)\n",
    "datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
       "       'gated', 'disabled', 'downloads', 'likes', 'paperswithcode_id', 'tags',\n",
       "       'card_data', 'siblings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                   amirveyseh/acronym_identification\n",
       "author                                                      amirveyseh\n",
       "sha                           15ef643450d589d5883e289ffadeb03563e80a9e\n",
       "created_at                                   2022-03-02 23:29:22+00:00\n",
       "last_modified                                2024-01-09 11:39:57+00:00\n",
       "private                                                          False\n",
       "gated                                                            False\n",
       "disabled                                                         False\n",
       "downloads                                                          206\n",
       "likes                                                               19\n",
       "paperswithcode_id                               acronym-identification\n",
       "tags                 [task_categories:token-classification, annotat...\n",
       "card_data                                                           {}\n",
       "siblings                                                          None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_categories:question-answering',\n",
       " 'task_ids:extractive-qa',\n",
       " 'task_ids:open-domain-qa',\n",
       " 'annotations_creators:crowdsourced',\n",
       " 'language_creators:found',\n",
       " 'multilinguality:monolingual',\n",
       " 'source_datasets:original',\n",
       " 'language:en',\n",
       " 'license:cc-by-sa-4.0',\n",
       " 'size_categories:10K<n<100K',\n",
       " 'format:parquet',\n",
       " 'modality:text',\n",
       " 'library:datasets',\n",
       " 'library:pandas',\n",
       " 'library:mlcroissant',\n",
       " 'library:polars',\n",
       " 'arxiv:2002.00293',\n",
       " 'arxiv:1606.05250',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[2]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset_idx):\n",
    "\n",
    "\tdataset = datasets_df.loc[dataset_idx]\n",
    "\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset['id'])\n",
    "\tdataset_attributes['size [GB]'] = None\n",
    "\n",
    "\turl = \"https://huggingface.co/datasets/\" + dataset['id']\n",
    "\tfile_size_str = extract_file_size(url)\n",
    "\tif file_size_str:\n",
    "\t\tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t\tif file_size_gb:\n",
    "\t\t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_json(datasets_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for dataset_idx in range(datasets_df.shape[0]):\n",
    "        output.append(extract_datasets_attributes(dataset_idx))\n",
    "        if dataset_idx % 10 == 0:\n",
    "            print(f'Processed {dataset_idx} datasets, elapsed time: {time.time() - start_time:.2f} seconds')\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Dataset.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_json(datasets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes_optimized(dataset):\n",
    "\n",
    "\tdataset_tags = dataset.tags\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset.id)\n",
    "\tdataset_attributes['size [GB]'] = match_size(dataset_tags)\n",
    "\n",
    "\t# url = \"https://huggingface.co/datasets/\" + dataset.id\n",
    "\t# file_size_str = extract_file_size(url)\n",
    "\t# if file_size_str:\n",
    "\t# \tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t# \tif file_size_gb:\n",
    "\t# \t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence-similarity datasets...\n",
      "Processing summarization datasets...\n",
      "Processing text-classification datasets...\n",
      "Processing question-answering datasets...\n",
      "Processing feature-extraction datasets...\n",
      "Processing zero-shot-classification datasets...\n",
      "Processing token-classification datasets...\n",
      "Processing text-generation datasets...\n",
      "Processing translation datasets...\n",
      "Processing fill-mask datasets...\n",
      "Processing table-question-answering datasets...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'datasets_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} datasets...')\n",
    "    datasets = api.list_datasets(filter=task, full=True)\n",
    "    for dataset in datasets:\n",
    "        dataset_attributes = extract_datasets_attributes_optimized(dataset)\n",
    "        add_to_json_file(dataset_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} datasets processed, {time.time() - start_time} seconds elapsed, estimated time remaining: {(time.time() - start_time) / count * (199642 - count):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def datasets_statistics(file_name):\n",
    "\n",
    "\tname_count = 0\n",
    "\tsize_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tlicense_count = 0\n",
    "\tdomain_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuning_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "\tprint()\n",
    "\tdatasets_json = open(os.path.join(result_path, file_name))\n",
    "\tdatasets_data_json = json.load(datasets_json)\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['size [rows]'] is not None: # can be size [GB] or size [rows]\n",
    "\t\t\tsize_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\t\n",
    "\t\tif item['licenseToUse'] is not None:\t\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['domain']) > 0:\n",
    "\t\t\tdomain_count += 1\t\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuning'] is not None:\n",
    "\t\t\tfinetuning_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\t\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Size: {size_count} ({(size_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Domain: {domain_count} ({(domain_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuning: {finetuning_count} ({(finetuning_count / total_datasets) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tdatasets_attributes = datasets_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tdataset_name = item['name']\n",
    "\t\tfor attr in datasets_attributes:\n",
    "\t\t\tif item[attr] is not None and attr == 'size [GB]':\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of processed datasets: 773\n",
      "    Name: 773 (100.00%)\n",
      "    Size: 729 (94.31%)\n",
      "    Languages: 747 (96.64%)\n",
      "    License to use: 748 (96.77%)\n",
      "    Domain: 24 (3.10%)\n",
      "    URI: 107 (13.84%)\n",
      "    Fine-tuning: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "datasets_statistics('datasets_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of processed datasets: 199654\n",
      "    Name: 199654 (100.00%)\n",
      "    Size: 150976 (75.62%)\n",
      "    Languages: 22559 (11.30%)\n",
      "    License to use: 54002 (27.05%)\n",
      "    Domain: 7319 (3.67%)\n",
      "    URI: 7032 (3.52%)\n",
      "    Fine-tuning: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "datasets_statistics('datasets_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description, # TODO: text2text generation has no description\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "\n",
    "    with open(result_path + '/ChatIMPACT.DownstreamTask.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tasks_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def tasks_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tsub_task_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\ttask_json = open(os.path.join(result_path, 'ChatIMPACT.DownstreamTask.json'))\n",
    "\ttask_data_json = json.load(task_json)\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\t\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif len(item['sub-task']) > 0:\n",
    "\t\t\tsub_task_count += 1\n",
    "\t\n",
    "\ttask_count = idx + 1\n",
    "\tprint(f'Number of processed task: {idx + 1}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Sub-task: {sub_task_count} ({(sub_task_count / task_count) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\ttask_attributes = task_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in task_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability = tasks_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability.head(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "url_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "response = requests.get(url_metrics)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "h4_tags = soup.find_all('h4')\n",
    "metrics = [h4_tag.get_text(strip=True) for h4_tag in h4_tags]\n",
    "# print(metrics)\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "descriptions = [p_tag.get_text() for p_tag in p_tags]\n",
    "descriptions = descriptions[2:] # drop first lines\n",
    "# print(descriptions)\n",
    "\n",
    "# remove from the list the metrics withoud description (not useful for our purpose)\n",
    "metrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "metrics.remove('Aye10032/loss_metric')\n",
    "metrics.remove('giulio98/code_eval_outputs')\n",
    "metrics.remove('maysonma/lingo_judge_metric')\n",
    "metrics.remove('lvwerra/test')\n",
    "metrics.remove('sma2023/wil')\n",
    "\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    }
   ],
   "source": [
    "# From the lists, remove the descriptions and then the relative metric in the same index that have in the description 'TODO: add a description here\\n\\t\\t\\t\\t\\t\\t'ArithmeticError\n",
    "\n",
    "for i, description in enumerate(descriptions):\n",
    "    if 'TODO: add a description here' in description:\n",
    "        metrics.pop(i)\n",
    "        descriptions.pop(i)\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json(metrics, descriptions):\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['trained'] = None\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "    \n",
    "        metrics_data.append(metric_attributes)\n",
    "        \n",
    "\n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Metric.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json(metrics, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def metric_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tcontext_count = 0\n",
    "\tfeatureBased_endToEnd_count = 0\n",
    "\tgranularity_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tmetric_json = open(os.path.join(result_path, 'ChatIMPACT.Metric.json'))\n",
    "\tmetric_data_json = json.load(metric_json)\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif item['context'] is not None:\n",
    "\t\t\tcontext_count += 1\n",
    "\t\tif item['featureBased/endToEnd'] is not None:\n",
    "\t\t\tfeatureBased_endToEnd_count += 1\n",
    "\t\tif item['granularity'] is not None:\t\n",
    "\t\t\tgranularity_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Context: {context_count} ({(context_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    FeatureBased/endToEnd: {featureBased_endToEnd_count} ({(featureBased_endToEnd_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Granularity: {granularity_count} ({(granularity_count / total_datasets) * 100:.2f}%)')\n",
    "\t\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tmetric_attributes = metric_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in metric_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability = metric_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_relationship():\n",
    "    \n",
    "    train_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        datasets = match_dataset(model_tags)\n",
    "        \n",
    "        if not datasets:\n",
    "            continue\n",
    "        else:\n",
    "            train_relationship = dict()\n",
    "            train_relationship['Models'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            train_relationship['Datasets'] = datasets # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            train_relationship_list.append(train_relationship)\n",
    "    \n",
    "    return train_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\ttrain_relationship = extract_train_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.TrainRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(train_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_suited_for_relationship():\n",
    "    \n",
    "    suited_for_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        \n",
    "        tasks = []\n",
    "        for t in model_tags:\n",
    "            if t in TAG_DOWNSTREAM_TASK:\n",
    "                tasks.append(t)\n",
    "\n",
    "        if not tasks:\n",
    "            continue\n",
    "        else:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            suited_for_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            suited_for_relationship_list.append(suited_for_relationship)\n",
    "    \n",
    "    return suited_for_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tsuited_for_relationship = extract_suited_for_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.SuitedForRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(suited_for_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enable_relationship():\n",
    "\t\n",
    "\tenable_relationship_list = []\n",
    "\t\n",
    "\tfor dataset_idx in range(len(datasets_df)):\n",
    "\t\t\n",
    "\t\tdataset = datasets_df.loc[dataset_idx]\n",
    "\t\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\t\t\n",
    "\t\ttasks = match_tasks(dataset_tags)\n",
    "\n",
    "\t\tif not tasks:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset['id']) # {\"$oid\": <...>} for MongoDB\n",
    "\t\t\tenable_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "\t\t\tenable_relationship_list.append(enable_relationship)\n",
    "\t\n",
    "\treturn enable_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tenable_relationship = extract_enable_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EnableRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(enable_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enable_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: here https://huggingface.co/tasks some tasks have associated metrics, we could scrape the tasks one by one\n",
    "\n",
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\\n\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "    return assess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.AssessRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def extract_evaluate_relationship():\n",
    "\n",
    "\tevaluate_relationship_list = []\n",
    "\n",
    "\tfor model_idx in range(len(models_df)):\n",
    "\n",
    "\t\tmodel = models_df.loc[model_idx]\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tprint('No card data available for this model')\n",
    "\t\t\n",
    "\t\tmetrics = []\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\n",
    "\t\tif not metrics:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tevaluate_relationship = dict()\n",
    "\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model['id'])\n",
    "\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\tevaluate_relationship_list.append(evaluate_relationship)\n",
    "\t\n",
    "\treturn evaluate_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluate_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tevaluate_relationship = extract_evaluate_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EvaluateRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(evaluate_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

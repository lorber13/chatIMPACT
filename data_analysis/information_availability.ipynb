{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tags import * # tags.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>mask_token</th>\n",
       "      <th>card_data</th>\n",
       "      <th>widget_data</th>\n",
       "      <th>model_index</th>\n",
       "      <th>config</th>\n",
       "      <th>transformers_info</th>\n",
       "      <th>siblings</th>\n",
       "      <th>spaces</th>\n",
       "      <th>safetensors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert/albert-base-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>082438ba120d36b97b9288772a41144e941705b9</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:57:35+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>14172</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert/albert-base-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>8e2f239c5f8a2c0f253781ca60135db913e5c80c</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:14+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2380919</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert/albert-large-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>94fd741fb5d6cb5bc578fc154837016c583bafef</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1854</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert/albert-large-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>dfed3a5ef4499fb3351c4ebbcf487375d1e942c8</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>16199</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert/albert-xlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>ed6f87d14403b3c459a458fa6aa9dc5c51c517c1</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:28+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1300</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert/albert-xlarge-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>4fd2c2aa9aeb305f87704a7e595be7bfffa3db88</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-04-10 09:57:46+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>3787</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert/albert-xxlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>43129068ee5f6a481c148daeac11cc593b8ff440</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:42+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>4844</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert/albert-xxlarge-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>97d3e58863d3a41dc581882f73b34d110b18f1f8</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:02:09+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>8662</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google-bert/bert-base-cased-finetuned-mrpc</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>f150c1d609d1e50dd5e2e5408661cfac8339277c</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:03:21+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>51939</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google-bert/bert-base-cased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>cd5ef92a9fb2f889e972770a36d4ed042daf221e</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:02:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>5925540</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id       author  \\\n",
       "0                       albert/albert-base-v1       albert   \n",
       "1                       albert/albert-base-v2       albert   \n",
       "2                      albert/albert-large-v1       albert   \n",
       "3                      albert/albert-large-v2       albert   \n",
       "4                     albert/albert-xlarge-v1       albert   \n",
       "5                     albert/albert-xlarge-v2       albert   \n",
       "6                    albert/albert-xxlarge-v1       albert   \n",
       "7                    albert/albert-xxlarge-v2       albert   \n",
       "8  google-bert/bert-base-cased-finetuned-mrpc  google-bert   \n",
       "9                 google-bert/bert-base-cased  google-bert   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  082438ba120d36b97b9288772a41144e941705b9 2022-03-02 23:29:04+00:00   \n",
       "1  8e2f239c5f8a2c0f253781ca60135db913e5c80c 2022-03-02 23:29:04+00:00   \n",
       "2  94fd741fb5d6cb5bc578fc154837016c583bafef 2022-03-02 23:29:04+00:00   \n",
       "3  dfed3a5ef4499fb3351c4ebbcf487375d1e942c8 2022-03-02 23:29:04+00:00   \n",
       "4  ed6f87d14403b3c459a458fa6aa9dc5c51c517c1 2022-03-02 23:29:04+00:00   \n",
       "5  4fd2c2aa9aeb305f87704a7e595be7bfffa3db88 2022-03-02 23:29:04+00:00   \n",
       "6  43129068ee5f6a481c148daeac11cc593b8ff440 2022-03-02 23:29:04+00:00   \n",
       "7  97d3e58863d3a41dc581882f73b34d110b18f1f8 2022-03-02 23:29:04+00:00   \n",
       "8  f150c1d609d1e50dd5e2e5408661cfac8339277c 2022-03-02 23:29:04+00:00   \n",
       "9  cd5ef92a9fb2f889e972770a36d4ed042daf221e 2022-03-02 23:29:04+00:00   \n",
       "\n",
       "              last_modified  private  gated disabled  downloads  likes  ...  \\\n",
       "0 2024-02-19 10:57:35+00:00    False  False     None      14172      8  ...   \n",
       "1 2024-02-19 10:58:14+00:00    False  False     None    2380919     99  ...   \n",
       "2 2024-02-19 10:58:26+00:00    False  False     None       1854      3  ...   \n",
       "3 2024-02-19 10:58:48+00:00    False  False     None      16199     15  ...   \n",
       "4 2024-02-19 11:01:28+00:00    False  False     None       1300      4  ...   \n",
       "5 2024-04-10 09:57:46+00:00    False  False     None       3787      8  ...   \n",
       "6 2024-02-19 11:01:42+00:00    False  False     None       4844      5  ...   \n",
       "7 2024-02-19 11:02:09+00:00    False  False     None       8662     19  ...   \n",
       "8 2024-02-19 11:03:21+00:00    False  False     None      51939      1  ...   \n",
       "9 2024-02-19 11:02:26+00:00    False  False     None    5925540    246  ...   \n",
       "\n",
       "  pipeline_tag mask_token card_data widget_data model_index config  \\\n",
       "0    fill-mask       None      None        None        None   None   \n",
       "1    fill-mask       None      None        None        None   None   \n",
       "2    fill-mask       None      None        None        None   None   \n",
       "3    fill-mask       None      None        None        None   None   \n",
       "4    fill-mask       None      None        None        None   None   \n",
       "5    fill-mask       None      None        None        None   None   \n",
       "6    fill-mask       None      None        None        None   None   \n",
       "7    fill-mask       None      None        None        None   None   \n",
       "8    fill-mask       None      None        None        None   None   \n",
       "9    fill-mask       None      None        None        None   None   \n",
       "\n",
       "  transformers_info                                           siblings spaces  \\\n",
       "0              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "1              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "2              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "3              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "4              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "5              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "6              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "7              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "8              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "9              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "\n",
       "  safetensors  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  \n",
       "6        None  \n",
       "7        None  \n",
       "8        None  \n",
       "9        None  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = itertools.islice(models, 0, 1000)\n",
    "models_df = pd.DataFrame(model)\n",
    "models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
      "       'gated', 'disabled', 'downloads', 'likes', 'library_name', 'tags',\n",
      "       'pipeline_tag', 'mask_token', 'card_data', 'widget_data', 'model_index',\n",
      "       'config', 'transformers_info', 'siblings', 'spaces', 'safetensors'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(models_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers',\n",
       " 'pytorch',\n",
       " 'tf',\n",
       " 'safetensors',\n",
       " 'albert',\n",
       " 'fill-mask',\n",
       " 'exbert',\n",
       " 'en',\n",
       " 'dataset:bookcorpus',\n",
       " 'dataset:wikipedia',\n",
       " 'arxiv:1909.11942',\n",
       " 'license:apache-2.0',\n",
       " 'autotrain_compatible',\n",
       " 'endpoints_compatible',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag examples\n",
    "models_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers',\n",
       " 'pytorch',\n",
       " 'tensorboard',\n",
       " 'encoder-decoder',\n",
       " 'text2text-generation',\n",
       " 'generated_from_trainer',\n",
       " 'dataset:cnn_dailymail',\n",
       " 'autotrain_compatible',\n",
       " 'endpoints_compatible',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# richer tag information example\n",
    "example_model = api.list_models(model_name='albert_bert_summarization_cnn_dailymail')\n",
    "example_df = pd.DataFrame(example_model)\n",
    "example_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'zh', 'fr', 'es', 'ru', 'de', 'ja', 'ko', 'pt', 'ar', 'it', 'vi', 'tr', 'hi', 'id', 'pl', 'nl', 'th', 'cs', 'bn', 'fa', 'sv', 'ro', 'fi', 'ca', 'ta', 'da', 'hu', 'uk', 'ind', 'el', 'te', 'ur', 'bg', 'he', 'ml', 'ms', 'sl', 'mr', 'sw', 'sk', 'et', 'eu', 'kn', 'gu', 'sr', 'no', 'hr', 'lt', 'lv', 'pa', 'is', 'yo', 'am', 'vie', 'ne', 'az', 'af', 'ga', 'mt', 'si', 'gl', 'or', 'sq', 'kk', 'cy', 'tl', 'ceb', 'tha', 'as', 'mk', 'ha', 'hy', 'uz', 'my', 'ka', 'eng', 'ig', 'eo', 'be', 'nb', 'km', 'mn', 'ky', 'la', 'zu', 'so', 'min', 'jav', 'xh', 'ps', 'nn', 'rw', 'yue', 'jv', 'mya', 'tt', 'br', 'bs', 'lg', 'ckb', 'sa', 'lo', 'wo', 'ku', 'ug', 'ilo', 'sd', 'ast', 'tw', 'sun', 'tg', 'ace', 'lb', 'nso', 'gd', 'war', 'fil', 'su', 'tk', 'bug', 'oc', 'fy', 'tgl', 'sn', 'khm', 'bjn', 'gn', 'yi', 'ht', 'mai', 'bo', 'ba', 'ban', 'zlm', 'tn', 'fo', 'dv', 'kab', 'ln', 'bm', 'hin', 'ti', 'aa', 'ny', 'cv', 'shn', 'mi', 'sat', 'mg', 'lao', 'arz', 'sah', 'ee', 'ia', 'st', 'mar', 'pag', 'qu', 'hsb', 'azb', 'ab', 'vec', 'lij', 'mhr', 'sc', 'om', 'por', 'mni', 'ak', 'ts', 'als', 'tam', 'crh', 'li', 'scn', 'tpi', 'rm', 'rn', 'spa', 'fra', 'fur', 'nds', 'mad', 'tet', 'pam', 'luo', 'rus', 'pap', 'ltg', 'lmo', 'kmr', 'szl', 'fon', 'ch', 'ben', 'swh', 'hau', 'sm', 'deu', 'cmn', 'ita', 'ary', 'jpn', 'ks', 'tum', 'io', 'urd', 'mal', 'ss', 'nld', 'pcm', 'myv', 'os', 'an', 'bho', 'gom', 'kan', 'yor', 'guj', 'amh', 'pol', 'wa', 'ron', 'kor', 'pes', 'cbk', 'pan', 'dan', 'srp', 'swe', 'haw', 'zsm', 'hun', 'ki', 'tel', 'dz', 'kam', 'kg', 'lug', 'kin', 'afr', 'kbp', 'kw', 'ory', 'kea', 'udm', 'npi', 'ce', 'xho', 'vo', 'mrj', 'jbo', 'som', 'bcl', 'wuu', 'tur', 'dyu', 'pnb', 'awa', 'ibo', 'fj', 'quy', 'lit', 'nya', 'ukr', 'lus', 'umb', 'zul', 'mdf', 'co', 'heb', 'krc', 'ie', 'mos', 'zho', 'ces', 'cnh', 'pms', 'tdt', 'bem', 'hrv', 'sna', 'sg', 'dsb', 'tyv', 'fin', 'bar', 'to', 'kbd', 'sco', 'nap', 'csb', 'ay', 'wol', 'ell', 'azj', 'zgh', 'nan', 'new', 'hil', 've', 'bxr', 'tgk', 'ang', 'gv', 'bh', 'vep', 'sot', 'kac', 'bul', 'bel', 'tzm', 'gle', 'mzn', 'cym', 'kir', 'rue', 'srn', 'xmf', 'av', 'cat', 'asm', 'hne', 'ara', 'gor', 'hat', 'hak', 'bpy', 'slk', 'vls', 'ff', 'slv', 'isl', 'glk', 'ksh', 'dik', 'glg', 'xal', 'chr', 'taq', 'plt', 'lin', 'prs', 'diq', 'frr', 'kaa', 'se', 'mkd', 'mwl', 'jam', 'mnw', 'mlt', 'lez', 'brx', 'lad', 'rmy', 'hye', 'lua', 'koi', 'nob', 'kat', 'lzh', 'san', 'snd', 'got', 'apc', 'ltz', 'epo', 'bi', 'mri', 'twi', 'ydd', 'lfn', 'gaz', 'lvs', 'kaz', 'orm', 'pbt', 'skr', 'guc', 'kv', 'nv', 'ext', 'cjk', 'bbc', 'mag', 'arb', 'nov', 'msa', 'kmb', 'rup', 'bos', 'quc', 'ayr', 'mak', 'uzn', 'kl', 'gag', 'tsn', 'fas', 'frp', 'gan', 'pcd', 'aeb', 'pdc', 'oci', 'gsw', 'fuv', 'lrc', 'tso', 'lld', 'kek', 'iu', 'khk', 'nus', 'gla', 'zea', 'ty', 'cdo', 'eus', 'dty', 'hif', 'ars', 'knc', 'est', 'acq', 'tok', 'acm', 'arn', 'cak', 'uzb', 'cu', 'olo', 'kik', 'stq', 'grc', 'mam', 'inh', 'bew', 'pfl', 'lat', 'ewe', 'shi', 'swa', 'sqi', 'uig', 'za', 'nno', 'aii', 'nij', 'msb', 'aka', 'cho', 'ady', 'vot', 'mqj', 'chy', 'bam', 'nep', 'tcy', 'lbe', 'kur', 'din', 'bod', 'nia', 'pus', 'cgc', 'ae', 'vro', 'sin', 'bas', 'tig', 'dag', 'mus', 'dtp', 'sgs', 'atj', 'taj', 'mon', 'llg', 'pi', 'nhi', 'mh', 'hus', 'cr', 'arq', 'ksw', 'cbr', 'pih', 'tir', 'ik', 'yid', 'quz', 'bre', 'jiv', 'lav', 'hyw', 'bkx', 'mkn', 'nyu', 'kas', 'myk', 'ven', 'qub', 'pnt', 'smo', 'bef', 'kmu', 'amu', 'mui', 'ote', 'fuf', 'hla', 'fat', 'nas', 'chd', 'cni', 'ton', 'nyn', 'ssw', 'ami', 'tzo', 'ho', 'kok', 'agr', 'maz', 'alt', 'jra', 'acu', 'nqo', 'fuh', 'ori', 'ame', 'top', 'chk', 'nrf', 'shp', 'ng', 'quh', 'kbq', 'sxb', 'ii', 'tca', 'bzi', 'rej', 'djk', 'fao', 'mic', 'cab', 'rug', 'tnn', 'tnp', 'nfa', 'mcd', 'kr', 'hch', 'tuk', 'lbk', 'qvm', 'ino', 'mle', 'sag', 'krl', 'sps', 'pwg', 'mlg', 'qvh', 'zam', 'bis', 'tat', 'snn', 'mbb', 'toj', 'nor', 'wbp', 'sml', 'cop', 'hlt', 'qxn', 'hvn', 'kj', 'roo', 'kyq', 'kne', 'boa', 'sme', 'nsn', 'pmf', 'nwi', 'bjr', 'quf', 'srm', 'csy', 'kde', 'cbi', 'kmg', 'snk', 'wal', 'kms', 'arl', 'hnj', 'hmn', 'aze', 'cha', 'yap', 'pon', 'cbs', 'ahk', 'tbz', 'ful', 'omw', 'cbu', 'hns', 'agt', 'bgs', 'ken', 'hbo', 'bzd', 'qve', 'ctu', 'iba', 'fry', 'bss', 'lex', 'gym', 'meu', 'cbt', 'blz', 'zaw', 'dje', 'ach', 'qvc', 'kwd', 'obo', 'awb', 'dgc', 'nuj', 'enm', 'nch', 'alp', 'msk', 'ngu', 'tod', 'not', 'heg', 'na', 'tlh', 'lsi', 'mbt', 'yad', 'huu', 'kmk', 'teo', 'aia', 'wrs', 'myu', 'ptu', 'blw', 'sus', 'atd', 'smk', 'ada', 'lhu', 'bba', 'att', 'bkd', 'kri', 'yva', 'nhe', 'agn', 'cof', 'bps', 'vmw', 'auc', 'msm', 'rmc', 'atb', 'anp', 'qvn', 'mbs', 'doi', 'ded', 'gyr', 'bpr', 'ixl', 'ese', 'trv', 'xog', 'cmo', 'rop', 'smn', 'kau', 'yka', 'mps', 'bon', 'mgh', 'dhg', 'kak', 'usp', 'kpx', 'amp', 'zai', 'kqc', 'poh', 'yao', 'cub', 'bcc', 'cao', 'nst', 'qvw', 'gwi', 'bgc', 'snc', 'bbj', 'mxb', 'mgm', 'liv', 'yaa', 'ikk', 'nys', 'hmo', 'ata', 'acr', 'nhw', 'tzj', 'cuk', 'yaq', 'laj', 'crx', 'yle', 'zyp', 'gul', 'udu', 'und', 'ncj', 'opm', 'nod', 'tcz', 'mph', 'mau', 'gun', 'bzj', 'hub', 'nuy', 'dgz', 'aoj', 'suz', 'clu', 'fij', 'aaz', 'kje', 'bhg', 'myw', 'amk', 'poi', 'tbl', 'run', 'brv', 'ksd', 'tuc', 'kwi', 'pis', 'ake', 'qvs', 'kos', 'qwh', 'pib', 'mpm', 'qul', 'dzo', 'hbs', 'apu', 'bdd', 'mwf', 'piu', 'sey', 'myx', 'nhg', 'dob', 'prf', 'lgg', 'car', 'aoi', 'mcf', 'yml', 'hui', 'nbl', 'srq', 'azz', 'sgb', 'knv', 'bqc', 'bmu', 'csw', 'abx', 'cot', 'bvr', 'mhx', 'gaa', 'gum', 'amr', 'acf', 'seh', 'zap', 'mto', 'gcr', 'con', 'gvc', 'pls', 'abt', 'ddg', 'ruf', 'ura', 'alq', 'kbh', 'kkc', 'tay', 'gur', 'gub', 'ncu', 'mpx', 'poy', 'rro', 'gup', 'ktm', 'zao', 'cut', 'nak', 'cta', 'cav', 'ubu', 'spm', 'pce', 'bxh', 'kgp', 'gaw', 'kpw', 'mbc', 'kpr', 'wmt', 'msy', 'zad', 'lcm', 'ign', 'sid', 'for', 'tgp', 'ksr', 'yss', 'kwj', 'row', 'nrm', 'sab', 'fai', 'kup', 'kyz', 'zac', 'niu', 'med', 'nbq', 'dwr', 'gdn', 'kjh', 'ong', 'faa', 'kqf', 'mwe', 'mmx', 'caa', 'mcq', 'dnw', 'gux', 'trc', 'nii', 'wsk', 'wim', 'mxt', 'tpu', 'miq', 'nna', 'pwn', 'azg', 'chq', 'okv', 'nxa', 'lwl', 'sny', 'tke', 'anv', 'kwf', 'kjs', 'bbb', 'rwo', 'kpg', 'snl', 'xav', 'pau', 'bch', 'gdr', 'abs', 'kdl', 'rif', 'yet', 'tna', 'qxh', 'hto', 'dad', 'gnn', 'pma', 'zia', 'loz', 'krr', 'wmw', 'auy', 'ikw', 'tnk', 'iws', 'ghs', 'tbo', 'dga', 'klv', 'cac', 'aui', 'mva', 'mco', 'maa', 'mwc', 'too', 'geb', 'soq', 'aly', 'ood', 'avk', 'fro', 'kew', 'mkz', 'zpu', 'pao', 'poe', 'rgu', 'mvn', 'mwp', 'xla', 'avt', 'esk', 'yon', 'khz', 'zaj', 'amf', 'ppo', 'cya', 'sim', 'txu', 'wnc', 'kgk', 'ffm', 'tbc', 'mti', 'kbc', 'tos', 'zpq', 'amm', 'xtd', 'apr', 'bsp', 'ewo', 'xtm', 'tcs', 'sue', 'kvg', 'yuj', 'enq', 'bbr', 'mdy', 'kha', 'lif', 'gfk', 'myy', 'dmg', 'crs', 'snp', 'tmd', 'brb', 'hot', 'xmm', 'kpj', 'aau', 'xbi', 'bya', 'apz', 'big', 'mdr', 'guw', 'emp', 'agu', 'adz', 'viv', 'sbe', 'kjb', 'beu', 'sea', 'mpj', 'orv', 'cwe', 'aey', 'gai', 'gui', 'caf', 'kdc', 'arp', 'tvk', 'ape', 'ots', 'zpm', 'ebk', 'wms', 'gng', 'sja', 'wat', 'wuv', 'sgd', 'aso', 'djr', 'zav', 'tfr', 'nlg', 'tnc', 'swp', 'nin', 'mek', 'tte', 'ian', 'mbh', 'ziw', 'xon', 'apb', 'bla', 'tif', 'bhl', 'kyc', 'kcg', 'kmh', 'cpu', 'tgo', 'bnp', 'plu', 'tuf', 'cap', 'npl', 'khb', 'ttc', 'kaq', 'tkd', 'gvn', 'bsn', 'lac', 'leu', 'jac', 'tku', 'tbg', 'bus', 'ssx', 'buk', 'oji', 'wbi', 'wed', 'gvf', 'mjc', 'pad', 'aln', 'nhr', 'dgr', 'yua', 'aom', 'tlf', 'zos', 'lgl', 'cfm', 'etr', 'mie', 'yal', 'crn', 'cjv', 'mca', 'dww', 'bjp', 'wap', 'cax', 'tiy', 'dop', 'kqw', 'ntp', 'bal', 'usa', 'tue', 'kbm', 'agd', 'gah', 'gmv', 'pjt', 'sbk', 'zpo', 'upv', 'ubr', 'spp', 'ntu', 'mfe', 'kkl', 'guz', 'gug', 'aon', 'ncl', 'ter', 'qvi', 'zat', 'kvn', 'toc', 'guo', 'agg', 'uvl', 'nif', 'ksj', 'kue', 'yut', 'nca', 'blk', 'zpv', 'huv', 'cbv', 'mxp', 'otq', 'tiw', 'mbj', 'kql', 'lns', 'spl', 'sbs', 'sll', 'box', 'beo', 'ido', 'jid', 'cpa', 'nab', 'txq', 'gjn', 'qxo', 'hre', 'tuo', 'tbf', 'pah', 'bjk', 'yrb', 'eve', 'mzz', 'ssg', 'aai', 'glv', 'cnt', 'mih', 'gil', 'wln', 'cui', 'kpf', 'kal', 'guh', 'nou', 'rom', 'cnl', 'pri', 'mgc', 'wos', 'ekk', 'are', 'hsn', 'zar', 'emi', 'toi', 'quw', 'bco', 'cle', 'mir', 'bjz', 'daa', 'msc', 'lww', 'cso', 'rkb', 'sua', 'mbl', 'cpc', 'tew', 'ccp', 'tom', 'kmo', 'mox', 'shj', 'ngp', 'bea', 'muy', 'pdu', 'nav', 'mux', 'lid', 'kyg', 'tly', 'tnt', 'bax', 'bmr', 'zas', 'cbc', 'tav', 'srd', 'tee', 'thd', 'inb', 'boj', 'ctp', 'qug', 'far', 'tpa', 'dah', 'urt', 'cek', 'pot', 'kto', 'mop', 'xnn', 'zpl', 'moh', 'hop', 'bgt', 'lbb', 'ntj', 'mkl', 'zpz', 'yuw', 'tac', 'roh', 'bao', 'jao', 'urb', 'bmh', 'eri', 'btx', 'ptp', 'nho', 'uvh', 'zca', 'cjo', 'wbm', 'pio', 'cpb', 'bkq', 'hix', 'lim', 'uri', 'nhy', 'ina', 'spy', 'mgw', 'gcf', 'fli', 'tah', 'tar', 'eko', 'apw', 'pir', 'stp', 'met', 'zza', 'mmo', 'mil', 'mit', 'klt', 'mxq', 'mna', 'ndj', 'bvd', 'lue', 'nko', 'naf', 'knf', 'chf', 'gam', 'mcp', 'jae', 'cuc', 'nnq', 'xed', 'ajz', 'yby', 'tof', 'oki', 'bzh', 'bak', 'mpt', 'aer', 'yre', 'qup', 'cco', 'bmk', 'ycn', 'kqa', 'gvs', 'kyf', 'nhu', 'mks', 'ztq', 'agq', 'mio', 'vmy', 'awx', 'iou', 'bjv', 'kze', 'kpv', 'zaa', 'wro', 'ota', 'jni', 'kiw', 'jvn', 'pbb', 'byr', 'taw', 'mlh', 'noa', 'nyo', 'nop', 'kud', 'ipi', 'sgz', 'mib', 'zab', 'rai', 'khs', 'tsw', 'imo', 'nss', 'oss', 'fub', 'cux', 'ctd', 'gnw', 'amn', 'aby', 'byx', 'zpc', 'wiu', 'maj', 'apn', 'mig', 'tsz', 'wnu', 'amo', 'gof', 'ssd', 'mav', 'ann', 'cnr', 'mcr', 'waj', 'knj', 'jic', 'nde', 'agm', 'otm', 'rmn', 'mpp', 'fue', 'hni', 'cme', 'pab', 'wrk', 'bkm', 'unr', 'aoz', 'syc', 'tim', 'fkv', 'yom', 'gwr', 'rmq', 'div', 'nmw', 'boz', 'meq', 'swg', 'cpy', 'bri', 'tvl', 'thk', 'ckt', 'jmx', 'prg', 'kby', 'chv', 'abk', 'ivv', 'lun', 'sri', 'urw', 'grn', 'ase', 'lmp', 'kon', 'xsi', 'amx', 'vif', 'tbj', 'hig', 'ses', 'clo', 'soy', 'loh', 'nzi', 'miz', 'tpt', 'stk', 'ktu', 'naq', 'tuv', 'saq', 'ndo', 'nvm', 'kgf', 'nr', 'ojb', 'bhd', 'srr', 'atg', 'chz', 'mlp', 'ybh', 'tuz', 'mqb', 'che', 'bag', 'duu', 'hna', 'ons', 'ybb', 'aak', 'qvz', 'koo', 'bqp', 'dwy', 'vid', 'swc', 'sms', 'mup', 'gbm', 'wer', 'bfz', 'nhx', 'non', 'sma', 'men', 'mcb', 'isn', 'cos', 'zty', 'maq', 'ppl', 'bze', 'sdh', 'kiu', 'mfy', 'mah', 'chp', 'tpz', 'xsm', 'swb', 'nog', 'kqn', 'szy', 'vol', 'nhn', 'hbb', 'kkh', 'ozm', 'ivb', 'thl', 'yin', 'suk', 'ile', 'blt', 'mee', 'zdj', 'isd', 'max', 'rnl', 'yrk', 'brh', 'kbx', 'bec', 'yij', 'efi', 'fuc', 'nd', 'mfq', 'bfd', 'idu', 'pkb', 'adq', 'oj', 'bkc', 'isu', 'tob', 'mgo', 'dua', 'lhm', 'orh', 'lia', 'frm', 'bud', 'mnf', 'tkr', 'mxv', 'tdx', 'mas', 'tiv', 'the', 'mfj', 'kxv', 'mog', 'ngn', 'keo', 'pex', 'mfh', 'omb', 'kij', 'wes', 'zyb', 'cor', 'bob', 'dak', 'sas', 'mve', 'dig', 'oro', 'otk', 'shk', 'xkg', 'pov', 'nge', 'ibb', 'zsr', 'yea', 'adh', 'mrw', 'gue', 'buo', 'mot', 'bbk', 'akh', 'dao', 'plw', 'kca', 'mrn', 'tdg', 'wbr', 'tnl', 'mlw', 'hoj', 'lgr', 'arg', 'nco', 'baw', 'kua', 'syw', 'ags', 'nym', 'ain', 'akb', 'gmh', 'tio', 'kvt', 'lan', 'mne', 'kwu', 'rel', 'rar', 'otn', 'enb', 'nnd', 'nlv', 'bgf', 'aaa', 'nnb', 'dov', 'bum', 'tsb', 'gou', 'nza', 'syr', 'tzl', 'ike', 'rap', 'bxa', 'dyo', 'bfm', 'crk', 'rcf', 'cuv', 'vai', 'que', 'mzm', 'sxn', 'ppk', 'wni', 'lsm', 'gos', 'mww', 'bwx', 'tbk', 'apy', 'nlc', 'tpl', 'bin', 'kfm', 'bwt', 'rhg', 'ify', 'mhl', 'gbi', 'sbd', 'ybi', 'lkt', 'bts', 'qvo', 'mxu', 'alz', 'bci', 'kbo', 'bra', 'ktz', 'evn', 'lbr', 'mlk', 'dug', 'tvs', 'hup', 'tzh', 'luc', 'kdj', 'ium', 'xmg', 'lki', 'nlx', 'csa', 'dwu', 'myb', 'lew', 'gia', 'lob', 'ena', 'bgr', 'mvp', 'sda', 'bgn', 'bru', 'sdk', 'ifu', 'kky', 'mxx', 'tdb', 'zom', 'bgg', 'kng', 'kwx', 'mwn', 'pnz', 'nyy', 'abz', 'rki', 'nce', 'gkp', 'smj', 'pcg', 'kdh', 'ssn', 'mej', 'sga', 'aeu', 'her', 'aym', 'gqr', 'xbr', 'ifk', 'xty', 'nyq', 'miy', 'tab', 'rub', 'ceg', 'cag', 'pui', 'dip', 'knn', 'gpe', 'gaq', 'ifa', 'gdg', 'mta', 'xsr', 'bqi', 'kum', 'afb', 'kyu', 'mwv', 'hrx', 'bgz', 'krj', 'wwa', 'mor', 'guk', 'kqs', 'lzz', 'kqe', 'ava', 'ttj', 'mbi', 'kdi', 'nla', 'enx', 'src', 'pbu', 'syl', 'bxk', 'mnb', 'mbd', 'kln', 'vun', 'mhw', 'akl', 'shy', 'bwo', 'hnn', 'byn', 'ljp', 'ddn', 'due', 'pck', 'hac', 'czt', 'dts', 'nba', 'nci', 'ngl', 'mjw', 'tgj', 'bhp', 'hz', 'ahr', 'yas', 'qxu', 'pdt', 'tby', 'rtm', 'bht', 'ijs', 'akk', 'iku', 'mgg', 'sbl', 'kck', 'npy', 'iii', 'mtg', 'nbe', 'mwm', 'dgo', 'btd', 'hwc', 'itv', 'chj', 'tlb', 'lah', 'zne', 'twu', 'bhs', 'shs', 'fvr', 'kru', 'hms', 'cjs', 'aar', 'bik', 'plg', 'hoc', 'mnk', 'sei', 'zro', 'cku', 'rjs', 'peg', 'mhi', 'kfo', 'nag', 'ryu', 'mer', 'btt', 'acn', 'odk', 'mmn', 'slr', 'rmb', 'lot', 'muv', 'udg', 'bfa', 'guu', 'njo', 'mzi', 'kxp', 'ykg', 'ajg', 'kwn', 'tem', 'nku', 'cri', 'xcl', 'mzh', 'thy', 'amc', 'lhi', 'mqy', 'mbf', 'moc', 'syb', 'bua', 'nit', 'bom', 'ahg', 'kbr', 'nsa', 'ute', 'zin', 'raw', 'mqu', 'aqt', 'cnw', 'dbq', 'nbc', 'ldi', 'muh', 'rui', 'yli', 'yns', 'kod', 'klb', 'khw', 'bfj', 'sjn', 'kqi', 'bdq', 'sch', 'bsk', 'pem', 'tao', 'cyb', 'bez', 'kzf', 'cce', 'kxm', 'bdh', 'tls', 'erk', 'acw', 'ngt', 'bni', 'hea', 'isk', 'kff', 'bgj', 'tfn', 'koh', 'msn', 'zak', 'sro', 'dis', 'sop', 'sby', 'way', 'shb', 'biy', 'kfs', 'nmn', 'dma', 'asa', 'haq', 'ihp', 'trp', 'trf', 'ksp', 'hag', 'bou', 'njm', 'alj', 'kmm', 'dnj', 'tts', 'gej', 'rag', 'bhw', 'coe', 'bbw', 'tui', 'ogc', 'ghl', 'qxs', 'szb', 'pmq', 'bfw', 'tmf', 'kuj', 'skg', 'mul', 'dcr', 'mip', 'sgc', 'tli', 'nyb', 'enl', 'lmn', 'qus', 'nid', 'dar', 'ker', 'xtc', 'chm', 'dib', 'lub', 'bmv', 'njn', 'gbk', 'esi', 'sba', 'nmc', 'bzt', 'kxj', 'nph', 'ikx', 'mfi', 'lti', 'trn', 'wno', 'nih', 'lbm', 'mif', 'bqt', 'khg', 'aol', 'lam', 'xkv', 'aim', 'shu', 'uzs', 'tih', 'tsi', 'avu', 'bji', 'ayo', 'tpn', 'gqa', 'kun', 'tek', 'ssp', 'pst', 'snw', 'irk', 'esu', 'eip', 'kyh', 'wsg', 'bno', 'rmo', 'buw', 'ifb', 'kix', 'end', 'mrh', 'kus', 'zpi', 'xmv', 'hdy', 'bot', 'mtp', 'gby', 'goh', 'pum', 'adi', 'aro', 'miu', 'mns', 'hdn', 'klr', 'bej', 'heh', 'kks', 'mix', 'mgu', 'mua', 'crj', 'lui', 'skt', 'lev', 'djm', 'mrq', 'see', 'guq', 'tll', 'nnw', 'cay', 'shg', 'sck', 'kit', 'kjd', 'old', 'mjg', 'lag', 'mck', 'yur', 'qva', 'ale', 'lbj', 'xdy', 'bjg', 'lut', 'frc', 'sef', 'wlv', 'mgr', 'urk', 'nau', 'arr', 'bvz', 'dru', 'hay', 'dyi', 'nim', 'ndh', 'rad', 'kmw', 'leh', 'mvv', 'lea', 'njz', 'gww', 'sil', 'pqm', 'emk', 'spn', 'nnp', 'laa', 'ngj', 'ldn', 'suj', 'sgw', 'yan', 'gyd', 'bdv', 'chb', 'tvu', 'nyh', 'kjc', 'ebu', 'kpz', 'crt', 'nut', 'ttv', 'mov', 'ort', 'duo', 'kwv', 'moz', 'kfx', 'tap', 'maw', 'anu', 'did', 'mfm', 'sux', 'aqz', 'xte', 'abq', 'wca', 'bkl', 'ksc', 'fit', 'pai', 'bva', 'jit', 'nbu', 'ekg', 'cnk', 'mla', 'ctz', 'nfu', 'yak', 'msi', 'rim', 'ktb', 'mzj', 'hra', 'tdj', 'gyn', 'kgr', 'mur', 'wau', 'any', 'snf', 'mmy', 'bbq', 'sdc', 'sgh', 'mwq', 'iso', 'tdc', 'zun', 'cgg', 'jya', 'xsb', 'otd', 'suc', 'trq', 'oym', 'hoy', 'chu', 'bim', 'xnr', 'waw', 'qxl', 'mbq', 'srb', 'bku', 'dng', 'hmr', 'lud', 'tsc', 'wdd', 'cld', 'rwr', 'apt', 'tnr', 'thf', 'kpo', 'tog', 'izh', 'cas', 'lai', 'tqo', 'btm', 'thq', 'chn', 'gnd', 'bje', 'tlj', 'pne', 'kye', 'biu', 'bpx', 'nyk', 'soz', 'mde', 'lme', 'odu', 'mnx', 'nmz', 'ttq', 'bkw', 'jaa', 'lmk', 'tvt', 'adj', 'ndc', 'zag', 'nmf', 'won', 'jup', 'trs', 'mrz', 'dws', 'gwd', 'alh', 'lis', 'yrl', 'haz', 'pny', 'nmm', 'ikt', 'lnd', 'ocu', 'unk', 'lwg', 'tsg', 'agw', 'wic', 'kfc', 'mat', 'shr', 'mim', 'zoh', 'one', 'bcj', 'pse', 'rml', 'bnj', 'chw', 'ciw', 'rwk', 'shh', 'urh', 'sac', 'dav', 'ksb', 'mjl', 'cla', 'pak', 'ibg', 'ntk', 'egl', 'atq', 'mye', 'unx', 'kgo', 'yae', 'zyg', 'jmc', 'dhv', 'iry', 'ggw', 'icr', 'meo', 'sbp', 'mzp', 'nzm', 'kjp', 'bwd', 'pmy', 'nio', 'ury', 'rin', 'kap', 'bhb', 'geg', 'caz', 'uki', 'gde', 'wtm', 'tsv', 'bft', 'srl', 'tii', 'iby', 'xks', 'kmt', 'ram', 'bnx', 'gru', 'kvv', 'dni', 'noz', 'kbj', 'iwm', 'bvm', 'eka', 'jun', 'qui', 'gvo', 'itd', 'cra', 'kih', 'mtf', 'vas', 'hul', 'weo', 'buh', 'qxq', 'iko', 'zlj', 'mhk', 'duc', 'arw', 'yey', 'kdr', 'psi', 'chx', 'cll', 'ida', 'kvy', 'nnc', 'umm', 'thv', 'apd', 'xwg', 'ydg', 'wls', 'vkl', 'nja', 'lor', 'bhf', 'czh', 'kqy', 'nmb', 'ish', 'bli', 'kdz', 'pnq', 'ank', 'mgf', 'bdi', 'var', 'hgm', 'tbt', 'tcc', 'iyx', 'com', 'wob', 'ogb', 'azd', 'ipo', 'lch', 'duw', 'sse', 'tkg', 'bzy', 'mep', 'hol', 'sau', 'bzf', 'tqu', 'tus', 'oma', 'izr', 'noe', 'utr', 'bmm', 'nda', 'aao', 'kst', 'fpe', 'lbx', 'agh', 'ckx', 'ayz', 'lma', 'mte', 'dsh', 'igs', 'swl', 'anc', 'qwa', 'akf', 'akp', 'ekl', 'ora', 'int', 'bpn', 'lmg', 'pei', 'isi', 'aog', 'tye', 'apm', 'rau', 'niw', 'loy', 'luj', 'wji', 'kvq', 'poc', 'xmh', 'tix', 'iqw', 'cpx', 'kef', 'xrw', 'sbr', 'skx', 'bdb', 'drg', 'lep', 'ruk', 'bcf', 'pht', 'bbt', 'eky', 'bjc', 'los', 'tlr', 'sdp', 'yaf', 'tro', 'ncb', 'bhr', 'win', 'nuk', 'gra', 'qux', 'slp', 'tmr', 'aup', 'ppq', 'ite', 'gae', 'kfa', 'xvi', 'bfq', 'liu', 'jru', 'bxb', 'avn', 'djc', 'knp', 'pko', 'tpe', 'lkr', 'lgu', 'bte', 'lmu', 'opa', 'lva', 'nyi', 'zng', 'pbp', 'bws', 'tva', 'ywq', 'wrm', 'elk', 'brq', 'tik', 'fut', 'swv', 'soe', 'cae', 'sym', 'wle', 'yis', 'jow', 'kbb', 'stj', 'agl', 'mmc', 'huh', 'lae', 'pic', 'jna', 'uhn', 'pha', 'bwm', 'ged', 'bth', 'wgi', 'ngb', 'kfz', 'plr', 'itz', 'sad', 'tji', 'ano', 'gax', 'gua', 'ncm', 'fab', 'nnh', 'afo', 'dtb', 'kmz', 'dmw', 'twf', 'biv', 'pbo', 'ral', 'sif', 'zpn', 'dva', 'gvj', 'ebo', 'kcd', 'tiq', 'yra', 'aoa', 'nbh', 'mez', 'jgk', 'scs', 'bqv', 'nke', 'stf', 'arv', 'rnd', 'twy', 'fqs', 'mrt', 'vaf', 'knx', 'zoc', 'rab', 'irx', 'ngz', 'tow', 'quv', 'vem', 'sjo', 'mtk', 'rgs', 'deg', 'nuf', 'nhd', 'bmi', 'alw', 'knk', 'dny', 'pua', 'juk', 'mbm', 'emb', 'kzq', 'coz', 'ghn', 'alx', 'lok', 'cqd', 'les', 'poo', 'smw', 'wli', 'wmb', 'kof', 'hid', 'oku', 'bol', 'otr', 'txt', 'xom', 'raj', 'khu', 'mtr', 'kjr', 'szp', 'msj', 'kwa', 'kma', 'zim', 'mzk', 'jum', 'mct', 'diu', 'tti', 'erh', 'pre', 'tdh', 'aec', 'abr', 'mmh', 'psa', 'ndv', 'guf', 'kcx', 'kqj', 'pww', 'tsj', 'klw', 'kbl', 'kyo', 'dcc', 'gvp', 'kom', 'tcf', 'lkn', 'nnu', 'tmy', 'aif', 'bhq', 'dya', 'kul', 'tdo', 'bif', 'mfd', 'ats', 'byp', 'bgi', 'mwi', 'scl', 'pac', 'liz', 'svs', 'nbb', 'ayg', 'gdu', 'slc', 'neb', 'sns', 'jax', 'gwt', 'aee', 'yup', 'puu', 'nlo', 'bhy', 'hux', 'gdx', 'keb', 'vmp', 'aiw', 'prx', 'gol', 'tgs', 'wry', 'jbj', 'eto', 'zay', 'afh', 'bwq', 'gvr', 'zhi', 'ahl', 'dij', 'ndu', 'knt', 'gnb', 'gid', 'tra', 'mjt', 'bnv', 'mrp', 'caq', 'lgq', 'dim', 'gwn', 'kui', 'atk', 'dgi', 'mlm', 'mnp', 'pcn', 'mcs', 'ega', 'mym', 'hts', 'kfp', 'qun', 'ekr', 'cly', 'jaq', 'kpk', 'pll', 'fmp', 'tpp', 'tbp', 'ysn', 'lup', 'onp', 'ktv', 'agb', 'jiu', 'agf', 'dgh', 'tlp', 'zbc', 'sjr', 'ael', 'clk', 'mzb', 'dow', 'nux', 'ulu', 'bet', 'grs', 'bey', 'dnn', 'bio', 'hml', 'sjl', 'moj', 'wbf', 'job', 'sru', 'lbn', 'kib', 'krh', 'gek', 'pca', 'jqr', 'bmf', 'ekp', 'mlv', 'sdq', 'akc', 'pwm', 'spo', 'wlc', 'iri', 'khe', 'nms', 'vah', 'xra', 'der', 'bdw', 'kol', 'mgl', 'kkk', 'dms', 'wom', 'kei', 'kgb', 'itr', 'say', 'blr', 'mki', 'vaj', 'tty', 'lem', 'ass', 'hed', 'sip', 'ner', 'uar', 'ijn', 'ldm', 'ybe', 'ksf', 'vag', 'lna', 'grx', 'doz', 'ksg', 'bda', 'yoy', 'kao', 'bqw', 'lyn', 'bsi', 'sbc', 'kni', 'twb', 'ilu', 'yog', 'knd', 'nfd', 'esh', 'bfy', 'kjq', 'lig', 'une', 'plv', 'yah', 'kxc', 'asr', 'crq', 'aaw', 'duq', 'hnd', 'nps', 'xmz', 'ijj', 'kgj', 'ldl', 'cfa', 'ril', 'sel', 'las', 'nar', 'bpu', 'inj', 'qya', 'gju', 'mug', 'kfy', 'nbp', 'iru', 'dee', 'mxe', 'buf', 'tsu', 'bjh', 'jer', 'sgr', 'bbi', 'acv', 'anm', 'yaz', 'akw', 'yig', 'pwa', 'ayn', 'yuf', 'xuu', 'xkl', 'xkb', 'aik', 'mrr', 'juo', 'dna', 'xmw', 'mey', 'tpx', 'kcr', 'gow', 'mln', 'gni', 'bkv', 'lul', 'org', 'mme', 'ztg', 'etu', 'lou', 'keu', 'lmy', 'kai', 'bpw', 'mae', 'kvb', 'zyj', 'ksi', 'saw', 'wod', 'bbf', 'xnz', 'lvk', 'nat', 'mkc', 'iar', 'tnm', 'mnl', 'sso', 'sce', 'tww', 'ncr', 'mqz', 'wlo', 'orx', 'ncx', 'hal', 'nhb', 'tdf', 'mho', 'moi', 'rkm', 'tuq', 'ttr', 'kna', 'nga', 'thr', 'sst', 'pru', 'cbg', 'ccg', 'bvw', 'asy', 'mtd', 'msh', 'kxz', 'dai', 'lsr', 'kyk', 'cbn', 'naj', 'akg', 'xri', 'kxn', 'pia', 'mlu', 'pip', 'cul', 'bys', 'sax', 'ppi', 'ijc', 'sjm', 'syk', 'kpe', 'nhv', 'wut', 'oub', 'tce', 'iti', 'ati', 'tan', 'muo', 'plc', 'ywa', 'xns', 'psn', 'abo', 'tcx', 'kcl', 'adx', 'daq', 'gar', 'lbu', 'sdg', 'biz', 'akd', 'bee', 'nuo', 'gvl', 'idi', 'led', 'ayu', 'jms', 'xmt', 'fal', 'asi', 'rji', 'skv', 'tma', 'gcd', 'ttw', 'ibd', 'zpx', 'nfr', 'qvj', 'hur', 'cjp', 'btu', 'thz', 'kfr', 'kxh', 'krp', 'zpw', 'kfk', 'awi', 'ndy', 'cfd', 'sui', 'coc', 'dks', 'meh', 'ito', 'drs', 'iyo', 'kub', 'bhz', 'dur', 'ahb', 'aul', 'mbx', 'hae', 'bpp', 'sed', 'emg', 'snv', 'mbo', 'pym', 'ilb', 'yam', 'dgg', 'dus', 'bcr', 'cro', 'mjs', 'kjg', 'lar', 'lum', 'tsx', 'uiv', 'wsa', 'ymb', 'tdy', 'yyu', 'lu', 'cto', 'alu', 'gno', 'cjm', 'azm', 'mkw', 'nfl', 'yaw', 'wwo', 'ria', 'cbd', 'smy', 'mtj', 'bsf', 'ert', 'ndx', 'lgt', 'ksn', 'bby', 'saz', 'bzz', 'mvo', 'mzq', 'pmx', 'alk', 'ayi', 'cbj', 'czn', 'ndb', 'sgi', 'soc', 'ssb', 'asc', 'mtu', 'pwo', 'bye', 'mke', 'hit', 'ngs', 'ula', 'mxn', 'ife', 'fap', 'lef', 'kwl', 'jbu', 'giw', 'grh', 'hia', 'mtt', 'mnm', 'tpr', 'kvf', 'twx', 'ler', 'ndd', 'bja', 'tex', 'beq', 'phk', 'mhz', 'haa', 'ndm', 'tcn', 'anx', 'llp', 'res', 'mpg', 'bww', 'kvm', 'kpq', 'naz', 'njb', 'ncf', 'wan', 'ygr', 'sqq', 'hvv', 'fod', 'ttk', 'wdj', 'zaz', 'wbl', 'weh', 'cvn', 'gwa', 'sha', 'sie', 'mdb', 'ney', 'sne', 'cch', 'lyg', 'zyn', 'mng', 'ets', 'moe', 'ywn', 'kys', 'bwe', 'owi', 'tlx', 'uya', 'whk', 'nyj', 'mmp', 'shw', 'nxg', 'tsr', 'agx', 'alf', 'bvi', 'jeh', 'set', 'zkr', 'tkl', 'qud', 'gbe', 'ung', 'dmo', 'rwa', 'ghk', 'dtm', 'hru', 'thm', 'adn', 'zmp', 'bcw', 'gea', 'tei', 'lky', 'tri', 'ssy', 'kfq', 'vra', 'kxw', 'bmd', 'nri', 'ado', 'lnu', 'yuy', 'kfi', 'mcw', 'abu', 'ckl', 'chl', 'yes', 'aio', 'eme', 'dry', 'brg', 'xub', 'kia', 'cdn', 'brd', 'mnz', 'lih', 'muk', 'jul', 'cko', 'bcq', 'rbb', 'mfn', 'tgd', 'lcp', 'akq', 'nka', 'kcv', 'sbb', 'ebr', 'bor', 'prk', 'nil', 'bnn', 'huf', 'vmk', 'bkr', 'mpe', 'nng', 'oka', 'coj', 'lwo', 'nlu', 'avd', 'ala', 'mlq', 'lol', 'mef', 'cdj', 'tlq', 'bap', 'lcc', 'mhc', 'sry', 'tgc', 'kgq', 'lop', 'zgb', 'bun', 'byd', 'bmb', 'wib', 'khn', 'bep', 'wsi', 'lht', 'lse', 'boq', 'cog', 'jei', 'goa', 'lra', 'bcg', 'txy', 'kem', 'myh', 'pbs', 'bzk', 'mbv', 'glo', 'fan', 'nyd', 'gcn', 'jeb', 'jen', 'kdq', 'gkn', 'gaj', 'dio', 'krs', 'duv', 'ncg', 'bfg', 'tbw', 'mev', 'mjx', 'moa', 'mdm', 'hut', 'had', 'aty', 'csh', 'mma', 'hum', 'kel', 'rhp', 'lil', 'prm', 'bcp', 'brp', 'aot', 'bab', 'glj', 'mnv', 'nxr', 'aap', 'lec', 'mue', 'lap', 'bly', 'krf', 'kwe', 'aac', 'wrp', 'fie', 'sti', 'tkp', 'dri', 'bza', 'pbi', 'eot', 'kwo', 'giz', 'orc', 'bil', 'gdl', 'kmy', 'rav', 'lga', 'krx', 'lbw', 'bqj', 'tdk', 'dmr', 'ldb', 'mwa', 'kxb', 'vut', 'ble', 'jmb', 'kvr', 'ior', 'mfl', 'bky', 'kml', 'etn', 'mmd', 'jpa', 'fir', 'mbz', 'mlx', 'baa', 'aqm', 'mtq', 'bqs', 'lee', 'cia', 'tul', 'gya', 'anw', 'jml', 'xem', 'mlf', 'mxm', 'lur', 'scg', 'gga', 'tsa', 'sur', 'klu', 'doa', 'siw', 'zpr', 'mkg', 'nbr', 'bub', 'bsy', 'afe', 'hmt', 'lek', 'aal', 'can', 'glw', 'nbn', 'pos', 'hoo', 'mrl', 'lmx', 'aca', 'ald', 'adl', 'mbp', 'wyy', 'abi', 'mpd', 'nkx', 'kuy', 'ndr', 'erg', 'mfc', 'diz', 'dih', 'hca', 'enn', 'tmq', 'ica', 'llc', 'mrm', 'gew', 'psh', 'mrf', 'klz', 'avi', 'kis', 'vaa', 'pbv', 'suv', 'jmd', 'kbz', 'cdm', 'iqu', 'lpa', 'khc', 'tft', 'bcz', 'haj', 'raf', 'mfv', 'jaf', 'lla', 'ssk', 'suq', 'loa', 'oks', 'mro', 'umu', 'xes', 'bst', 'mgb', 'doo', 'nzk', 'cbo', 'dge', 'ems', 'uba', 'ths', 'sho', 'kci', 'fak', 'brr', 'tkq', 'yui', 'uuu', 'tdv', 'eyo', 'brf', 'kwb', 'arh', 'sbu', 'mij', 'wmd', 'loe', 'aha', 'pfe', 'brl', 'tuy', 'kpl', 'vay', 'crc', 'pbg', 'ldj', 'toh', 'cfg', 'sly', 'scp', 'jnj', 'cob', 'kpc', 'kny', 'bdu', 'sgp', 'bau', 'mls', 'mvz', 'tba', 'fla', 'kzr', 'udl', 'plk', 'aez', 'leq', 'gog', 'she', 'gnm', 'kvd', 'mgk', 'rir', 'sek', 'fuy', 'vmz', 'kee', 'kfd', 'bpv', 'glh', 'kza', 'nnj', 'auu', 'mhy', 'kli', 'hoe', 'psw', 'kay', 'swj', 'awe', 'mmm', 'bei', 'dox', 'dsq', 'kjl', 'pnu', 'hlb', 'gut', 'esg', 'ola', 'yer', 'kji', 'wad', 'bxq', 'jib', 'cns', 'moy', 'oia', 'aun', 'svb', 'kzi', 'ngc', 'sor', 'byv', 'bbv', 'sug', 'mfb', 'bzx', 'oyb', 'njj', 'pkg', 'ttm', 'prc', 'oke', 'tgw', 'slu', 'awn', 'nez', 'bcy', 'kwy', 'cdh', 'dhm', 'qxr', 'neq', 'kie', 'dbm', 'kio', 'drd', 'mji', 'msw', 'bgv', 'kla', 'sde', 'knm', 'zkd', 'xta', 'ndz', 'ggu', 'mzw', 'cdr', 'swo', 'bsb', 'teq', 'izz', 'ckh', 'shq', 'nre', 'coh', 'nal', 'suy', 'ktn', 'swi', 'sky', 'mgv', 'pil', 'sgy', 'bgp', 'hav', 'nkb', 'cwt', 'jns', 'hmb', 'aks', 'bvc', 'aqg', 'doy', 'bca', 'dgd', 'mnj', 'fay', 'txa', 'iki', 'dir', 'gnk', 'nbv', 'szv', 'nac', 'pcj', 'bav', 'zik', 'pdo', 'dax', 'huc', 'kzs', 'blq', 'flr', 'xky', 'pof', 'knw', 'zae', 'skb', 'cyo', 'bsc', 'tal', 'khq', 'gel', 'hmd', 'msl', 'blb', 'kwk', 'hue', 'wof', 'mvf', 'jma', 'ndi', 'etx', 'mse', 'ppm', 'lbq', 'pga', 'vam', 'mgi', 'maf', 'bdm', 'ghc', 'sam', 'yuz', 'jmr', 'dun', 'ema', 'kez', 'mfg', 'mea', 'xer', 'age', 'kno', 'bqr', 'yix', 'bib', 'sev', 'bhj', 'lsh', 'tug', 'lle', 'siu', 'tol', 'dta', 'txn', 'tyn', 'kfe', 'grd', 'gbr', 'lom', 'bid', 'aof', 'kcf', 'bcs', 'pcb', 'bzu', 'sou', 'mzr', 'cje', 'nbm', 'auk', 'okr', 'mcu', 'dia', 'jge', 'bfu', 'eja', 'klx', 'tdd', 'sbh', 'twm', 'kqo', 'dos', 'kxf', 'tvd', 'myp', 'cmr', 'cdf', 'lml', 'bde', 'ore', 'zrs', 'gna', 'msg', 'eza', 'kws', 'vor', 'app', 'png', 'mzv', 'kil', 'oni', 'ibl', 'eit', 'sob', 'bqg', 'tcd', 'nen', 'ogo', 'pid', 'pbn', 'tis', 'kdu', 'svc', 'mfz', 'sto', 'abn', 'ver', 'wci', 'hoa', 'jio', 'xpe', 'gxx', 'kvo', 'zts', 'ymm', 'osi', 'mza', 'fng', 'gmb', 'ybj', 'lro', 'blm', 'kgy', 'scu', 'bwu', 'nti', 'sxw', 'jub', 'mmg', 'lje', 'bzv', 'kmq', 'gmm', 'otw', 'mtl', 'jig', 'igl', 'yba', 'hbn', 'bta', 'zln', 'nsu', 'dof', 'git', 'sen', 'wnp', 'myl', 'kaj', 'zps', 'twp', 'kdp', 'tau', 'wbk', 'lic', 'ugo', 'vmm', 'ctl', 'vnk', 'hah', 'hno', 'kex', 'onn', 'sao', 'bdl', 'bxg', 'nrg', 'ykm', 'nup', 'siy', 'oyd', 'niq', 'dem', 'kvj', 'afi', 'rao', 'ilp', 'djn', 'kvu', 'mch', 'naw', 'fll', 'nkk', 'gdb', 'sir', 'mpq', 'ono', 'ilk', 'bcv', 'log', 'bpz', 'iow', 'kbv', 'ask', 'dil', 'cvg', 'kmn', 'muz', 'bks', 'sge', 'sss', 'ato', 'all', 'mfo', 'nir', 'pmm', 'raa', 'zwa', 'kga', 'epi', 'kdx', 'nkh', 'bwi', 'mhu', 'bsh', 'wbj', 'phl', 'acd', 'dot', 'dbb', 'mbu', 'tdn', 'mda', 'nwb', 'tld', 'wgb', 'gnu', 'stv', 'yum', 'afu', 'mcn', 'zaf', 'aki', 'rol', 'dme', 'tds', 'bbp', 'kmd', 'gri', 'fip', 'sep', 'ifm', 'ige', 'snq', 'ctg', 'vrs', 'rmt', 'kzc', 'mgd', 'cja', 'sng', 'bzw', 'mwg', 'mkf', 'ofu', 'pkt', 'hwo', 'nev', 'pqa', 'mmz', 'har', 'kkz', 'emn', 'onj', 'orz', 'xod', 'yun', 'mdj', 'sku', 'ntm', 'szg', 'mdt', 'beh', 'sol', 'mdw', 'pss', 'dhi', 'llu', 'gpa', 'ums', 'lip', 'kge', 'hio', 'gop', 'yki', 'bbu', 'nnm', 'nqg', 'clc', 'xkn', 'jku', 'niz', 'sys', 'aji', 'nto', 'fud', 'kdm', 'kcj', 'soj', 'cdz', 'ich', 'nzy', 'tdl', 'frd', 'rah', 'wlx', 'lkh', 'kss', 'spu', 'trd', 'nud', 'yim', 'khj', 'jle', 'kzm', 'smq', 'jmi', 'yll', 'blc', 'law', 'nxq', 'sya', 'pmi', 'aab', 'xtn', 'dor', 'niy', 'ksm', 'kic', 'mga', 'tru', 'jdt', 'bux', 'tmc', 'gbg', 'asu', 'nma', 'kps', 'bns', 'axk', 'dre', 'qxp', 'kog', 'tpm', 'cik', 'puo', 'anj', 'hkk', 'sok', 'xok', 'was', 'mql', 'has', 'diw', 'kdd', 'ybl', 'gis', 'twh', 'ccj', 'mfa', 'ktp', 'qxa', 'ldk', 'gab', 'lbo', 'twe', 'khl', 'yev', 'mxy', 'kqb', 'scw', 'kfb', 'bkk', 'sos', 'wja', 'khy', 'sbg', 'gad', 'ayb', 'cua', 'gdf', 'kid', 'nyf', 'sdo', 'aad', 'foi', 'bit', 'dbj', 'ade', 'amt', 'sts', 'mdh', 'mrg', 'vig', 'crl', 'gbz', 'bex', 'cod', 'add', 'kle', 'liq', 'brt', 'piv', 'uta', 'bhh', 'taz', 'lln', 'tyz', 'wew', 'bgd', 'cli', 'ycl', 'bnm', 'kwg', 'asb', 'khr', 'kmc', 'sjg', 'tyr', 'ukp', 'vap', 'grt', 'dbn', 'iai', 'nnz', 'bfh', 'nli', 'zns', 'hve', 'thp', 'ess', 'xsu', 'bjt', 'bwr', 'mpc', 'agc', 'wti', 'gaf', 'byz', 'ego', 'yay', 'nmh', 'cnb', 'sig', 'njh', 'puc', 'crw', 'dby', 'nbi', 'prn', 'yot', 'anf', 'dzg', 'ott', 'bxl', 'cry', 'skj', 'blf', 'bsq', 'ppt', 'tmn', 'arx', 'scv', 'kvw', 'bcn', 'kph', 'daw', 'elm', 'tes', 'zmb', 'mds', 'apj', 'ynq', 'peb', 'bek', 'pek', 'klq', 'wmo', 'koe', 'hei', 'bfs', 'nmk', 'pku', 'aix', 'kdt', 'gim', 'mqn', 'nru', 'akr', 'aba', 'pcc', 'kht', 'gud', 'rog', 'zzj', 'rey', 'txo', 'ztp', 'lax', 'nlj', 'ngi', 'mqx', 'tpj', 'nsm', 'cou', 'pav', 'nkw', 'van', 'cte', 'pow', 'prq', 'pay', 'lir', 'dbd', 'dhd', 'bwf', 'bgq', 'kkj', 'mpn', 'dwa', 'cin', 'nun', 'klg', 'pyu', 'mwt', 'bof', 'bse', 'wme', 'tic', 'lbf', 'sew', 'wow', 'ahs', 'pta', 'ted', 'mxj', 'ksv', 'kmi', 'qum', 'mgp', 'pkh', 'ygw', 'xac', 'kot', 'boh', 'abm', 'dnd', 'kls', 'saf', 'gox', 'fun', 'wbb', 'bpa', 'igb', 'skd', 'jab', 'sav', 'bov', 'kad', 'nhp', 'afz', 'nzb', 'saj', 'dsn', 'oru', 'pci', 'ndp', 'mcc', 'tgy', 'tjg', 'bev', 'gro', 'lrl', 'dbi', 'pln', 'fuu', 'mxd', 'ggb', 'mhs', 'mdd', 'jkp', 'kpm', 'wog', 'byo', 'dei', 'irn', 'slz', 'wlw', 'how', 'mpr', 'blh', 'srz', 'nos', 'tef', 'zrg', 'kfu', 'kow', 'jat', 'glr', 'pug', 'mqg', 'agi', 'jmn', 'shm', 'skq', 'tga', 'nqy', 'lel', 'ukw', 'spt', 'zms', 'tnv', 'xuj', 'avl', 'duh', 'kqm', 'otx', 'kqp', 'yuq', 'des', 'lmd', 'mum', 'hmj', 'dyg', 'wss', 'nnl', 'jdg', 'tla', 'klo', 'snm', 'cox', 'ccl', 'kyv', 'tsp', 'tpq', 'rat', 'liw', 'ngw', 'nhz', 'shc', 'kfg', 'kep', 'itl', 'nse', 'sez', 'mny', 'vmj', 'zuy', 'buz', 'nki', 'anl', 'kwc', 'csk', 'okx', 'xkk', 'ibm', 'fuq', 'gau', 'jad', 'cde', 'yno', 'soo', 'etc', 'clt', 'gec', 'vmx', 'dhn', 'sjb', 'njs', 'krw', 'amb', 'bhu', 'stn', 'gbv', 'dho', 'ztl', 'crv', 'tkb', 'toq', 'nsy', 'qws', 'ctt', 'bkg', 'dub', 'knz', 'its', 'zpp', 'ldo', 'vkn', 'asg', 'xkj', 'kkf', 'yif', 'sbn', 'itt', 'nmo', 'gbh', 'rng', 'soa', 'atu', 'zau', 'ahp', 'twr', 'mku', 'zmq', 'jwi', 'yiq', 'smh', 'dez', 'ztx', 'kcq', 'uis', 'gyz', 'jbm', 'kch', 'rei', 'kty', 'def', 'tml', 'vmc', 'zeh', 'wbq', 'zpy', 'cdi', 'yiu', 'mkk', 'cma', 'tgt', 'bmj', 'ghe', 'pbc', 'cov', 'pwb', 'jda', 'nao', 'agy', 'yat', 'cmi', 'bjx', 'soi', 'hrm', 'kmj', 'kvx', 'bac', 'bpe', 'byj', 'bfb', 'aoe', 'gmz', 'ors', 'tqb', 'ate', 'yde', 'jog', 'tny', 'njx', 'zkn', 'nqt', 'prt', 'nlk', 'zpk', 'pcl', 'pgg', 'bqh', 'gry', 'kfv', 'kuh', 'kjo', 'ldg', 'phq', 'kwt', 'cib', 'nmi', 'zpg', 'swr', 'tng', 'lpo', 'kfh', 'lhp', 'rdb', 'cna', 'bbo', 'ojs', 'okh', 'xwe', 'gso', 'lal', 'usi', 'kra', 'bqa', 'lhl', 'xtl', 'dkx', 'mdu', 'auq', 'cih', 'nyw', 'vmh', 'kqk', 'nkf', 'aaf', 'srx', 'mut', 'sct', 'nwm', 'tkt', 'ttb', 'xti', 'goj', 'mel', 'kyb', 'tks', 'grj', 'aug', 'sfw', 'lrk', 'mtb', 'kyy', 'nct', 'yiz', 'gbl', 'kxx', 'sre', 'sdr', 'kpb', 'bip', 'fad', 'mjv', 'zph', 'ktf', 'grv', 'ktc', 'acz', 'luz', 'pom', 'tcp', 'mrd', 'bvy', 'nes', 'ntr', 'pbl', 'klk', 'pez', 'mxs', 'ssi', 'swk', 'krn', 'kki', 'nxk', 'tfi', 'ruy', 'xkz', 'irr', 'zpt', 'kvi', 'sle', 'hmg', 'kev', 'bsl', 'zte', 'bfr', 'kjt', 'dzl', 'bka', 'jnd', 'nuz', 'mnu', 'zpd', 'ont', 'djo', 'aww', 'url', 'eze', 'ity', 'tcu', 'bix', 'bmq', 'clj', 'cwd', 'sbx', 'wud', 'buj', 'tvn', 'kcs', 'kce', 'mdk', 'hmw', 'cgk', 'ayt', 'mml', 'bah', 'sgj', 'pxm', 'bqx', 'deh', 'xkf', 'jrt', 'wba', 'gok', 'gjk', 'ktj', 'zcd', 'kip', 'atp', 'pud', 'kft', 'kkd', 'ykk', 'seg', 'mqh', 'dbv', 'mii', 'gas', 'mdn', 'yhd', 'ayp', 'drt', 'ghr', 'tnb', 'jnl', 'xmc', 'pdn', 'lri', 'hii', 'smf', 'nyg', 'env', 'nni', 'nix', 'kku', 'dgx', 'bzs', 'pps', 'xgu', 'whg', 'pnc', 'tov', 'xwl', 'blo', 'bjo', 'mpu', 'bxs', 'bjj', 'vav', 'bge', 'awu', 'ckm', 'dhw', 'uss', 'cky', 'phr', 'olu', 'xuo', 'kdy', 'xsn', 'pcf', 'kpa', 'ggg', 'zpj', 'ksu', 'tou', 'bfe', 'mhp', 'xtt', 'smu', 'dka', 'knu', 'lgm', 'npo', 'mrv', 'kzn', 'gig', 'lik', 'goz', 'btg', 'tth', 'xtj', 'mxh', 'bqo', 'yeu', 'ost', 'uth', 'gbo', 'mik', 'smt', 'vum', 'pbm', 'kkn', 'mxl', 'sfm', 'kif', 'kcc', 'bui', 'fwe', 'hld', 'pcw', 'xkt', 'kmp', 'knl', 'mab', 'bgw', 'skn', 'saa', 'boo', 'ogg', 'pwr', 'ruz', 'nxd', 'cnc', 'buu', 'dza', 'cnq', 'ldp', 'rme', 'byc', 'ymk', 'zpe', 'xsq', 'dde', 'tja', 'sjp', 'bhi', 'ncq', 'mvg', 'zpa', 'bha', 'mzl', 'bga', 'pmj', 'tyy', 'wem', 'nof', 'wkd', 'gez', 'akt', 'lmi', 'mxa', 'tto', 'xrb', 'zpf', 'anr', 'tkx', 'ldq', 'xdo', 'dln', 'jiy', 'crm', 'mkb', 'xkc', 'hgw', 'sbz', 'abh', 'gbn', 'nuq', 'tak', 'mjz', 'gba', 'zbu', 'bma', 'azt', 'das', 'bvu', 'ywl', 'jkr', 'sci', 'bhx', 'bgx', 'lrm', 'cok', 'oso', 'key', 'xum', 'loq', 'hmz', 'sju', 'mfk', 'bfo', 'dwz', 'rmz', 'kej', 'pch', 'sld', 'ysp', 'yax', 'bvh', 'lie', 'kvl', 'lpn', 'aoc', 'slx', 'npb', 'stt', 'krv', 'piy', 'csg', 'zxx', 'mfs', 'prl', 'fse', 'vsl', 'aed', 'csn', 'ymr']\n"
     ]
    }
   ],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "print(tag_language)\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)')\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model_idx):\n",
    "\n",
    "\tmodel = models_df.loc[model_idx]\n",
    "\tmodel_tags = models_df.loc[model_idx]['tags']\n",
    "\tmodel_card_data = None\n",
    "\ttry:\n",
    "\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\texcept AttributeError:\n",
    "\t\tprint('No card data available for this model')\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model['id'])\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No architecture data available for this model')\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tbase_model = model_card_data['base_model']\n",
    "\t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model['author']]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No emission data available for this model')\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_json(model_idx):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    model_attributes = extract_model_attributes(model_idx)\n",
    "    \n",
    "    with open(os.path.join(result_path, 'test_models.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_attributes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_json(models_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    for model_idx in range(models_df.shape[0]):\n",
    "        output.append(extract_model_attributes(model_idx))\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.LargeLanguageModel.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_json(models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True, cardData=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes_optimized(model):\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tbase_model = model_card_data['base_model']\n",
    "\t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 models processed, 5.654198884963989 seconds elapsed\n",
      "2000 models processed, 7.976945877075195 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 models processed, 17.254856824874878 seconds elapsed\n",
      "4000 models processed, 19.138203859329224 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 models processed, 30.274333953857422 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 models processed, 37.526339054107666 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 models processed, 40.21756291389465 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 models processed, 104.0346748828888 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 models processed, 105.22932696342468 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 models processed, 106.99660682678223 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000 models processed, 109.539794921875 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 models processed, 119.60219693183899 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'models_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for model in models:\n",
    "    model_attributes = extract_model_attributes_optimized(model)\n",
    "    add_to_json_file(model_attributes, file_path)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = next(models)\n",
    "model_attributes = extract_model_attributes_optimized(model)\n",
    "add_to_json_file(model_attributes, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def models_statistics():\n",
    "\n",
    "\tname_count = 0\n",
    "\tversion_count = 0\n",
    "\tnumber_of_parameters_count = 0\n",
    "\tquantization_count = 0\n",
    "\tarchitecture_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tmodel_creator_count = 0\n",
    "\tlicense_count = 0\n",
    "\tlibrary_count = 0\n",
    "\tcontext_length_count = 0\n",
    "\tdevelopers_count = 0\n",
    "\topen_source_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuned_count = 0\n",
    "\tcarbon_emission_count = 0\n",
    "\ttokenizer_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "\n",
    "\tmodels_json = open(os.path.join(result_path, 'models_data.json'))\n",
    "\tmodels_data_json = json.load(models_json)\n",
    "\n",
    "\tmodels_df = pd.DataFrame(models_data_json) \n",
    "\n",
    "\t# TODO: add more attributes (?)\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['version'] is not None:\n",
    "\t\t\tversion_count += 1\n",
    "\t\tif item['numberOfParameters'] is not None:\n",
    "\t\t\tnumber_of_parameters_count += 1\n",
    "\t\tif item['quantization'] is not None:\n",
    "\t\t\tquantization_count += 1\n",
    "\t\tif item['architecture'] is not None:\n",
    "\t\t\tarchitecture_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\n",
    "\t\tif item['modelCreator'] is not None:\n",
    "\t\t\tmodel_creator_count += 1\n",
    "\t\tif item['licenseToUse'] is not None:\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['libraryFramework']) > 0:\n",
    "\t\t\tlibrary_count += 1\n",
    "\t\tif item['contextLength'] is not None:\n",
    "\t\t\tcontext_length_count += 1\n",
    "\t\tif len(item['developers']) > 0:\n",
    "\t\t\tdevelopers_count += 1\n",
    "\t\tif item['openSource'] is not None:\n",
    "\t\t\topen_source_count += 1\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuned'] is not None:\n",
    "\t\t\tfinetuned_count += 1\n",
    "\t\tif item['carbonEmission [CO2eq tons]'] is not None:\n",
    "\t\t\tcarbon_emission_count += 1\n",
    "\t\tif item['tokenizer'] is not None:\n",
    "\t\t\ttokenizer_count += 1\n",
    "\t\n",
    "\ttotal_models = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed models: {total_models}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Version: {version_count} ({(version_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Number of Parameters: {number_of_parameters_count} ({(number_of_parameters_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Quantization: {quantization_count} ({(quantization_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Architecture: {architecture_count} ({(architecture_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Model creator: {model_creator_count} ({(model_creator_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Library: {library_count} ({(library_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Context Length: {context_length_count} ({(context_length_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Developers: {developers_count} ({(developers_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Open Source: {open_source_count} ({(open_source_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuned: {finetuned_count} ({(finetuned_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Carbon emission: {carbon_emission_count} ({(carbon_emission_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Tokenizer: {tokenizer_count} ({(tokenizer_count / total_models) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tllm_attributes = models_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tmodel_name = item['name']\n",
    "\t\tfor attr in llm_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed models: 834415\n",
      "    Name: 834415 (100.00%)\n",
      "    Version: 0 (0.00%)\n",
      "    Number of Parameters: 0 (0.00%)\n",
      "    Quantization: 16553 (1.98%)\n",
      "    Architecture: 0 (0.00%)\n",
      "    Languages: 96803 (11.60%)\n",
      "    Model creator: 0 (0.00%)\n",
      "    License to use: 310804 (37.25%)\n",
      "    Library: 545046 (65.32%)\n",
      "    Context Length: 0 (0.00%)\n",
      "    Developers: 834415 (100.00%)\n",
      "    Open Source: 834415 (100.00%)\n",
      "    URI: 117353 (14.06%)\n",
      "    Fine-tuned: 0 (0.00%)\n",
      "    Carbon emission: 0 (0.00%)\n",
      "    Tokenizer: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "models_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>version</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>numberOfParameters</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>quantization</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>modelCreator</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>libraryFramework</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>contextLength</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>developers</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>openSource</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>fineTuned</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>carbonEmission [CO2eq tons]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>tokenizer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>version</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>numberOfParameters</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>quantization</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>modelCreator</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>libraryFramework</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>contextLength</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>developers</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>openSource</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>fineTuned</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id entity name               attribute name available API  \\\n",
       "0   albert-base-v1         LLM                         name          True   \n",
       "1   albert-base-v1         LLM                      version         False   \n",
       "2   albert-base-v1         LLM           numberOfParameters         False   \n",
       "3   albert-base-v1         LLM                 quantization         False   \n",
       "4   albert-base-v1         LLM                 architecture         False   \n",
       "5   albert-base-v1         LLM                    languages          True   \n",
       "6   albert-base-v1         LLM                 modelCreator         False   \n",
       "7   albert-base-v1         LLM                 licenseToUse          True   \n",
       "8   albert-base-v1         LLM             libraryFramework          True   \n",
       "9   albert-base-v1         LLM                contextLength         False   \n",
       "10  albert-base-v1         LLM                   developers          True   \n",
       "11  albert-base-v1         LLM                   openSource          True   \n",
       "12  albert-base-v1         LLM                          uri          True   \n",
       "13  albert-base-v1         LLM                    fineTuned         False   \n",
       "14  albert-base-v1         LLM  carbonEmission [CO2eq tons]         False   \n",
       "15  albert-base-v1         LLM                    tokenizer         False   \n",
       "16  albert-base-v2         LLM                         name          True   \n",
       "17  albert-base-v2         LLM                      version         False   \n",
       "18  albert-base-v2         LLM           numberOfParameters         False   \n",
       "19  albert-base-v2         LLM                 quantization         False   \n",
       "20  albert-base-v2         LLM                 architecture         False   \n",
       "21  albert-base-v2         LLM                    languages          True   \n",
       "22  albert-base-v2         LLM                 modelCreator         False   \n",
       "23  albert-base-v2         LLM                 licenseToUse          True   \n",
       "24  albert-base-v2         LLM             libraryFramework          True   \n",
       "25  albert-base-v2         LLM                contextLength         False   \n",
       "26  albert-base-v2         LLM                   developers          True   \n",
       "27  albert-base-v2         LLM                   openSource          True   \n",
       "28  albert-base-v2         LLM                          uri          True   \n",
       "29  albert-base-v2         LLM                    fineTuned         False   \n",
       "\n",
       "   available scraping  \n",
       "0               False  \n",
       "1               False  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  \n",
       "5               False  \n",
       "6               False  \n",
       "7               False  \n",
       "8               False  \n",
       "9               False  \n",
       "10              False  \n",
       "11              False  \n",
       "12              False  \n",
       "13              False  \n",
       "14              False  \n",
       "15              False  \n",
       "16              False  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  \n",
       "20              False  \n",
       "21              False  \n",
       "22              False  \n",
       "23              False  \n",
       "24              False  \n",
       "25              False  \n",
       "26              False  \n",
       "27              False  \n",
       "28              False  \n",
       "29              False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>paperswithcode_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>card_data</th>\n",
       "      <th>siblings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amirveyseh/acronym_identification</td>\n",
       "      <td>amirveyseh</td>\n",
       "      <td>15ef643450d589d5883e289ffadeb03563e80a9e</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:39:57+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>19</td>\n",
       "      <td>acronym-identification</td>\n",
       "      <td>[task_categories:token-classification, annotat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ade-benchmark-corpus/ade_corpus_v2</td>\n",
       "      <td>ade-benchmark-corpus</td>\n",
       "      <td>4ba01c71687dd7c996597042449448ea312126cf</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:42:58+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_cat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLNLP/adversarial_qa</td>\n",
       "      <td>UCLNLP</td>\n",
       "      <td>c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 14:20:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>139</td>\n",
       "      <td>32</td>\n",
       "      <td>adversarialqa</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yale-LILY/aeslc</td>\n",
       "      <td>Yale-LILY</td>\n",
       "      <td>2305f2e63b68056f9b9037a3805c8c196e0d5581</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:49:13+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>124</td>\n",
       "      <td>12</td>\n",
       "      <td>aeslc</td>\n",
       "      <td>[task_categories:summarization, annotations_cr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nwu-ctext/afrikaans_ner_corpus</td>\n",
       "      <td>nwu-ctext</td>\n",
       "      <td>445834a997dce8b40e1d108638064381de80c497</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:51:47+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:token-classification, task_id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fancyzhx/ag_news</td>\n",
       "      <td>fancyzhx</td>\n",
       "      <td>eb185aade064a813bc0b7f42de02595523103ca4</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 12:02:37+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7297</td>\n",
       "      <td>123</td>\n",
       "      <td>ag-news</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>allenai</td>\n",
       "      <td>210d026faf9955653af8916fad021475a3f00453</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 15:09:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>785162</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google/air_dialogue</td>\n",
       "      <td>google</td>\n",
       "      <td>dbdbe7bcef8d344bc3c68a05600f3d95917d6898</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 15:22:15+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-generation, task_categor...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>komari6/ajgt_twitter_ar</td>\n",
       "      <td>komari6</td>\n",
       "      <td>af3f2fa5462ac461b696cb300d66e07ad366057f</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:58:01+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>legacy-datasets/allegro_reviews</td>\n",
       "      <td>legacy-datasets</td>\n",
       "      <td>71593d1379934286885c53d147bc863ffe830745</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:59:39+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>allegro-reviews</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                author  \\\n",
       "0   amirveyseh/acronym_identification            amirveyseh   \n",
       "1  ade-benchmark-corpus/ade_corpus_v2  ade-benchmark-corpus   \n",
       "2               UCLNLP/adversarial_qa                UCLNLP   \n",
       "3                     Yale-LILY/aeslc             Yale-LILY   \n",
       "4      nwu-ctext/afrikaans_ner_corpus             nwu-ctext   \n",
       "5                    fancyzhx/ag_news              fancyzhx   \n",
       "6                     allenai/ai2_arc               allenai   \n",
       "7                 google/air_dialogue                google   \n",
       "8             komari6/ajgt_twitter_ar               komari6   \n",
       "9     legacy-datasets/allegro_reviews       legacy-datasets   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  15ef643450d589d5883e289ffadeb03563e80a9e 2022-03-02 23:29:22+00:00   \n",
       "1  4ba01c71687dd7c996597042449448ea312126cf 2022-03-02 23:29:22+00:00   \n",
       "2  c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9 2022-03-02 23:29:22+00:00   \n",
       "3  2305f2e63b68056f9b9037a3805c8c196e0d5581 2022-03-02 23:29:22+00:00   \n",
       "4  445834a997dce8b40e1d108638064381de80c497 2022-03-02 23:29:22+00:00   \n",
       "5  eb185aade064a813bc0b7f42de02595523103ca4 2022-03-02 23:29:22+00:00   \n",
       "6  210d026faf9955653af8916fad021475a3f00453 2022-03-02 23:29:22+00:00   \n",
       "7  dbdbe7bcef8d344bc3c68a05600f3d95917d6898 2022-03-02 23:29:22+00:00   \n",
       "8  af3f2fa5462ac461b696cb300d66e07ad366057f 2022-03-02 23:29:22+00:00   \n",
       "9  71593d1379934286885c53d147bc863ffe830745 2022-03-02 23:29:22+00:00   \n",
       "\n",
       "              last_modified  private  gated  disabled  downloads  likes  \\\n",
       "0 2024-01-09 11:39:57+00:00    False  False     False        180     19   \n",
       "1 2024-01-09 11:42:58+00:00    False  False     False        241     25   \n",
       "2 2023-12-21 14:20:00+00:00    False  False     False        139     32   \n",
       "3 2024-01-09 11:49:13+00:00    False  False     False        124     12   \n",
       "4 2024-01-09 11:51:47+00:00    False  False     False        112      6   \n",
       "5 2024-03-07 12:02:37+00:00    False  False     False       7297    123   \n",
       "6 2023-12-21 15:09:48+00:00    False  False     False     785162    111   \n",
       "7 2024-03-07 15:22:15+00:00    False  False     False         75     15   \n",
       "8 2024-01-09 11:58:01+00:00    False  False     False        119      4   \n",
       "9 2024-01-09 11:59:39+00:00    False  False     False         84      4   \n",
       "\n",
       "        paperswithcode_id                                               tags  \\\n",
       "0  acronym-identification  [task_categories:token-classification, annotat...   \n",
       "1                    None  [task_categories:text-classification, task_cat...   \n",
       "2           adversarialqa  [task_categories:question-answering, task_ids:...   \n",
       "3                   aeslc  [task_categories:summarization, annotations_cr...   \n",
       "4                    None  [task_categories:token-classification, task_id...   \n",
       "5                 ag-news  [task_categories:text-classification, task_ids...   \n",
       "6                    None  [task_categories:question-answering, task_ids:...   \n",
       "7                    None  [task_categories:text-generation, task_categor...   \n",
       "8                    None  [task_categories:text-classification, task_ids...   \n",
       "9         allegro-reviews  [task_categories:text-classification, task_ids...   \n",
       "\n",
       "  card_data siblings  \n",
       "0        {}     None  \n",
       "1        {}     None  \n",
       "2        {}     None  \n",
       "3        {}     None  \n",
       "4        {}     None  \n",
       "5        {}     None  \n",
       "6        {}     None  \n",
       "7        {}     None  \n",
       "8        {}     None  \n",
       "9        {}     None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO run for all datasets\n",
    "datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "datasets_df = pd.DataFrame(datasets)\n",
    "datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
       "       'gated', 'disabled', 'downloads', 'likes', 'paperswithcode_id', 'tags',\n",
       "       'card_data', 'siblings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                   amirveyseh/acronym_identification\n",
       "author                                                      amirveyseh\n",
       "sha                           15ef643450d589d5883e289ffadeb03563e80a9e\n",
       "created_at                                   2022-03-02 23:29:22+00:00\n",
       "last_modified                                2024-01-09 11:39:57+00:00\n",
       "private                                                          False\n",
       "gated                                                            False\n",
       "disabled                                                         False\n",
       "downloads                                                          180\n",
       "likes                                                               19\n",
       "paperswithcode_id                               acronym-identification\n",
       "tags                 [task_categories:token-classification, annotat...\n",
       "card_data                                                           {}\n",
       "siblings                                                          None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_categories:question-answering',\n",
       " 'task_ids:extractive-qa',\n",
       " 'task_ids:open-domain-qa',\n",
       " 'annotations_creators:crowdsourced',\n",
       " 'language_creators:found',\n",
       " 'multilinguality:monolingual',\n",
       " 'source_datasets:original',\n",
       " 'language:en',\n",
       " 'license:cc-by-sa-4.0',\n",
       " 'size_categories:10K<n<100K',\n",
       " 'format:parquet',\n",
       " 'modality:text',\n",
       " 'library:datasets',\n",
       " 'library:pandas',\n",
       " 'library:mlcroissant',\n",
       " 'library:polars',\n",
       " 'arxiv:2002.00293',\n",
       " 'arxiv:1606.05250',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[2]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset_idx):\n",
    "\n",
    "\tdataset = datasets_df.loc[dataset_idx]\n",
    "\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset['id'])\n",
    "\tdataset_attributes['size [GB]'] = None\n",
    "\n",
    "\turl = \"https://huggingface.co/datasets/\" + dataset['id']\n",
    "\tfile_size_str = extract_file_size(url)\n",
    "\tif file_size_str:\n",
    "\t\tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t\tif file_size_gb:\n",
    "\t\t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_json(datasets_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for dataset_idx in range(datasets_df.shape[0]):\n",
    "        output.append(extract_datasets_attributes(dataset_idx))\n",
    "        if dataset_idx % 10 == 0:\n",
    "            print(f'Processed {dataset_idx} datasets, elapsed time: {time.time() - start_time:.2f} seconds')\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Dataset.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 datasets, elapsed time: 1.00 seconds\n",
      "Processed 10 datasets, elapsed time: 9.65 seconds\n",
      "Processed 20 datasets, elapsed time: 17.75 seconds\n",
      "Processed 30 datasets, elapsed time: 25.48 seconds\n",
      "Processed 40 datasets, elapsed time: 33.46 seconds\n",
      "Processed 50 datasets, elapsed time: 42.07 seconds\n",
      "Processed 60 datasets, elapsed time: 51.79 seconds\n",
      "Processed 70 datasets, elapsed time: 58.55 seconds\n",
      "Processed 80 datasets, elapsed time: 65.34 seconds\n",
      "Processed 90 datasets, elapsed time: 73.20 seconds\n",
      "Processed 100 datasets, elapsed time: 81.37 seconds\n",
      "Processed 110 datasets, elapsed time: 89.41 seconds\n",
      "Processed 120 datasets, elapsed time: 98.28 seconds\n",
      "Processed 130 datasets, elapsed time: 105.30 seconds\n",
      "Processed 140 datasets, elapsed time: 113.13 seconds\n",
      "Processed 150 datasets, elapsed time: 119.39 seconds\n",
      "Processed 160 datasets, elapsed time: 125.61 seconds\n",
      "Processed 170 datasets, elapsed time: 133.69 seconds\n",
      "Processed 180 datasets, elapsed time: 140.15 seconds\n",
      "Processed 190 datasets, elapsed time: 146.29 seconds\n",
      "Processed 200 datasets, elapsed time: 153.17 seconds\n",
      "Processed 210 datasets, elapsed time: 160.06 seconds\n",
      "Processed 220 datasets, elapsed time: 167.29 seconds\n",
      "Processed 230 datasets, elapsed time: 174.49 seconds\n",
      "Processed 240 datasets, elapsed time: 181.96 seconds\n",
      "Processed 250 datasets, elapsed time: 188.86 seconds\n",
      "Processed 260 datasets, elapsed time: 195.17 seconds\n",
      "Processed 270 datasets, elapsed time: 201.14 seconds\n",
      "Processed 280 datasets, elapsed time: 206.82 seconds\n",
      "Processed 290 datasets, elapsed time: 212.84 seconds\n",
      "Processed 300 datasets, elapsed time: 218.66 seconds\n",
      "Processed 310 datasets, elapsed time: 225.04 seconds\n",
      "Processed 320 datasets, elapsed time: 232.35 seconds\n",
      "Processed 330 datasets, elapsed time: 239.15 seconds\n",
      "Processed 340 datasets, elapsed time: 245.25 seconds\n",
      "Processed 350 datasets, elapsed time: 250.93 seconds\n",
      "Processed 360 datasets, elapsed time: 257.13 seconds\n",
      "Processed 370 datasets, elapsed time: 264.18 seconds\n",
      "Processed 380 datasets, elapsed time: 269.91 seconds\n",
      "Processed 390 datasets, elapsed time: 275.97 seconds\n",
      "Processed 400 datasets, elapsed time: 281.88 seconds\n",
      "Processed 410 datasets, elapsed time: 287.41 seconds\n",
      "Processed 420 datasets, elapsed time: 294.33 seconds\n",
      "Processed 430 datasets, elapsed time: 301.62 seconds\n",
      "Processed 440 datasets, elapsed time: 310.20 seconds\n",
      "Processed 450 datasets, elapsed time: 317.36 seconds\n",
      "Processed 460 datasets, elapsed time: 325.05 seconds\n",
      "Processed 470 datasets, elapsed time: 332.20 seconds\n",
      "Processed 480 datasets, elapsed time: 338.33 seconds\n",
      "Processed 490 datasets, elapsed time: 346.54 seconds\n",
      "Processed 500 datasets, elapsed time: 356.02 seconds\n",
      "Processed 510 datasets, elapsed time: 363.41 seconds\n",
      "Processed 520 datasets, elapsed time: 371.07 seconds\n",
      "Processed 530 datasets, elapsed time: 376.83 seconds\n",
      "Processed 540 datasets, elapsed time: 384.30 seconds\n",
      "Processed 550 datasets, elapsed time: 390.66 seconds\n",
      "Processed 560 datasets, elapsed time: 397.23 seconds\n",
      "Processed 570 datasets, elapsed time: 403.73 seconds\n",
      "Processed 580 datasets, elapsed time: 411.70 seconds\n",
      "Processed 590 datasets, elapsed time: 419.07 seconds\n",
      "Processed 600 datasets, elapsed time: 426.37 seconds\n",
      "Processed 610 datasets, elapsed time: 433.19 seconds\n",
      "Processed 620 datasets, elapsed time: 438.24 seconds\n",
      "Processed 630 datasets, elapsed time: 444.84 seconds\n",
      "Processed 640 datasets, elapsed time: 451.92 seconds\n",
      "Processed 650 datasets, elapsed time: 458.82 seconds\n",
      "Processed 660 datasets, elapsed time: 466.19 seconds\n",
      "Processed 670 datasets, elapsed time: 473.30 seconds\n",
      "Processed 680 datasets, elapsed time: 480.77 seconds\n",
      "Processed 690 datasets, elapsed time: 488.38 seconds\n",
      "Processed 700 datasets, elapsed time: 495.65 seconds\n",
      "Processed 710 datasets, elapsed time: 504.77 seconds\n",
      "Processed 720 datasets, elapsed time: 512.01 seconds\n",
      "Processed 730 datasets, elapsed time: 521.36 seconds\n",
      "Processed 740 datasets, elapsed time: 527.86 seconds\n",
      "Processed 750 datasets, elapsed time: 536.91 seconds\n",
      "Processed 760 datasets, elapsed time: 544.58 seconds\n",
      "Processed 770 datasets, elapsed time: 551.46 seconds\n",
      "Processed 780 datasets, elapsed time: 557.88 seconds\n",
      "Processed 790 datasets, elapsed time: 565.73 seconds\n",
      "Processed 800 datasets, elapsed time: 571.26 seconds\n",
      "Processed 810 datasets, elapsed time: 576.84 seconds\n",
      "Processed 820 datasets, elapsed time: 582.55 seconds\n",
      "Processed 830 datasets, elapsed time: 588.46 seconds\n",
      "Processed 840 datasets, elapsed time: 596.36 seconds\n",
      "Processed 850 datasets, elapsed time: 602.47 seconds\n",
      "Processed 860 datasets, elapsed time: 608.91 seconds\n",
      "Processed 870 datasets, elapsed time: 614.91 seconds\n",
      "Processed 880 datasets, elapsed time: 621.03 seconds\n",
      "Processed 890 datasets, elapsed time: 627.99 seconds\n",
      "Processed 900 datasets, elapsed time: 635.31 seconds\n",
      "Processed 910 datasets, elapsed time: 644.85 seconds\n",
      "Processed 920 datasets, elapsed time: 654.47 seconds\n",
      "Processed 930 datasets, elapsed time: 664.13 seconds\n",
      "Processed 940 datasets, elapsed time: 670.25 seconds\n",
      "Processed 950 datasets, elapsed time: 675.47 seconds\n",
      "Processed 960 datasets, elapsed time: 682.08 seconds\n",
      "Processed 970 datasets, elapsed time: 689.75 seconds\n",
      "Processed 980 datasets, elapsed time: 696.21 seconds\n",
      "Processed 990 datasets, elapsed time: 703.07 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_to_json(datasets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def datasets_statistics():\n",
    "\n",
    "\tname_count = 0\n",
    "\tsize_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tlicense_count = 0\n",
    "\tdomain_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuning_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tdatasets_json = open(os.path.join(result_path, 'ChatIMPACT.Dataset.json'))\n",
    "\tdatasets_data_json = json.load(datasets_json)\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['size [GB]'] is not None:\n",
    "\t\t\tsize_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\t\n",
    "\t\tif item['licenseToUse'] is not None:\t\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['domain']) > 0:\n",
    "\t\t\tdomain_count += 1\t\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuning'] is not None:\n",
    "\t\t\tfinetuning_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\t\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Size [GB]: {size_count} ({(size_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Domain: {domain_count} ({(domain_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuning: {finetuning_count} ({(finetuning_count / total_datasets) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tdatasets_attributes = datasets_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tdataset_name = item['name']\n",
    "\t\tfor attr in datasets_attributes:\n",
    "\t\t\tif item[attr] is not None and attr == 'size [GB]':\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed datasets: 1000\n",
      "    Name: 1000 (100.00%)\n",
      "    Size [GB]: 525 (52.50%)\n",
      "    Languages: 817 (81.70%)\n",
      "    License to use: 784 (78.40%)\n",
      "    Domain: 2 (0.20%)\n",
      "    URI: 323 (32.30%)\n",
      "    Fine-tuning: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = datasets_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>fineTuning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>fineTuning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id entity name attribute name available API  \\\n",
       "0   acronym_identification     Dataset           name          True   \n",
       "1   acronym_identification     Dataset      size [GB]         False   \n",
       "2   acronym_identification     Dataset      languages          True   \n",
       "3   acronym_identification     Dataset   licenseToUse          True   \n",
       "4   acronym_identification     Dataset         domain         False   \n",
       "5   acronym_identification     Dataset            uri          True   \n",
       "6   acronym_identification     Dataset     fineTuning         False   \n",
       "7            ade_corpus_v2     Dataset           name          True   \n",
       "8            ade_corpus_v2     Dataset      size [GB]         False   \n",
       "9            ade_corpus_v2     Dataset      languages          True   \n",
       "10           ade_corpus_v2     Dataset   licenseToUse          True   \n",
       "11           ade_corpus_v2     Dataset         domain         False   \n",
       "12           ade_corpus_v2     Dataset            uri         False   \n",
       "13           ade_corpus_v2     Dataset     fineTuning         False   \n",
       "14          adversarial_qa     Dataset           name          True   \n",
       "15          adversarial_qa     Dataset      size [GB]         False   \n",
       "16          adversarial_qa     Dataset      languages          True   \n",
       "17          adversarial_qa     Dataset   licenseToUse          True   \n",
       "18          adversarial_qa     Dataset         domain         False   \n",
       "19          adversarial_qa     Dataset            uri          True   \n",
       "\n",
       "   available scraping  \n",
       "0               False  \n",
       "1                True  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  \n",
       "5               False  \n",
       "6               False  \n",
       "7               False  \n",
       "8                True  \n",
       "9               False  \n",
       "10              False  \n",
       "11              False  \n",
       "12              False  \n",
       "13              False  \n",
       "14              False  \n",
       "15               True  \n",
       "16              False  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description, # TODO: text2text generation has no description\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "\n",
    "    with open(result_path + '/ChatIMPACT.DownstreamTask.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tasks_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def tasks_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tsub_task_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\ttask_json = open(os.path.join(result_path, 'ChatIMPACT.DownstreamTask.json'))\n",
    "\ttask_data_json = json.load(task_json)\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\t\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif len(item['sub-task']) > 0:\n",
    "\t\t\tsub_task_count += 1\n",
    "\t\n",
    "\ttask_count = idx + 1\n",
    "\tprint(f'Number of processed task: {idx + 1}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Sub-task: {sub_task_count} ({(sub_task_count / task_count) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\ttask_attributes = task_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in task_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed task: 11\n",
      "    Name: 11 (100.00%)\n",
      "    Description: 11 (100.00%)\n",
      "    Sub-task: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = tasks_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id     entity name attribute name available API  \\\n",
       "0        text-classification  DownstreamTask           name         False   \n",
       "1        text-classification  DownstreamTask    description         False   \n",
       "2        text-classification  DownstreamTask       sub-task         False   \n",
       "3       token-classification  DownstreamTask           name         False   \n",
       "4       token-classification  DownstreamTask    description         False   \n",
       "5       token-classification  DownstreamTask       sub-task         False   \n",
       "6   table-question-answering  DownstreamTask           name         False   \n",
       "7   table-question-answering  DownstreamTask    description         False   \n",
       "8   table-question-answering  DownstreamTask       sub-task         False   \n",
       "9         question-answering  DownstreamTask           name         False   \n",
       "10        question-answering  DownstreamTask    description         False   \n",
       "11        question-answering  DownstreamTask       sub-task         False   \n",
       "12  zero-shot-classification  DownstreamTask           name         False   \n",
       "13  zero-shot-classification  DownstreamTask    description         False   \n",
       "14  zero-shot-classification  DownstreamTask       sub-task         False   \n",
       "15               translation  DownstreamTask           name         False   \n",
       "16               translation  DownstreamTask    description         False   \n",
       "17               translation  DownstreamTask       sub-task         False   \n",
       "18             summarization  DownstreamTask           name         False   \n",
       "19             summarization  DownstreamTask    description         False   \n",
       "20             summarization  DownstreamTask       sub-task         False   \n",
       "21        feature-extraction  DownstreamTask           name         False   \n",
       "22        feature-extraction  DownstreamTask    description         False   \n",
       "23        feature-extraction  DownstreamTask       sub-task         False   \n",
       "24           text-generation  DownstreamTask           name         False   \n",
       "25           text-generation  DownstreamTask    description         False   \n",
       "26           text-generation  DownstreamTask       sub-task         False   \n",
       "27                 fill-mask  DownstreamTask           name         False   \n",
       "28                 fill-mask  DownstreamTask    description         False   \n",
       "29                 fill-mask  DownstreamTask       sub-task         False   \n",
       "30       sentence-similarity  DownstreamTask           name         False   \n",
       "31       sentence-similarity  DownstreamTask    description         False   \n",
       "32       sentence-similarity  DownstreamTask       sub-task         False   \n",
       "\n",
       "   available scraping  \n",
       "0                True  \n",
       "1                True  \n",
       "2               False  \n",
       "3                True  \n",
       "4                True  \n",
       "5               False  \n",
       "6                True  \n",
       "7                True  \n",
       "8               False  \n",
       "9                True  \n",
       "10               True  \n",
       "11              False  \n",
       "12               True  \n",
       "13               True  \n",
       "14              False  \n",
       "15               True  \n",
       "16               True  \n",
       "17              False  \n",
       "18               True  \n",
       "19               True  \n",
       "20              False  \n",
       "21               True  \n",
       "22               True  \n",
       "23              False  \n",
       "24               True  \n",
       "25               True  \n",
       "26              False  \n",
       "27               True  \n",
       "28               True  \n",
       "29              False  \n",
       "30               True  \n",
       "31               True  \n",
       "32              False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "url_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "response = requests.get(url_metrics)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "h4_tags = soup.find_all('h4')\n",
    "metrics = [h4_tag.get_text(strip=True) for h4_tag in h4_tags]\n",
    "# print(metrics)\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "descriptions = [p_tag.get_text() for p_tag in p_tags]\n",
    "descriptions = descriptions[2:] # drop first lines\n",
    "# print(descriptions)\n",
    "\n",
    "# remove from the list the metrics withoud description (not useful for our purpose)\n",
    "metrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "metrics.remove('Aye10032/loss_metric')\n",
    "metrics.remove('giulio98/code_eval_outputs')\n",
    "metrics.remove('maysonma/lingo_judge_metric')\n",
    "metrics.remove('lvwerra/test')\n",
    "metrics.remove('sma2023/wil')\n",
    "\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "# From the lists, remove the descriptions and then the relative metric in the same index that have in the description 'TODO: add a description here\\n\\t\\t\\t\\t\\t\\t'ArithmeticError\n",
    "\n",
    "for i, description in enumerate(descriptions):\n",
    "    if 'TODO: add a description here' in description:\n",
    "        metrics.pop(i)\n",
    "        descriptions.pop(i)\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json(metrics, descriptions):\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "    \n",
    "        metrics_data.append(metric_attributes)\n",
    "        \n",
    "\n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Metric.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json(metrics, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def metric_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tcontext_count = 0\n",
    "\tfeatureBased_endToEnd_count = 0\n",
    "\tgranularity_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tmetric_json = open(os.path.join(result_path, 'ChatIMPACT.Metric.json'))\n",
    "\tmetric_data_json = json.load(metric_json)\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif item['context'] is not None:\n",
    "\t\t\tcontext_count += 1\n",
    "\t\tif item['featureBased/endToEnd'] is not None:\n",
    "\t\t\tfeatureBased_endToEnd_count += 1\n",
    "\t\tif item['granularity'] is not None:\t\n",
    "\t\t\tgranularity_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Context: {context_count} ({(context_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    FeatureBased/endToEnd: {featureBased_endToEnd_count} ({(featureBased_endToEnd_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Granularity: {granularity_count} ({(granularity_count / total_datasets) * 100:.2f}%)')\n",
    "\t\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tmetric_attributes = metric_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in metric_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed datasets: 218\n",
      "    Name: 218 (100.00%)\n",
      "    Description: 218 (100.00%)\n",
      "    Context: 0 (0.00%)\n",
      "    FeatureBased/endToEnd: 0 (0.00%)\n",
      "    Granularity: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = metric_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>featureBased/endToEnd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>granularity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>featureBased/endToEnd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>granularity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id entity name         attribute name available API  \\\n",
       "0   accuracy      Metric                   name         False   \n",
       "1   accuracy      Metric            description         False   \n",
       "2   accuracy      Metric                context         False   \n",
       "3   accuracy      Metric  featureBased/endToEnd         False   \n",
       "4   accuracy      Metric            granularity         False   \n",
       "5  bertscore      Metric                   name         False   \n",
       "6  bertscore      Metric            description         False   \n",
       "7  bertscore      Metric                context         False   \n",
       "8  bertscore      Metric  featureBased/endToEnd         False   \n",
       "9  bertscore      Metric            granularity         False   \n",
       "\n",
       "  available scraping  \n",
       "0               True  \n",
       "1               True  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  \n",
       "5               True  \n",
       "6               True  \n",
       "7              False  \n",
       "8              False  \n",
       "9              False  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_relationship():\n",
    "    \n",
    "    train_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        datasets = match_dataset(model_tags)\n",
    "        \n",
    "        if not datasets:\n",
    "            continue\n",
    "        else:\n",
    "            train_relationship = dict()\n",
    "            train_relationship['Models'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            train_relationship['Datasets'] = datasets # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            train_relationship_list.append(train_relationship)\n",
    "    \n",
    "    return train_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\ttrain_relationship = extract_train_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.TrainRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(train_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_suited_for_relationship():\n",
    "    \n",
    "    suited_for_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        \n",
    "        tasks = []\n",
    "        for t in model_tags:\n",
    "            if t in TAG_DOWNSTREAM_TASK:\n",
    "                tasks.append(t)\n",
    "\n",
    "        if not tasks:\n",
    "            continue\n",
    "        else:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            suited_for_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            suited_for_relationship_list.append(suited_for_relationship)\n",
    "    \n",
    "    return suited_for_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tsuited_for_relationship = extract_suited_for_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.SuitedForRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(suited_for_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enable_relationship():\n",
    "\t\n",
    "\tenable_relationship_list = []\n",
    "\t\n",
    "\tfor dataset_idx in range(len(datasets_df)):\n",
    "\t\t\n",
    "\t\tdataset = datasets_df.loc[dataset_idx]\n",
    "\t\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\t\t\n",
    "\t\ttasks = match_tasks(dataset_tags)\n",
    "\n",
    "\t\tif not tasks:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset['id']) # {\"$oid\": <...>} for MongoDB\n",
    "\t\t\tenable_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "\t\t\tenable_relationship_list.append(enable_relationship)\n",
    "\t\n",
    "\treturn enable_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tenable_relationship = extract_enable_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EnableRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(enable_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enable_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: here https://huggingface.co/tasks some tasks have associated metrics, we could scrape the tasks one by one\n",
    "\n",
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\\n\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "    return assess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.AssessRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task: text-classification\n",
      "\n",
      "Processing task: token-classification\n",
      "\n",
      "Processing task: table-question-answering\n",
      "\n",
      "Processing task: question-answering\n",
      "\n",
      "Processing task: zero-shot-classification\n",
      "\n",
      "Processing task: translation\n",
      "\n",
      "Processing task: summarization\n",
      "\n",
      "Processing task: feature-extraction\n",
      "\n",
      "Processing task: text-generation\n",
      "\n",
      "Processing task: fill-mask\n",
      "\n",
      "Processing task: sentence-similarity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def extract_evaluate_relationship():\n",
    "\n",
    "\tevaluate_relationship_list = []\n",
    "\n",
    "\tfor model_idx in range(len(models_df)):\n",
    "\n",
    "\t\tmodel = models_df.loc[model_idx]\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tprint('No card data available for this model')\n",
    "\t\t\n",
    "\t\tmetrics = []\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\n",
    "\t\tif not metrics:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tevaluate_relationship = dict()\n",
    "\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model['id'])\n",
    "\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\tevaluate_relationship_list.append(evaluate_relationship)\n",
    "\t\n",
    "\treturn evaluate_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluate_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tevaluate_relationship = extract_evaluate_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EvaluateRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(evaluate_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

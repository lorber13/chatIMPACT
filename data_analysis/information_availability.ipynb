{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tags import * # tags.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>mask_token</th>\n",
       "      <th>card_data</th>\n",
       "      <th>widget_data</th>\n",
       "      <th>model_index</th>\n",
       "      <th>config</th>\n",
       "      <th>transformers_info</th>\n",
       "      <th>siblings</th>\n",
       "      <th>spaces</th>\n",
       "      <th>safetensors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert/albert-base-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>082438ba120d36b97b9288772a41144e941705b9</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:57:35+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>14192</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert/albert-base-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>8e2f239c5f8a2c0f253781ca60135db913e5c80c</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:14+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2324178</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert/albert-large-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>94fd741fb5d6cb5bc578fc154837016c583bafef</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1853</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert/albert-large-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>dfed3a5ef4499fb3351c4ebbcf487375d1e942c8</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>16062</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert/albert-xlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>ed6f87d14403b3c459a458fa6aa9dc5c51c517c1</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:28+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1299</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert/albert-xlarge-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>4fd2c2aa9aeb305f87704a7e595be7bfffa3db88</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-04-10 09:57:46+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>3739</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert/albert-xxlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>43129068ee5f6a481c148daeac11cc593b8ff440</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:42+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>4787</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert/albert-xxlarge-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>97d3e58863d3a41dc581882f73b34d110b18f1f8</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:02:09+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>8349</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google-bert/bert-base-cased-finetuned-mrpc</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>f150c1d609d1e50dd5e2e5408661cfac8339277c</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:03:21+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>52041</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google-bert/bert-base-cased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>cd5ef92a9fb2f889e972770a36d4ed042daf221e</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:02:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>5829586</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id       author  \\\n",
       "0                       albert/albert-base-v1       albert   \n",
       "1                       albert/albert-base-v2       albert   \n",
       "2                      albert/albert-large-v1       albert   \n",
       "3                      albert/albert-large-v2       albert   \n",
       "4                     albert/albert-xlarge-v1       albert   \n",
       "5                     albert/albert-xlarge-v2       albert   \n",
       "6                    albert/albert-xxlarge-v1       albert   \n",
       "7                    albert/albert-xxlarge-v2       albert   \n",
       "8  google-bert/bert-base-cased-finetuned-mrpc  google-bert   \n",
       "9                 google-bert/bert-base-cased  google-bert   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  082438ba120d36b97b9288772a41144e941705b9 2022-03-02 23:29:04+00:00   \n",
       "1  8e2f239c5f8a2c0f253781ca60135db913e5c80c 2022-03-02 23:29:04+00:00   \n",
       "2  94fd741fb5d6cb5bc578fc154837016c583bafef 2022-03-02 23:29:04+00:00   \n",
       "3  dfed3a5ef4499fb3351c4ebbcf487375d1e942c8 2022-03-02 23:29:04+00:00   \n",
       "4  ed6f87d14403b3c459a458fa6aa9dc5c51c517c1 2022-03-02 23:29:04+00:00   \n",
       "5  4fd2c2aa9aeb305f87704a7e595be7bfffa3db88 2022-03-02 23:29:04+00:00   \n",
       "6  43129068ee5f6a481c148daeac11cc593b8ff440 2022-03-02 23:29:04+00:00   \n",
       "7  97d3e58863d3a41dc581882f73b34d110b18f1f8 2022-03-02 23:29:04+00:00   \n",
       "8  f150c1d609d1e50dd5e2e5408661cfac8339277c 2022-03-02 23:29:04+00:00   \n",
       "9  cd5ef92a9fb2f889e972770a36d4ed042daf221e 2022-03-02 23:29:04+00:00   \n",
       "\n",
       "              last_modified  private  gated disabled  downloads  likes  ...  \\\n",
       "0 2024-02-19 10:57:35+00:00    False  False     None      14192      8  ...   \n",
       "1 2024-02-19 10:58:14+00:00    False  False     None    2324178     99  ...   \n",
       "2 2024-02-19 10:58:26+00:00    False  False     None       1853      3  ...   \n",
       "3 2024-02-19 10:58:48+00:00    False  False     None      16062     15  ...   \n",
       "4 2024-02-19 11:01:28+00:00    False  False     None       1299      4  ...   \n",
       "5 2024-04-10 09:57:46+00:00    False  False     None       3739      8  ...   \n",
       "6 2024-02-19 11:01:42+00:00    False  False     None       4787      5  ...   \n",
       "7 2024-02-19 11:02:09+00:00    False  False     None       8349     19  ...   \n",
       "8 2024-02-19 11:03:21+00:00    False  False     None      52041      1  ...   \n",
       "9 2024-02-19 11:02:26+00:00    False  False     None    5829586    246  ...   \n",
       "\n",
       "  pipeline_tag mask_token card_data widget_data model_index config  \\\n",
       "0    fill-mask       None      None        None        None   None   \n",
       "1    fill-mask       None      None        None        None   None   \n",
       "2    fill-mask       None      None        None        None   None   \n",
       "3    fill-mask       None      None        None        None   None   \n",
       "4    fill-mask       None      None        None        None   None   \n",
       "5    fill-mask       None      None        None        None   None   \n",
       "6    fill-mask       None      None        None        None   None   \n",
       "7    fill-mask       None      None        None        None   None   \n",
       "8    fill-mask       None      None        None        None   None   \n",
       "9    fill-mask       None      None        None        None   None   \n",
       "\n",
       "  transformers_info                                           siblings spaces  \\\n",
       "0              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "1              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "2              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "3              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "4              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "5              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "6              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "7              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "8              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "9              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "\n",
       "  safetensors  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  \n",
       "6        None  \n",
       "7        None  \n",
       "8        None  \n",
       "9        None  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = itertools.islice(models, 0, 1000)\n",
    "models_df = pd.DataFrame(model)\n",
    "models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
      "       'gated', 'disabled', 'downloads', 'likes', 'library_name', 'tags',\n",
      "       'pipeline_tag', 'mask_token', 'card_data', 'widget_data', 'model_index',\n",
      "       'config', 'transformers_info', 'siblings', 'spaces', 'safetensors'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(models_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers',\n",
       " 'pytorch',\n",
       " 'tf',\n",
       " 'safetensors',\n",
       " 'albert',\n",
       " 'fill-mask',\n",
       " 'exbert',\n",
       " 'en',\n",
       " 'dataset:bookcorpus',\n",
       " 'dataset:wikipedia',\n",
       " 'arxiv:1909.11942',\n",
       " 'license:apache-2.0',\n",
       " 'autotrain_compatible',\n",
       " 'endpoints_compatible',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag examples\n",
    "models_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers',\n",
       " 'pytorch',\n",
       " 'tensorboard',\n",
       " 'encoder-decoder',\n",
       " 'text2text-generation',\n",
       " 'generated_from_trainer',\n",
       " 'dataset:cnn_dailymail',\n",
       " 'autotrain_compatible',\n",
       " 'endpoints_compatible',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# richer tag information example\n",
    "example_model = api.list_models(model_name='albert_bert_summarization_cnn_dailymail')\n",
    "example_df = pd.DataFrame(example_model)\n",
    "example_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'zh', 'fr', 'es', 'ru', 'de', 'ja', 'pt', 'ko', 'ar', 'it', 'vi', 'tr', 'hi', 'id', 'pl', 'nl', 'th', 'cs', 'bn', 'fa', 'sv', 'ro', 'ca', 'fi', 'ta', 'da', 'hu', 'uk', 'ind', 'el', 'te', 'ur', 'bg', 'he', 'ml', 'ms', 'sl', 'mr', 'sw', 'sk', 'et', 'eu', 'kn', 'gu', 'sr', 'no', 'hr', 'lt', 'lv', 'pa', 'is', 'yo', 'am', 'vie', 'ne', 'az', 'af', 'ga', 'si', 'mt', 'or', 'gl', 'sq', 'kk', 'cy', 'tl', 'ceb', 'tha', 'as', 'ha', 'mk', 'hy', 'ka', 'uz', 'my', 'eng', 'ig', 'eo', 'be', 'nb', 'km', 'mn', 'ky', 'la', 'zu', 'so', 'min', 'xh', 'jav', 'nn', 'ps', 'rw', 'jv', 'yue', 'mya', 'tt', 'br', 'bs', 'ckb', 'lg', 'sa', 'lo', 'wo', 'ku', 'ug', 'sd', 'ilo', 'ast', 'tw', 'sun', 'tg', 'ace', 'lb', 'gd', 'nso', 'war', 'fy', 'tk', 'bug', 'tgl', 'oc', 'fil', 'su', 'sn', 'khm', 'bjn', 'gn', 'ht', 'yi', 'mai', 'bo', 'ba', 'ban', 'zlm', 'tn', 'fo', 'dv', 'ln', 'kab', 'bm', 'hin', 'aa', 'ny', 'cv', 'ti', 'shn', 'mg', 'sat', 'mi', 'lao', 'sah', 'arz', 'ia', 'mar', 'ee', 'st', 'pag', 'qu', 'azb', 'ab', 'hsb', 'lij', 'mhr', 'vec', 'om', 'ak', 'sc', 'por', 'mni', 'tam', 'li', 'crh', 'tpi', 'als', 'ts', 'scn', 'rn', 'rm', 'fur', 'fra', 'mad', 'spa', 'nds', 'pap', 'luo', 'rus', 'ltg', 'tet', 'pam', 'lmo', 'kmr', 'ch', 'fon', 'szl', 'sm', 'ben', 'hau', 'swh', 'jpn', 'ary', 'cmn', 'deu', 'ita', 'urd', 'tum', 'mal', 'io', 'ks', 'ss', 'pcm', 'myv', 'an', 'nld', 'os', 'guj', 'bho', 'yor', 'kan', 'gom', 'ron', 'cbk', 'amh', 'pan', 'kor', 'pes', 'wa', 'pol', 'haw', 'dz', 'tel', 'ki', 'lug', 'kam', 'srp', 'hun', 'kg', 'swe', 'dan', 'zsm', 'kw', 'ory', 'xho', 'kea', 'ce', 'npi', 'kbp', 'kin', 'udm', 'afr', 'vo', 'awa', 'mrj', 'ukr', 'som', 'dyu', 'nya', 'quy', 'lus', 'tur', 'bcl', 'pnb', 'lit', 'ibo', 'umb', 'fj', 'jbo', 'wuu', 'zul', 'heb', 'ie', 'krc', 'mdf', 'co', 'mos', 'cnh', 'ces', 'zho', 'bem', 'tdt', 'pms', 'tyv', 'kbd', 'sco', 'to', 'dsb', 'bar', 'hrv', 'nap', 'fin', 'sna', 'sg', 'csb', 'ay', 'azj', 'bxr', 'nan', 'ell', 'wol', 'hil', 've', 'ang', 'new', 'gv', 'tgk', 'zgh', 'bh', 'vep', 'bel', 'kac', 'tzm', 'sot', 'bul', 'xmf', 'gle', 'cat', 'kir', 'srn', 'rue', 'av', 'mzn', 'cym', 'ara', 'hak', 'glk', 'slv', 'vls', 'gor', 'hat', 'bpy', 'asm', 'hne', 'isl', 'slk', 'ff', 'kaa', 'ksh', 'diq', 'dik', 'glg', 'frr', 'lin', 'mwl', 'xal', 'taq', 'plt', 'mkd', 'se', 'chr', 'jam', 'prs', 'lzh', 'brx', 'lez', 'koi', 'rmy', 'mlt', 'kat', 'hye', 'mnw', 'lua', 'lad', 'nob', 'san', 'kaz', 'epo', 'orm', 'apc', 'skr', 'ydd', 'gaz', 'kv', 'twi', 'ltz', 'mri', 'pbt', 'bi', 'lvs', 'snd', 'guc', 'lfn', 'got', 'ayr', 'quc', 'msa', 'rup', 'kmb', 'mag', 'ext', 'bos', 'arb', 'nv', 'cjk', 'nov', 'bbc', 'tsn', 'fuv', 'gag', 'mak', 'aeb', 'gsw', 'pdc', 'oci', 'gan', 'pcd', 'frp', 'kl', 'uzn', 'fas', 'dty', 'ty', 'hif', 'iu', 'ars', 'khk', 'cdo', 'nus', 'eus', 'kek', 'lld', 'gla', 'knc', 'lrc', 'zea', 'est', 'tso', 'acm', 'inh', 'stq', 'uzb', 'tok', 'grc', 'olo', 'cu', 'kik', 'arn', 'acq', 'mam', 'cak', 'nij', 'shi', 'msb', 'uig', 'ewe', 'aka', 'nno', 'aii', 'bew', 'swa', 'za', 'lat', 'sqi', 'pfl', 'cho', 'chy', 'nep', 'nia', 'tig', 'dag', 'cgc', 'bod', 'din', 'sgs', 'ae', 'tcy', 'bas', 'sin', 'bam', 'mus', 'vot', 'lbe', 'vro', 'kur', 'mqj', 'dtp', 'ady', 'pus', 'cr', 'arq', 'cbr', 'atj', 'mh', 'hus', 'nhi', 'ksw', 'taj', 'pi', 'pih', 'mon', 'llg', 'bre', 'ven', 'hyw', 'bkx', 'myk', 'ik', 'qub', 'pnt', 'nyu', 'jiv', 'quz', 'lav', 'tir', 'yid', 'kas', 'mkn', 'kbq', 'tzo', 'kok', 'maz', 'chk', 'chd', 'amu', 'djk', 'ami', 'ho', 'cab', 'mic', 'shp', 'top', 'ssw', 'nqo', 'mui', 'bzi', 'hla', 'alt', 'jra', 'fuf', 'tca', 'kmu', 'nrf', 'quh', 'ote', 'agr', 'ii', 'fuh', 'nyn', 'ame', 'bef', 'ton', 'rej', 'ori', 'fao', 'acu', 'cni', 'ng', 'fat', 'sxb', 'smo', 'nas', 'rug', 'kms', 'cop', 'bis', 'csy', 'nsn', 'mbb', 'tnp', 'kne', 'lbk', 'hch', 'pwg', 'srm', 'quf', 'nor', 'hmn', 'bjr', 'aze', 'ino', 'sme', 'kmg', 'qvh', 'roo', 'tuk', 'sag', 'wal', 'hvn', 'sml', 'omw', 'kj', 'cha', 'tnn', 'qxn', 'qvm', 'ful', 'wbp', 'pmf', 'snk', 'mle', 'hlt', 'snn', 'hnj', 'tbz', 'cbs', 'pon', 'toj', 'sps', 'zam', 'kr', 'boa', 'yap', 'mcd', 'tat', 'ahk', 'nwi', 'cbi', 'kyq', 'nfa', 'arl', 'mlg', 'krl', 'kde', 'msk', 'lex', 'lhu', 'iba', 'bps', 'dje', 'kri', 'dgc', 'kmk', 'atd', 'cbu', 'blw', 'huu', 'bba', 'cbt', 'agn', 'mbt', 'myu', 'nuj', 'vmw', 'kwd', 'awb', 'ded', 'sus', 'not', 'tlh', 'atb', 'anp', 'yva', 'hns', 'rmc', 'auc', 'ken', 'bgs', 'nch', 'wrs', 'bkd', 'qve', 'yad', 'ptu', 'bss', 'qvn', 'ada', 'lsi', 'doi', 'zaw', 'enm', 'aia', 'ese', 'tod', 'mbs', 'ctu', 'bpr', 'ngu', 'xog', 'ixl', 'heg', 'gym', 'teo', 'na', 'blz', 'fry', 'obo', 'gyr', 'hbo', 'bzd', 'alp', 'msm', 'nhe', 'qvc', 'cof', 'trv', 'cmo', 'ach', 'att', 'meu', 'agt', 'smk', 'aaz', 'cuk', 'snc', 'bon', 'laj', 'myw', 'dhg', 'mpm', 'run', 'poi', 'nuy', 'ake', 'gcr', 'yaq', 'mgm', 'mps', 'amp', 'alq', 'piu', 'nbl', 'rop', 'bgc', 'prf', 'ata', 'pis', 'seh', 'dgz', 'yaa', 'nst', 'kbh', 'mcf', 'hub', 'csw', 'qwh', 'kau', 'gwi', 'sgb', 'brv', 'zap', 'kqc', 'nhg', 'azz', 'amk', 'apu', 'ruf', 'bcc', 'tuc', 'mxb', 'kos', 'zyp', 'bmu', 'mwf', 'poh', 'dob', 'opm', 'gun', 'yka', 'udu', 'kwi', 'nys', 'gvc', 'abx', 'pls', 'car', 'kje', 'tcz', 'mhx', 'liv', 'cao', 'bdd', 'crx', 'mph', 'amr', 'yml', 'qvw', 'smn', 'cot', 'con', 'kak', 'und', 'ddg', 'mgh', 'lgg', 'sey', 'qul', 'knv', 'zai', 'yle', 'hui', 'bzj', 'ikk', 'aoi', 'tbl', 'hmo', 'mau', 'bvr', 'pib', 'hbs', 'clu', 'cub', 'nhw', 'ncj', 'dzo', 'nod', 'ura', 'ksd', 'mto', 'tzj', 'qvs', 'bqc', 'srq', 'kpx', 'usp', 'gaa', 'aoj', 'acr', 'gul', 'abt', 'yao', 'bbj', 'myx', 'acf', 'bhg', 'suz', 'fij', 'gum', 'xmm', 'row', 'zos', 'lac', 'bhl', 'kjb', 'kbc', 'dgr', 'gup', 'leu', 'ong', 'ktm', 'kcg', 'abs', 'bbr', 'khb', 'pma', 'mcq', 'mwe', 'tbc', 'apr', 'rro', 'dad', 'viv', 'gmv', 'for', 'tte', 'miq', 'tuf', 'mvn', 'ood', 'kew', 'hto', 'wmt', 'tgo', 'sbk', 'xav', 'nrm', 'mdy', 'mpx', 'amf', 'kha', 'zaj', 'dmg', 'hot', 'ppo', 'niu', 'pjt', 'tcs', 'ewo', 'npl', 'lgl', 'aom', 'pau', 'kjs', 'nak', 'zad', 'cjv', 'ntu', 'xtd', 'wbi', 'kmh', 'mwp', 'soq', 'tnc', 'kkl', 'mkz', 'msy', 'ffm', 'amm', 'mti', 'gng', 'mdr', 'brb', 'dnw', 'kdc', 'cav', 'gnn', 'wap', 'wim', 'tay', 'xon', 'rgu', 'cya', 'guz', 'cut', 'cpu', 'ubu', 'auy', 'oji', 'aso', 'mwc', 'aui', 'bya', 'klv', 'zao', 'mmx', 'bbb', 'kpj', 'txu', 'too', 'wat', 'gux', 'kgp', 'zav', 'avk', 'iws', 'gub', 'sab', 'enq', 'lif', 'aly', 'aey', 'nin', 'gdr', 'ikw', 'tif', 'dga', 'pao', 'faa', 'orv', 'nhr', 'usa', 'tmd', 'trc', 'okv', 'mco', 'sbe', 'tiy', 'kvg', 'nna', 'caa', 'plu', 'mva', 'kpg', 'ntp', 'sja', 'beu', 'kpr', 'spm', 'chq', 'aln', 'poe', 'mbh', 'kwj', 'mxt', 'buk', 'esk', 'swp', 'tbo', 'ots', 'sgd', 'adz', 'kwf', 'loz', 'lcm', 'apb', 'kgk', 'bsn', 'geb', 'zac', 'zpu', 'mbc', 'pce', 'maa', 'pad', 'mpj', 'tpu', 'tos', 'qxh', 'sny', 'yal', 'mie', 'zia', 'khz', 'bus', 'sim', 'ttc', 'cta', 'xla', 'zpq', 'tkd', 'avt', 'yet', 'mek', 'crs', 'gvf', 'tku', 'etr', 'gfk', 'gui', 'pwn', 'yua', 'emp', 'agu', 'snl', 'dww', 'kdl', 'ghs', 'wed', 'dwr', 'sea', 'crn', 'aau', 'ebk', 'upv', 'zpm', 'fro', 'mjc', 'rif', 'tbg', 'fai', 'ncu', 'cax', 'bal', 'kkc', 'nxa', 'apz', 'tnk', 'gai', 'nbq', 'gdn', 'tue', 'azg', 'tvk', 'cwe', 'ian', 'dop', 'bsp', 'jac', 'wuv', 'med', 'big', 'rwo', 'yuj', 'lwl', 'gaw', 'guw', 'kaq', 'gvn', 'wmw', 'kjh', 'nii', 'caf', 'bnp', 'mfe', 'kyz', 'ksr', 'bxh', 'xtm', 'agd', 'kpw', 'tlf', 'nlg', 'tfr', 'yss', 'bjp', 'cac', 'ziw', 'kqw', 'wsk', 'wnc', 'poy', 'mca', 'kyc', 'gur', 'wms', 'anv', 'arp', 'krr', 'tgp', 'kbm', 'snp', 'ubr', 'gah', 'sue', 'spp', 'myy', 'djr', 'cfm', 'kup', 'ape', 'ssx', 'bla', 'xbi', 'zpo', 'cap', 'kqf', 'tna', 'yon', 'bch', 'sid', 'tke', 'ign', 'mil', 'ngp', 'zat', 'zza', 'ssg', 'cbv', 'cnl', 'wln', 'iou', 'bjv', 'yut', 'lww', 'wrk', 'rmn', 'muy', 'khs', 'jao', 'agg', 'ndj', 'fli', 'box', 'cso', 'mav', 'qug', 'ann', 'spl', 'cpa', 'sgz', 'mir', 'cpb', 'mpp', 'chf', 'rai', 'zas', 'kqa', 'zpc', 'otq', 'bco', 'stp', 'srd', 'kue', 'kpv', 'yrb', 'tav', 'pot', 'nou', 'bzh', 'cco', 'aon', 'tiw', 'guh', 'bjk', 'lbb', 'quw', 'knj', 'jae', 'bao', 'wbm', 'beo', 'zpv', 'hop', 'jid', 'nko', 'zpz', 'cnr', 'ekk', 'cme', 'mcr', 'eko', 'nyo', 'cui', 'kvn', 'amo', 'cbc', 'ptp', 'hsn', 'qxo', 'glv', 'zaa', 'met', 'wro', 'xnn', 'pbb', 'boj', 'mio', 'rkb', 'rom', 'ctd', 'ztq', 'tnt', 'amn', 'thd', 'bjz', 'qvi', 'knf', 'bak', 'sua', 'mgc', 'byx', 'byr', 'tom', 'tof', 'mxq', 'gam', 'yby', 'mit', 'ycn', 'oss', 'urb', 'shj', 'pab', 'cux', 'hix', 'dah', 'mks', 'cnt', 'pdu', 'hni', 'toi', 'apn', 'pri', 'noa', 'uvl', 'pah', 'cpc', 'taw', 'ajz', 'kmo', 'nhy', 'nss', 'are', 'aby', 'huv', 'bmh', 'pio', 'cle', 'tsz', 'tac', 'kto', 'lue', 'spy', 'aai', 'tuo', 'nde', 'ccp', 'lim', 'bvd', 'kql', 'cuc', 'mig', 'vmy', 'bax', 'yre', 'otm', 'mcp', 'txq', 'mzz', 'nca', 'cjo', 'mlh', 'nab', 'btx', 'kpf', 'agq', 'gjn', 'imo', 'hre', 'tah', 'tpa', 'fue', 'kyf', 'zpl', 'ntj', 'waj', 'uvh', 'kyg', 'ncl', 'zca', 'wiu', 'jni', 'tbf', 'bgt', 'lid', 'eve', 'maj', 'roh', 'kal', 'sbs', 'zar', 'sll', 'xed', 'gil', 'kud', 'blk', 'bmk', 'jic', 'nop', 'wnu', 'urt', 'pir', 'tew', 'awx', 'gnw', 'gvs', 'toc', 'mbl', 'ter', 'ksj', 'mib', 'apw', 'mna', 'naf', 'jvn', 'kiw', 'qup', 'tsw', 'klt', 'mxp', 'nav', 'moh', 'mpt', 'mox', 'guo', 'bmr', 'nhu', 'gof', 'mmo', 'zab', 'lns', 'aer', 'agm', 'bkq', 'mkl', 'bea', 'tly', 'ssd', 'wos', 'ctp', 'ipi', 'far', 'nnq', 'oki', 'mih', 'kze', 'inb', 'gug', 'tee', 'mux', 'msc', 'ota', 'gcf', 'nho', 'mbj', 'ido', 'tar', 'ina', 'uri', 'yuw', 'daa', 'nif', 'mop', 'emi', 'eri', 'cek', 'fub', 'mgw', 'ivb', 'swb', 'tpt', 'kgf', 'mqb', 'zty', 'nr', 'hig', 'ile', 'ppl', 'ses', 'ozm', 'men', 'meq', 'xsm', 'grn', 'tbj', 'kby', 'hbb', 'nzi', 'qvz', 'vid', 'bhd', 'yom', 'srr', 'clo', 'bqp', 'nog', 'wer', 'tuv', 'ojb', 'kon', 'thl', 'mup', 'yin', 'jmx', 'ase', 'sma', 'unr', 'che', 'naq', 'cos', 'mlp', 'hna', 'tuz', 'kqn', 'xsi', 'nhn', 'mee', 'div', 'soy', 'atg', 'sdh', 'thk', 'vol', 'abk', 'lmp', 'bkm', 'swg', 'mfy', 'aoz', 'gwr', 'rmq', 'dwy', 'prg', 'loh', 'duu', 'suk', 'mah', 'zdj', 'sms', 'stk', 'sri', 'boz', 'gbm', 'blt', 'non', 'ybh', 'kkh', 'maq', 'ckt', 'tim', 'ybb', 'lun', 'tpz', 'ktu', 'miz', 'ons', 'isn', 'mcb', 'nvm', 'tvl', 'koo', 'amx', 'urw', 'bfz', 'cpy', 'swc', 'ndo', 'szy', 'vif', 'saq', 'bze', 'fkv', 'nhx', 'ivv', 'chv', 'bag', 'bri', 'aak', 'chz', 'kiu', 'chp', 'nmw', 'syc', 'dov', 'tbk', 'ngn', 'mfh', 'isd', 'syr', 'pkb', 'evn', 'bec', 'crk', 'tdg', 'akb', 'bfm', 'dig', 'kca', 'enb', 'mlw', 'akh', 'bwx', 'tsb', 'mlk', 'nnd', 'yij', 'mhl', 'lhm', 'rnl', 'buo', 'nge', 'nlc', 'cor', 'bgf', 'luc', 'mgo', 'otn', 'syw', 'tvs', 'ify', 'mrn', 'ium', 'rhg', 'tzl', 'ain', 'idu', 'zsr', 'plw', 'omb', 'orh', 'max', 'nd', 'ags', 'kxv', 'rap', 'bra', 'otk', 'tdx', 'ybi', 'gou', 'tiv', 'nym', 'ike', 'ibb', 'mne', 'rar', 'nnb', 'bfd', 'qvo', 'que', 'xkg', 'oro', 'wni', 'cuv', 'hoj', 'mzm', 'bkc', 'tio', 'mnf', 'adh', 'mww', 'dao', 'tkr', 'ppk', 'kwu', 'sbd', 'lbr', 'aaa', 'tob', 'dyo', 'bwt', 'bbk', 'mve', 'nza', 'bob', 'tnl', 'ktz', 'kbx', 'bts', 'hup', 'pov', 'mot', 'mas', 'gos', 'oj', 'rel', 'arg', 'kbo', 'gbi', 'brh', 'mxv', 'alz', 'sxn', 'the', 'vai', 'sas', 'adq', 'bxa', 'pex', 'mfq', 'lgr', 'apy', 'kfm', 'keo', 'mfj', 'lsm', 'yrk', 'shk', 'lia', 'wbr', 'lkt', 'gue', 'dug', 'fuc', 'bud', 'kvt', 'mrw', 'bum', 'yea', 'dak', 'bin', 'nco', 'zyb', 'baw', 'gmh', 'nlv', 'bci', 'tzh', 'tpl', 'mog', 'efi', 'rcf', 'xmg', 'dua', 'lan', 'kua', 'isu', 'wes', 'frm', 'kdj', 'mxu', 'kij', 'lki', 'twu', 'hnn', 'csa', 'plg', 'bgn', 'xbr', 'ifa', 'bgz', 'cjs', 'sga', 'aym', 'shy', 'peg', 'krj', 'rub', 'bwo', 'pdt', 'mmn', 'bht', 'aar', 'cku', 'ceg', 'fvr', 'gaq', 'xsr', 'kln', 'ajg', 'bfa', 'pui', 'chj', 'kyu', 'mjw', 'ahr', 'xty', 'sbl', 'pcg', 'hz', 'ava', 'nce', 'nci', 'gdg', 'mta', 'ttj', 'hoc', 'gkp', 'dip', 'her', 'amc', 'syl', 'acn', 'bhp', 'mbd', 'lot', 'itv', 'myb', 'rmb', 'mwm', 'btt', 'hrx', 'ykg', 'hac', 'tgj', 'ijs', 'smj', 'yas', 'guk', 'hwc', 'mqy', 'mer', 'kqe', 'ljp', 'mnk', 'akl', 'mwv', 'rki', 'pck', 'bik', 'mzi', 'cag', 'kqs', 'mxx', 'muv', 'lah', 'nag', 'moc', 'mvp', 'src', 'miy', 'ngl', 'dts', 'dgo', 'nku', 'nyy', 'pbu', 'kky', 'bgg', 'slr', 'mwn', 'nba', 'mhw', 'mor', 'sei', 'ddn', 'bgr', 'syb', 'aeu', 'kwx', 'knn', 'bhs', 'wwa', 'npy', 'due', 'iii', 'kdi', 'kck', 'tab', 'qxu', 'enx', 'thy', 'shs', 'gqr', 'tdb', 'pnz', 'kdh', 'ena', 'ifu', 'zro', 'kwn', 'byn', 'nyq', 'ryu', 'gpe', 'kxp', 'zom', 'bxk', 'tlb', 'guu', 'nla', 'lob', 'dwu', 'ifk', 'bqi', 'mej', 'sdk', 'mgg', 'rtm', 'kum', 'mnb', 'xcl', 'ssn', 'odk', 'zne', 'nbe', 'sda', 'afb', 'njo', 'mzh', 'bru', 'mhi', 'czt', 'tem', 'kfo', 'lew', 'vun', 'gia', 'kru', 'nlx', 'tby', 'abz', 'udg', 'kng', 'lhi', 'mbf', 'mbi', 'bua', 'iku', 'lzz', 'akk', 'cri', 'btd', 'mtg', 'rjs', 'hms', 'dng', 'sgh', 'bku', 'chb', 'egl', 'gwd', 'szb', 'mrq', 'kpz', 'dis', 'chn', 'esi', 'hdy', 'isk', 'kbr', 'mck', 'duo', 'ghl', 'kfx', 'mbq', 'msn', 'trs', 'dyi', 'urk', 'zoh', 'sjn', 'mip', 'nid', 'xnr', 'kun', 'tts', 'urh', 'leh', 'ttv', 'oym', 'qxs', 'zak', 'kye', 'hoy', 'dnj', 'kap', 'mnx', 'nsa', 'pum', 'djm', 'hag', 'nim', 'sgc', 'way', 'pny', 'hra', 'haz', 'gyd', 'spn', 'zyg', 'agw', 'njn', 'nbu', 'aqz', 'dru', 'tll', 'bgj', 'tqo', 'skg', 'sch', 'chu', 'rui', 'wdd', 'njz', 'lag', 'ksc', 'nyk', 'rag', 'sop', 'wic', 'xte', 'bej', 'njm', 'nau', 'ury', 'kwv', 'bje', 'tnr', 'sro', 'otd', 'dbq', 'irk', 'yns', 'kfs', 'mim', 'bfw', 'trq', 'qus', 'tfn', 'kks', 'alh', 'lmn', 'bdv', 'suj', 'heh', 'mqu', 'rmo', 'ciw', 'srb', 'uzs', 'kxm', 'kpo', 'kmm', 'erk', 'klr', 'tui', 'iso', 'unx', 'nmn', 'lam', 'bez', 'tpn', 'koh', 'tvu', 'bwd', 'biy', 'rad', 'chw', 'apt', 'izh', 'nyb', 'pai', 'skt', 'kjp', 'acw', 'cas', 'ihp', 'mgr', 'kgr', 'xtc', 'ngt', 'bom', 'qxl', 'gqa', 'wlv', 'ekg', 'yan', 'dib', 'cnk', 'nzm', 'mwq', 'tdj', 'pmq', 'kff', 'pak', 'alj', 'mua', 'miu', 'bji', 'nut', 'sba', 'ibg', 'snw', 'yli', 'asa', 'nmf', 'lti', 'tli', 'tdc', 'sux', 'one', 'any', 'dws', 'hay', 'mul', 'cld', 'gej', 'tvt', 'nfu', 'lwg', 'khg', 'shh', 'mye', 'tog', 'gyn', 'lub', 'rml', 'bno', 'jup', 'kmw', 'kuj', 'bbw', 'crt', 'lea', 'bmv', 'ndh', 'bsk', 'pst', 'meo', 'ggw', 'shu', 'adj', 'sby', 'ogc', 'crj', 'muh', 'mjg', 'tao', 'zag', 'tlj', 'esu', 'dhv', 'pem', 'jaa', 'qva', 'lev', 'nnw', 'khw', 'mzj', 'ort', 'cnw', 'jit', 'jya', 'kgo', 'tek', 'pne', 'jmc', 'kfc', 'end', 'sef', 'gby', 'bou', 'shb', 'avu', 'aro', 'yrl', 'haq', 'mvv', 'won', 'pqm', 'nio', 'dar', 'tsg', 'ngj', 'lud', 'mgu', 'klb', 'bjg', 'mtp', 'nmc', 'zin', 'gbk', 'ocu', 'xmv', 'ldn', 'xdy', 'ifb', 'ndc', 'cyb', 'aqt', 'pse', 'nnp', 'nmm', 'bdq', 'mrz', 'mfm', 'zpi', 'ale', 'sgw', 'lut', 'btm', 'lui', 'ayo', 'frc', 'laa', 'bpx', 'xsb', 'ker', 'trn', 'tsi', 'lbm', 'cla', 'rim', 'nmz', 'icr', 'mur', 'nyh', 'guq', 'lnd', 'kit', 'hdn', 'wno', 'tap', 'sbp', 'mmy', 'lbj', 'mjl', 'coe', 'mns', 'yak', 'mif', 'bhw', 'gnd', 'maw', 'iry', 'lme', 'bkl', 'zun', 'bcj', 'sck', 'ttq', 'kjd', 'sdc', 'mfi', 'soz', 'eip', 'biu', 'ktb', 'ahg', 'mat', 'yae', 'cgg', 'xkv', 'kix', 'trp', 'msi', 'bfj', 'tih', 'bnj', 'arr', 'shr', 'kus', 'anu', 'thf', 'fit', 'kjc', 'trf', 'emk', 'nit', 'rwr', 'bdh', 'ssp', 'tmf', 'mov', 'bbq', 'ikt', 'tls', 'rin', 'lmk', 'ute', 'suc', 'cay', 'lis', 'gww', 'goh', 'nih', 'adi', 'unk', 'wsg', 'aol', 'buw', 'hmr', 'waw', 'thq', 'bva', 'nph', 'mix', 'sac', 'hea', 'atq', 'ksb', 'dma', 'cce', 'lai', 'kxj', 'chm', 'sil', 'odu', 'ldi', 'mrh', 'bot', 'ebu', 'ksp', 'did', 'see', 'enl', 'dcr', 'mla', 'kqi', 'mzp', 'shg', 'snf', 'bim', 'wca', 'ntk', 'bni', 'kod', 'pmy', 'rwk', 'moz', 'kzf', 'mde', 'kyh', 'old', 'bqt', 'aim', 'abq', 'wau', 'ctz', 'yur', 'nbc', 'raw', 'ikx', 'bkw', 'tsc', 'bzt', 'dav', 'bvz', 'say', 'dnd', 'pyu', 'dna', 'cob', 'mmd', 'whk', 'ncr', 'kmn', 'tcd', 'kxc', 'byo', 'aha', 'inj', 'mma', 'sos', 'dry', 'ijj', 'mct', 'kai', 'pcj', 'bsf', 'nbh', 'bky', 'mxy', 'mtj', 'dun', 'vor', 'ner', 'tyr', 'ney', 'cul', 'tpp', 'khq', 'nux', 'diw', 'glh', 'mkw', 'noz', 'wbj', 'lsh', 'bkv', 'gab', 'kli', 'lwo', 'qud', 'kot', 'yix', 'cdn', 'twe', 'tru', 'pmm', 'pht', 'kpm', 'kay', 'kcd', 'pha', 'wmb', 'sau', 'ilk', 'ala', 'mmp', 'gvp', 'uiv', 'bpv', 'nwb', 'csh', 'brg', 'nps', 'hkk', 'msh', 'pku', 'nfl', 'nlu', 'ngb', 'uya', 'eka', 'rab', 'ora', 'gkn', 'luj', 'wle', 'juk', 'bhb', 'bof', 'pow', 'fod', 'cch', 'bei', 'kao', 'hnd', 'gud', 'peb', 'gxx', 'ijc', 'cbn', 'cvn', 'sse', 'abo', 'ebo', 'slc', 'neb', 'sso', 'zns', 'qxq', 'rji', 'dmr', 'jml', 'kfq', 'cjp', 'trd', 'mnl', 'nke', 'xvi', 'sao', 'sly', 'dbj', 'bcy', 'les', 'nkb', 'snq', 'svb', 'grx', 'tex', 'psh', 'lbf', 'asr', 'nzk', 'njh', 'dge', 'ppm', 'sxw', 'kvy', 'kwb', 'mmh', 'ybe', 'zae', 'bhf', 'zoc', 'cqd', 'otr', 'cdm', 'prx', 'uki', 'doo', 'psi', 'umm', 'duc', 'kgy', 'lil', 'tlx', 'tug', 'btu', 'czn', 'arx', 'kpc', 'sir', 'ife', 'lva', 'sob', 'yis', 'tld', 'gru', 'awi', 'wib', 'hue', 'jab', 'gbr', 'lla', 'klg', 'lmu', 'ifm', 'bsq', 'arw', 'xok', 'pnu', 'bte', 'bpz', 'sad', 'hed', 'gni', 'ati', 'mtr', 'jeb', 'pbv', 'stf', 'bbu', 'bse', 'ade', 'kqj', 'goa', 'wob', 'njj', 'pmx', 'coz', 'mcw', 'pqa', 'bww', 'izr', 'tuy', 'kmq', 'ngc', 'erg', 'agx', 'jow', 'vut', 'pav', 'fud', 'nma', 'duw', 'bdm', 'gel', 'wmd', 'gmb', 'uta', 'akr', 'klu', 'szp', 'naj', 'hux', 'ydg', 'sst', 'ado', 'grd', 'dzg', 'sbc', 'vra', 'yah', 'mbu', 'nkw', 'nlo', 'xmw', 'yes', 'she', 'bpp', 'jkp', 'tpr', 'agh', 'las', 'dtb', 'oku', 'blr', 'niq', 'bdi', 'ijn', 'pko', 'aad', 'tlp', 'iru', 'snv', 'blf', 'zyn', 'alx', 'xub', 'siw', 'thr', 'vmk', 'lkn', 'mnp', 'bmb', 'doy', 'tft', 'yam', 'kgq', 'nyi', 'mjs', 'muz', 'aaw', 'tkl', 'bdb', 'lbn', 'pua', 'nuo', 'vas', 'tix', 'kcf', 'zps', 'fie', 'noe', 'tri', 'ktn', 'clk', 'lec', 'huh', 'mpq', 'itr', 'bpn', 'tdf', 'fuy', 'oni', 'nbb', 'mtt', 'etu', 'phk', 'mxd', 'der', 'bgi', 'dmo', 'loe', 'ged', 'pbg', 'rau', 'nez', 'kex', 'sel', 'yaz', 'kfb', 'dus', 'kzm', 'nka', 'kfd', 'sng', 'bwu', 'crq', 'ank', 'ats', 'dmw', 'ipo', 'mqx', 'dbb', 'bwm', 'gvj', 'kic', 'tti', 'isi', 'mqn', 'thz', 'cra', 'yuf', 'mzw', 'cmr', 'amt', 'nqg', 'xac', 'bgp', 'kef', 'caq', 'kdx', 'nev', 'kmz', 'sjr', 'aks', 'knm', 'lem', 'adx', 'cfa', 'qwa', 'wow', 'kxh', 'nnc', 'ndp', 'wew', 'zzj', 'kvd', 'gew', 'bvw', 'txa', 'mgf', 'dih', 'tyz', 'anf', 'ott', 'mzb', 'dee', 'nen', 'lur', 'svc', 'dot', 'nmh', 'nyj', 'scp', 'utr', 'wgb', 'jku', 'igl', 'smw', 'etx', 'tol', 'yra', 'ahb', 'pbs', 'teq', 'zln', 'onj', 'pek', 'dgh', 'rog', 'yup', 'jns', 'alk', 'wdj', 'gid', 'saz', 'cod', 'ctl', 'hei', 'kfp', 'ttk', 'sgp', 'gga', 'wbk', 'awe', 'kyo', 'meh', 'acd', 'pia', 'ema', 'bxq', 'tmn', 'fun', 'saw', 'kdp', 'mvf', 'tgy', 'cja', 'agc', 'geg', 'lyg', 'ywq', 'dio', 'qvj', 'boh', 'fab', 'tlq', 'bca', 'int', 'hmb', 'lbo', 'mhy', 'mqz', 'mrg', 'knd', 'gra', 'tbt', 'mgb', 'aof', 'ayu', 'lbu', 'loy', 'win', 'bsc', 'khn', 'tma', 'lar', 'fng', 'mbz', 'dei', 'iyo', 'kdq', 'kvb', 'ywa', 'jum', 'sev', 'mbm', 'tgd', 'xmh', 'mij', 'ppq', 'moi', 'aki', 'aix', 'skb', 'szg', 'dia', 'ekl', 'fpe', 'kfk', 'siy', 'bye', 'wof', 'syk', 'gox', 'hia', 'pwm', 'yer', 'ogo', 'akw', 'sha', 'alf', 'hae', 'gut', 'mlu', 'tbw', 'cll', 'hmt', 'tes', 'pln', 'fll', 'prk', 'sdq', 'sew', 'etn', 'rgs', 'ncm', 'xkb', 'kzs', 'xky', 'czh', 'vrs', 'gpa', 'bgv', 'une', 'lih', 'mho', 'ncx', 'ttm', 'auu', 'kub', 'oub', 'tdh', 'afz', 'lro', 'bav', 'guf', 'yum', 'cbg', 'zik', 'raj', 'gaj', 'nhv', 'slp', 'tce', 'bhj', 'pnq', 'nbm', 'bqw', 'vmz', 'knx', 'bap', 'bcf', 'dbd', 'thp', 'iri', 'ilp', 'spu', 'gad', 'klx', 'yki', 'tul', 'mlf', 'agb', 'mlq', 'brd', 'hur', 'ndb', 'prn', 'rmt', 'xes', 'asy', 'bzu', 'vag', 'aez', 'lgq', 'ksm', 'krs', 'xsu', 'nfr', 'com', 'dri', 'mez', 'thv', 'kla', 'wlo', 'tnm', 'tow', 'caz', 'bbi', 'kwy', 'ono', 'sed', 'maf', 'zyj', 'yey', 'bcg', 'cdj', 'khl', 'hah', 'oru', 'jun', 'qum', 'siu', 'kyk', 'bdl', 'cwt', 'bns', 'jer', 'anj', 'ybl', 'kwo', 'deg', 'lmy', 'cdf', 'cjm', 'wgi', 'cdh', 'gnm', 'mug', 'aca', 'grh', 'idi', 'kbb', 'weo', 'sjo', 'vam', 'hul', 'nng', 'mch', 'moe', 'bwf', 'pll', 'pei', 'ppt', 'bdw', 'wci', 'pkt', 'otw', 'dny', 'lop', 'xuu', 'hoo', 'mga', 'nbr', 'tvd', 'lse', 'djc', 'hoe', 'rol', 'jen', 'lmg', 'igs', 'scv', 'nrg', 'cte', 'kjr', 'mtq', 'mwt', 'ygr', 'pic', 'ask', 'mfo', 'ktv', 'bfu', 'dox', 'smy', 'rhp', 'eto', 'lnu', 'mfn', 'sjm', 'kzi', 'nnz', 'kfy', 'gbe', 'ayz', 'huc', 'bnm', 'fap', 'org', 'poo', 'swi', 'soc', 'lgu', 'gdb', 'bhr', 'gro', 'tkg', 'kem', 'puu', 'biz', 'mdw', 'sou', 'fay', 'qun', 'gim', 'aee', 'xtn', 'uhn', 'brl', 'mlx', 'kog', 'sor', 'mcu', 'gnu', 'bit', 'zkd', 'wom', 'mng', 'liz', 'wji', 'zrs', 'twh', 'qxp', 'scs', 'nuf', 'aqg', 'bov', 'tpm', 'nbp', 'hve', 'lef', 'bbt', 'nda', 'opa', 'bio', 'kmt', 'aap', 'wls', 'vem', 'sam', 'xpe', 'kei', 'dby', 'iti', 'gya', 'ldj', 'mrr', 'mds', 'apm', 'hml', 'gbg', 'yll', 'itz', 'ola', 'aec', 'mnm', 'bwr', 'mdb', 'wtm', 'jub', 'lu', 'tii', 'kvw', 'bet', 'dsh', 'ymb', 'mzk', 'bmm', 'lky', 'sne', 'raa', 'fuu', 'asu', 'sto', 'bxg', 'glw', 'cia', 'jgk', 'git', 'drg', 'arh', 'lip', 'agf', 'nal', 'sol', 'kzq', 'bly', 'nxq', 'pcn', 'aik', 'knk', 'cko', 'ofu', 'iow', 'tdk', 'gvo', 'pos', 'toh', 'ano', 'kih', 'suv', 'zpx', 'ztg', 'bvc', 'thm', 'ksv', 'ayi', 'mfl', 'wry', 'zwa', 'enn', 'diz', 'uuu', 'abr', 'ich', 'kxb', 'suy', 'mbp', 'bhz', 'ego', 'lok', 'bta', 'jaq', 'lma', 'lor', 'mmc', 'mme', 'ngs', 'soj', 'dme', 'nlj', 'ghk', 'nyd', 'bwq', 'cns', 'kvv', 'app', 'kfi', 'nud', 'twp', 'kmi', 'ndu', 'doa', 'crw', 'lcp', 'asc', 'akq', 'ggu', 'nyf', 'klz', 'bhy', 'ppi', 'pid', 'bqs', 'cou', 'leq', 'mte', 'cbo', 'ald', 'ndv', 'lbx', 'mrp', 'fip', 'mdh', 'gea', 'aoa', 'coh', 'nxr', 'xri', 'krf', 'akd', 'bun', 'ess', 'erh', 'lep', 'hmd', 'mcs', 'wsa', 'bcz', 'lap', 'cdr', 'kgb', 'mki', 'mpc', 'dim', 'dbi', 'lml', 'ldl', 'wnp', 'zmb', 'esg', 'nms', 'blb', 'coj', 'glj', 'sbh', 'ncb', 'ssb', 'aab', 'scw', 'png', 'gua', 'khe', 'kjg', 'gdu', 'bvm', 'boq', 'kez', 'mtf', 'ahl', 'coc', 'ths', 'twy', 'anm', 'xem', 'ibl', 'zim', 'cyo', 'itd', 'lkr', 'shq', 'jiu', 'dos', 'gaf', 'jio', 'knt', 'dsq', 'mue', 'dya', 'mse', 'koe', 'kol', 'fqs', 'mji', 'tsr', 'yev', 'ems', 'ndy', 'mdt', 'sry', 'bli', 'bby', 'cin', 'cvg', 'ymm', 'wrm', 'sbb', 'adl', 'pay', 'ukp', 'kys', 'akc', 'sgr', 'kdr', 'wog', 'tgs', 'msw', 'yot', 'how', 'mnz', 'iqw', 'lvk', 'pac', 'prc', 'sdp', 'aal', 'tqu', 'daq', 'msg', 'skx', 'iai', 'sho', 'ntm', 'jms', 'dva', 'byv', 'dhm', 'nbn', 'jle', 'lir', 'dni', 'oka', 'blc', 'bpw', 'byd', 'iwm', 'tis', 'kcj', 'wja', 'afu', 'bst', 'mrm', 'abm', 'xod', 'fak', 'bmd', 'shw', 'tdy', 'vah', 'nsm', 'kul', 'juo', 'aup', 'skj', 'mhc', 'tus', 'tro', 'nbi', 'khj', 'tgc', 'jdt', 'tal', 'yba', 'pci', 'tmr', 'ndx', 'knp', 'kvf', 'doz', 'hru', 'elm', 'brp', 'mtu', 'pww', 'iqu', 'mzv', 'raf', 'pwo', 'sif', 'bqj', 'akf', 'ksi', 'mfd', 'hwo', 'puo', 'pbp', 'kfe', 'bbf', 'rbb', 'frd', 'mzr', 'dow', 'kqy', 'owi', 'plv', 'blh', 'ztp', 'pym', 'sdg', 'ram', 'bza', 'dor', 'epi', 'vkl', 'dgg', 'tmy', 'gmm', 'bcr', 'lje', 'bcs', 'bys', 'kkk', 'sky', 'slz', 'gol', 'kji', 'nun', 'bif', 'piv', 'moy', 'kpe', 'ruk', 'scu', 'sgi', 'tdl', 'blm', 'sbr', 'mtl', 'aac', 'mrl', 'jig', 'knw', 'flr', 'led', 'tei', 'bcp', 'kel', 'ysn', 'mea', 'crc', 'ilu', 'bja', 'asi', 'quv', 'zts', 'bex', 'kxw', 'wmo', 'wsi', 'bpu', 'hum', 'gae', 'kjq', 'ayb', 'mey', 'kza', 'oke', 'bzv', 'jna', 'klq', 'vmp', 'bil', 'cae', 'jma', 'twx', 'anc', 'mwg', 'vaa', 'duq', 'mmg', 'sen', 'lht', 'mlv', 'tty', 'bid', 'mvz', 'nkh', 'sep', 'van', 'zkr', 'bfh', 'ayg', 'jax', 'dhd', 'ahs', 'umu', 'kzr', 'zng', 'dsn', 'bxl', 'bzz', 'lle', 'mda', 'gdf', 'fan', 'mfg', 'hgm', 'ril', 'sym', 'was', 'jmd', 'kjl', 'biv', 'pru', 'diu', 'liq', 'hts', 'mjt', 'ngi', 'acv', 'bcw', 'dil', 'kkj', 'qxa', 'huf', 'wyy', 'ore', 'ccg', 'bzy', 'akp', 'bzx', 'dbm', 'lmx', 'nnh', 'gju', 'bjh', 'ert', 'vnk', 'mcc', 'akg', 'ica', 'nuk', 'log', 'pca', 'wme', 'auk', 'scl', 'anx', 'yig', 'twm', 'tmc', 'eja', 'smq', 'txy', 'nnu', 'cog', 'ato', 'qui', 'iar', 'iko', 'jmb', 'lic', 'apj', 'saf', 'bor', 'cry', 'law', 'job', 'dai', 'kxf', 'yog', 'nbv', 'ekp', 'swj', 'stv', 'bsh', 'mfc', 'kaj', 'msj', 'kvm', 'byp', 'hbn', 'khr', 'mzq', 'pbn', 'gnb', 'fmp', 'abn', 'jru', 'abu', 'gnk', 'hav', 'jpa', 'ksf', 'kee', 'dhi', 'ets', 'uba', 'mgl', 'scg', 'kqo', 'lom', 'cly', 'vay', 'sts', 'krh', 'zgb', 'dta', 'lkh', 'lsr', 'nxg', 'ycl', 'txn', 'emg', 'drd', 'ndr', 'muk', 'kio', 'bey', 'kpq', 'kle', 'iyx', 'pkh', 'kcl', 'drs', 'bqr', 'tsu', 'aio', 'mrf', 'gvl', 'pga', 'xkl', 'bft', 'kib', 'mgp', 'aqm', 'apd', 'hlb', 'ral', 'xmz', 'dem', 'sde', 'nkx', 'spo', 'mef', 'tmq', 'kma', 'tau', 'nir', 'dof', 'lna', 'wan', 'mfb', 'chx', 'bdu', 'myh', 'sax', 'tuq', 'swo', 'vaj', 'lou', 'tsa', 'ish', 'kmy', 'bqg', 'naw', 'tlr', 'ldb', 'nti', 'zbc', 'ass', 'soe', 'kfa', 'brq', 'lga', 'ogb', 'mfv', 'sek', 'sns', 'glo', 'blq', 'fla', 'cdz', 'weh', 'zmp', 'xkn', 'dbn', 'jbj', 'sjg', 'kga', 'mjx', 'izz', 'cto', 'sui', 'eot', 'oia', 'mke', 'bfs', 'kbj', 'mpd', 'jmr', 'bau', 'kvu', 'ite', 'kdu', 'nga', 'bzw', 'skd', 'keb', 'kph', 'bwe', 'nzy', 'gow', 'xrw', 'ldm', 'mrt', 'sdo', 'alu', 'mmm', 'mwa', 'bjt', 'emb', 'wti', 'ver', 'lee', 'wad', 'aji', 'yun', 'avi', 'bcn', 'bev', 'tpx', 'khy', 'sig', 'prq', 'aif', 'fal', 'dcc', 'qya', 'dij', 'saj', 'mgd', 'aul', 'add', 'ckl', 'kxn', 'lrl', 'hvv', 'ksg', 'lol', 'wut', 'eyo', 'keu', 'pta', 'kno', 'anw', 'myl', 'onp', 'uar', 'kcx', 'irn', 'pkg', 'bde', 'ula', 'kom', 'niw', 'nil', 'ksn', 'ums', 'afo', 'kvq', 'bek', 'sgy', 'poc', 'ndz', 'kss', 'bvi', 'mae', 'ynq', 'azm', 'mtk', 'tds', 'tcf', 'ctg', 'khu', 'jul', 'gwa', 'kge', 'mdd', 'mdm', 'kmd', 'nto', 'kmc', 'kie', 'mvo', 'pmi', 'wlx', 'vig', 'mcn', 'stj', 'zhi', 'ndi', 'pcc', 'var', 'kdd', 'ilb', 'nmk', 'moj', 'nnj', 'cje', 'yim', 'kvr', 'gde', 'kcv', 'bcv', 'mev', 'mpr', 'pil', 'kpl', 'psn', 'lbw', 'cfg', 'gis', 'agl', 'skv', 'gdx', 'tdn', 'tpe', 'udl', 'chl', 'tye', 'vmm', 'cpx', 'mpn', 'lek', 'tsv', 'mlm', 'aao', 'iki', 'tyn', 'wod', 'aun', 'sav', 'sip', 'zaz', 'lch', 'wlw', 'ttw', 'lln', 'afe', 'dgd', 'kml', 'srz', 'afh', 'ckx', 'sjl', 'dur', 'dtm', 'ghn', 'clc', 'nar', 'bfg', 'mhu', 'haj', 'ybj', 'alw', 'bcq', 'kil', 'dnn', 'tcx', 'zpn', 'bnn', 'beh', 'oyb', 'bbp', 'wbb', 'bzk', 'tcn', 'mbx', 'cro', 'tjg', 'beq', 'txt', 'sce', 'kdm', 'mln', 'gop', 'baa', 'kdz', 'adn', 'bfq', 'hal', 'xmt', 'bgd', 'bwi', 'buf', 'kwk', 'qux', 'sug', 'nkk', 'atk', 'pbo', 'rir', 'giz', 'moa', 'lig', 'kuy', 'jnj', 'kdt', 'brr', 'ler', 'grs', 'xom', 'azd', 'nup', 'sys', 'bfy', 'eky', 'awn', 'kia', 'orx', 'prm', 'ygw', 'cbj', 'naz', 'ncf', 'tiq', 'tbp', 'wwo', 'all', 'nri', 'ykm', 'niz', 'tdd', 'mxn', 'lul', 'jaf', 'kzc', 'mxj', 'ugo', 'gcn', 'rwa', 'mfz', 'mza', 'los', 'xer', 'pre', 'mkg', 'aot', 'bmf', 'lum', 'kwl', 'gri', 'ekr', 'kcr', 'gbz', 'dwa', 'ebr', 'kfz', 'sya', 'irx', 'muo', 'ulu', 'nru', 'gog', 'taz', 'cik', 'nhb', 'bnx', 'dre', 'gcd', 'mro', 'nac', 'ayn', 'khc', 'bws', 'ywn', 'sbg', 'mnv', 'pwa', 'gek', 'gwt', 'brt', 'tkp', 'kwa', 'iby', 'nnm', 'rav', 'mmz', 'kvj', 'cbd', 'pfe', 'bpa', 'mls', 'sqq', 'hid', 'ige', 'twf', 'nzb', 'har', 'mnj', 'zlj', 'kbz', 'sur', 'sru', 'mbv', 'oma', 'kbl', 'nhd', 'vap', 'aba', 'plc', 'tsj', 'mxe', 'klw', 'asb', 'has', 'dms', 'mxm', 'rey', 'mhk', 'set', 'lgt', 'sti', 'psa', 'ssk', 'ega', 'gna', 'okr', 'mdj', 'llu', 'neq', 'tkq', 'svs', 'eme', 'kst', 'nli', 'gvr', 'arv', 'ldk', 'tdo', 'wbf', 'lax', 'xks', 'yoy', 'tww', 'eza', 'mkc', 'nfd', 'phl', 'lyn', 'axk', 'njb', 'kbv', 'nja', 'psw', 'tva', 'brf', 'fut', 'kws', 'szv', 'jbu', 'mhz', 'swv', 'ael', 'tic', 'crl', 'hno', 'krp', 'had', 'hoa', 'ngz', 'ktp', 'yuz', 'ssy', 'tdv', 'fir', 'cli', 'kxz', 'bks', 'mfa', 'can', 'emn', 'qxr', 'bqv', 'yaw', 'buh', 'bux', 'bsb', 'kci', 'cua', 'lup', 'elk', 'foi', 'ibd', 'bzf', 'gdl', 'sge', 'kfr', 'llc', 'kwg', 'nre', 'ghc', 'msl', 'avn', 'ble', 'hio', 'rnd', 'xnz', 'hca', 'bhh', 'gwn', 'ida', 'duv', 'krx', 'tji', 'pip', 'dax', 'yyu', 'cnb', 'kvo', 'bkr', 'ior', 'hit', 'igb', 'haa', 'wlc', 'mgv', 'xra', 'tra', 'oyd', 'kkz', 'kls', 'sss', 'jge', 'kgj', 'dir', 'pdo', 'bab', 'wbl', 'jeh', 'bee', 'tba', 'grt', 'mpg', 'hut', 'afi', 'slu', 'nhp', 'avd', 'aog', 'kna', 'onn', 'yaf', 'kid', 'ria', 'kni', 'nat', 'rkm', 'bbv', 'plk', 'res', 'zaf', 'orz', 'xns', 'wli', 'srl', 'bjc', 'sok', 'nmb', 'esh', 'pof', 'ito', 'kpk', 'xta', 'yay', 'sku', 'bda', 'sbu', 'orc', 'lra', 'vaf', 'abi', 'mtd', 'gar', 'ncg', 'mym', 'mgk', 'plr', 'liu', 'aiw', 'bsy', 'suq', 'nsu', 'mbo', 'rao', 'tik', 'jmi', 'kwe', 'age', 'gno', 'jqr', 'kis', 'tcc', 'zay', 'osi', 'tgw', 'oks', 'jib', 'daw', 'gax', 'tpj', 'cfd', 'twb', 'bgq', 'mkf', 'hol', 'pcb', 'pss', 'kqb', 'bhq', 'bkk', 'ndm', 'dgi', 'kht', 'kof', 'rah', 'zpw', 'kad', 'jei', 'bub', 'ggb', 'loa', 'aty', 'kny', 'bmi', 'ttr', 'zpr', 'bxb', 'kps', 'byz', 'swl', 'bth', 'mpe', 'mql', 'giw', 'mwi', 'bib', 'pbi', 'dks', 'eit', 'tsx', 'ckh', 'ung', 'bol', 'puc', 'txo', 'mep', 'lae', 'bnv', 'djn', 'ccj', 'myp', 'wrp', 'yui', 'lpa', 'bep', 'mgi', 'llp', 'lbq', 'tan', 'yuy', 'ndd', 'ted', 'sie', 'niy', 'lcc', 'xwg', 'bsi', 'kui', 'mhs', 'des', 'amb', 'dho', 'tny', 'cmi', 'irr', 'kow', 'bgx', 'kki', 'jog', 'mdu', 'gbo', 'smf', 'kuh', 'pcl', 'hgw', 'pbc', 'mxh', 'seg', 'ssi', 'hmz', 'tnv', 'ztl', 'tpq', 'fad', 'xwe', 'bka', 'kcq', 'kev', 'hrm', 'hmj', 'mml', 'pom', 'mut', 'kra', 'bqa', 'sld', 'clj', 'kkf', 'kjt', 'bqh', 'xmc', 'key', 'pcw', 'bqx', 'zuy', 'kmj', 'yiz', 'tkx', 'bqo', 'shc', 'nhz', 'gbv', 'xum', 'pch', 'mel', 'yde', 'soi', 'vmh', 'hii', 'hmg', 'grv', 'ukw', 'kpb', 'bge', 'zte', 'awu', 'mum', 'gbh', 'mzl', 'mik', 'bbo', 'jad', 'zpf', 'bhx', 'nix', 'olu', 'xkj', 'bvy', 'nyw', 'asg', 'uss', 'bui', 'agi', 'sbx', 'skn', 'gyz', 'vum', 'kfu', 'mrv', 'zpe', 'mpu', 'rme', 'zmq', 'nsy', 'cnc', 'lrm', 'nuq', 'gba', 'vmj', 'xuo', 'jbm', 'mjv', 'duh', 'lrk', 'mdk', 'sez', 'sjp', 'whg', 'pwr', 'kch', 'nuz', 'tfi', 'bzs', 'swr', 'ojs', 'tnb', 'yno', 'bga', 'kkd', 'kip', 'nyg', 'tou', 'buz', 'mfk', 'crm', 'buj', 'xdo', 'yhd', 'rei', 'tcp', 'avl', 'phr', 'kkn', 'kwc', 'zau', 'bkg', 'knl', 'kvx', 'tkt', 'goz', 'crv', 'agy', 'bah', 'shm', 'mvg', 'sbz', 'njs', 'okh', 'knu', 'akt', 'cgk', 'tov', 'swk', 'cdi', 'zbu', 'bma', 'eze', 'sre', 'zph', 'mdn', 'klo', 'wud', 'ayp', 'pdn', 'yeu', 'mxl', 'anr', 'btg', 'nes', 'gry', 'pxm', 'mkk', 'sfm', 'kyy', 'ldp', 'mkb', 'gjk', 'zcd', 'ksu', 'anl', 'zpy', 'bmj', 'xtt', 'dgx', 'xsq', 'lel', 'ors', 'yax', 'ruz', 'wss', 'kwt', 'ost', 'kzn', 'aoc', 'dhn', 'xkc', 'mxs', 'gec', 'goj', 'bvu', 'stt', 'lgm', 'kmp', 'aww', 'stn', 'smu', 'bjx', 'tth', 'ztx', 'uth', 'cih', 'bix', 'ity', 'kku', 'toq', 'jmn', 'xsn', 'sju', 'ktf', 'wbq', 'jda', 'ldq', 'kqm', 'bfe', 'mjz', 'jnd', 'sle', 'tgt', 'cox', 'lpn', 'ymk', 'dzl', 'nxk', 'ahp', 'bfb', 'das', 'kvi', 'jdg', 'zpj', 'tks', 'lmd', 'bhi', 'kdy', 'def', 'bxs', 'lal', 'kvl', 'url', 'tto', 'zkn', 'snm', 'saa', 'ccl', 'nmi', 'dza', 'mqh', 'bac', 'tvn', 'fuq', 'rat', 'byc', 'xtl', 'gmz', 'aug', 'kxx', 'kyb', 'ttb', 'cky', 'ont', 'mrd', 'npb', 'wba', 'mii', 'sbn', 'oso', 'lpo', 'buu', 'sdr', 'krw', 'mhp', 'vmc', 'lhp', 'byj', 'ghr', 'acz', 'klk', 'sfw', 'dez', 'krn', 'pbl', 'knz', 'its', 'bgw', 'azt', 'tla', 'abh', 'mab', 'zpd', 'cde', 'kpa', 'nkf', 'cnq', 'ldo', 'otx', 'sjb', 'nos', 'yuq', 'glr', 'ktc', 'aoe', 'ogg', 'rng', 'zpg', 'tqb', 'lri', 'gbl', 'xgu', 'dkx', 'kjo', 'mku', 'zpk', 'bvh', 'ctt', 'blo', 'okx', 'wem', 'nwm', 'yat', 'dbv', 'clt', 'twr', 'lmi', 'kyv', 'aaf', 'dde', 'pgg', 'ruy', 'nlk', 'lhl', 'xuj', 'rdb', 'dub', 'tja', 'jat', 'itt', 'krv', 'bmq', 'nao', 'deh', 'ywl', 'djo', 'zeh', 'gso', 'dyg', 'kft', 'liw', 'tyy', 'mxa', 'hld', 'tef', 'zpp', 'ysp', 'kqk', 'zrg', 'nki', 'bip', 'kcs', 'zms', 'yif', 'gas', 'xwl', 'kfh', 'lik', 'smh', 'kce', 'kep', 'ktj', 'ibm', 'rmz', 'pcf', 'xkf', 'atu', 'xkk', 'cma', 'gok', 'dln', 'zpa', 'zpt', 'pmj', 'bpe', 'cwd', 'spt', 'njx', 'uis', 'phq', 'xkt', 'srx', 'tcu', 'nmo', 'kty', 'nof', 'gez', 'hmw', 'cov', 'tga', 'mqg', 'xti', 'sgj', 'cib', 'boo', 'slx', 'sct', 'vkn', 'ncq', 'bjj', 'yiq', 'bjo', 'nse', 'jkr', 'ckm', 'ayt', 'itl', 'atp', 'tml', 'cok', 'xkz', 'tsp', 'bhu', 'pug', 'drt', 'sci', 'csk', 'pbm', 'wkd', 'tak', 'lie', 'grj', 'mnu', 'env', 'prt', 'vmx', 'nxd', 'piy', 'pps', 'pwb', 'etc', 'kej', 'ldg', 'usi', 'dka', 'pud', 'jiy', 'dwz', 'xrb', 'nct', 'vav', 'bfr', 'ghe', 'smt', 'gbn', 'mtb', 'luz', 'auq', 'gau', 'ykk', 'kqp', 'ntr', 'jrt', 'ggg', 'kfv', 'ngw', 'xtj', 'npo', 'bsl', 'fwe', 'kcc', 'bfo', 'qws', 'nqy', 'pnc', 'kif', 'tkb', 'jnl', 'nni', 'kfg', 'yiu', 'gig', 'loq', 'skq', 'cna', 'jwi', 'nnl', 'soa', 'nqt', 'soo', 'dhw', 'ate', 'mny', 'tng', 'bha', 'pez', 'mfs', 'ymr', 'vsl', 'fse', 'zxx', 'csn', 'prl', 'aed', 'csg']\n"
     ]
    }
   ],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "print(tag_language)\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)')\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model_idx):\n",
    "\n",
    "\tmodel = models_df.loc[model_idx]\n",
    "\tmodel_tags = models_df.loc[model_idx]['tags']\n",
    "\tmodel_card_data = None\n",
    "\ttry:\n",
    "\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\texcept AttributeError:\n",
    "\t\tprint('No card data available for this model')\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model['id'])\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No architecture data available for this model')\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tbase_model = model_card_data['base_model']\n",
    "\t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model['author']]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No emission data available for this model')\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_json(model_idx):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    model_attributes = extract_model_attributes(model_idx)\n",
    "    \n",
    "    with open(os.path.join(result_path, 'test_models.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_attributes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_json(models_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    for model_idx in range(models_df.shape[0]):\n",
    "        output.append(extract_model_attributes(model_idx))\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.LargeLanguageModel.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_json(models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True, cardData=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes_optimized(model):\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tbase_model = model_card_data['base_model']\n",
    "\t\t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t\t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 models processed, 4.8661699295043945 seconds elapsed\n",
      "2000 models processed, 9.638122081756592 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 models processed, 12.947071075439453 seconds elapsed\n",
      "4000 models processed, 17.33626890182495 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 models processed, 59.024887800216675 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 models processed, 74.5662248134613 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 models processed, 86.49017786979675 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 models processed, 496.54301381111145 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 models processed, 502.37821793556213 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 models processed, 512.1899127960205 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000 models processed, 523.7838318347931 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 models processed, 550.5555510520935 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 models processed, 721.4379160404205 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000 models processed, 828.7220559120178 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 models processed, 833.6701850891113 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 models processed, 839.5334300994873 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000 models processed, 844.970294713974 seconds elapsed\n",
      "18000 models processed, 851.498272895813 seconds elapsed\n",
      "19000 models processed, 859.409961938858 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 models processed, 883.879644870758 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 models processed, 955.4416658878326 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000 models processed, 1092.3147630691528 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000 models processed, 1123.9396250247955 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 models processed, 1157.9179730415344 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 models processed, 1170.930862903595 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000 models processed, 1245.1924228668213 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000 models processed, 1248.3609569072723 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 models processed, 1251.249834060669 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000 models processed, 1256.2309539318085 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 models processed, 1352.5865271091461 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31000 models processed, 1362.180922985077 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 models processed, 1465.3529648780823 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000 models processed, 1476.4668498039246 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000 models processed, 1480.210664987564 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 models processed, 1592.9396607875824 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000 models processed, 1597.7212579250336 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000 models processed, 1645.9878618717194 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38000 models processed, 1772.1514129638672 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39000 models processed, 1822.6043510437012 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 models processed, 1830.6929297447205 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41000 models processed, 1884.9968709945679 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 models processed, 1887.1929678916931 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43000 models processed, 1941.1289699077606 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44000 models processed, 1955.4508409500122 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 models processed, 2068.0531017780304 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000 models processed, 2072.6176269054413 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47000 models processed, 2093.848504781723 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 models processed, 2165.3767640590668 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49000 models processed, 2268.5388309955597 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 models processed, 2458.98180103302 seconds elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000 models processed, 2505.727035999298 seconds elapsed\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: ConnectionResetError(54, 'Connection reset by peer')\", ConnectionResetError(54, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:444\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:593\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:461\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e, e)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: (\"Connection broken: ConnectionResetError(54, 'Connection reset by peer')\", ConnectionResetError(54, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 10\u001b[0m     model_attributes \u001b[38;5;241m=\u001b[39m \u001b[43mextract_model_attributes_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     add_to_json_file(model_attributes, file_path)\n\u001b[1;32m     12\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m, in \u001b[0;36mextract_model_attributes_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     33\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_card_data:\n\u001b[1;32m     34\u001b[0m \t\t\tbase_model \u001b[38;5;241m=\u001b[39m model_card_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m \t\t\tbase_model_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \t\t\tmodel_attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelCreator\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m base_model_data\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:710\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    708\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1633\u001b[0m, in \u001b[0;36mHfApi.list_models\u001b[0;34m(self, filter, author, library, language, model_name, task, trained_dataset, tags, search, emissions_thresholds, sort, direction, limit, full, cardData, fetch_config, token, pipeline_tag)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1632\u001b[0m     items \u001b[38;5;241m=\u001b[39m islice(items, limit)  \u001b[38;5;66;03m# Do not iterate over all pages\u001b[39;00m\n\u001b[0;32m-> 1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiblings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m item:\n\u001b[1;32m   1635\u001b[0m         item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiblings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_pagination.py:45\u001b[0m, in \u001b[0;36mpaginate\u001b[0;34m(path, params, headers)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPagination detected. Requesting next page: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_page\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     hf_raise_for_status(r)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: (\"Connection broken: ConnectionResetError(54, 'Connection reset by peer')\", ConnectionResetError(54, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'models_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for model in models:\n",
    "    model_attributes = extract_model_attributes_optimized(model)\n",
    "    add_to_json_file(model_attributes, file_path)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def models_statistics(file_name):\n",
    "\n",
    "\tname_count = 0\n",
    "\tversion_count = 0\n",
    "\tnumber_of_parameters_count = 0\n",
    "\tquantization_count = 0\n",
    "\tarchitecture_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tmodel_creator_count = 0\n",
    "\tlicense_count = 0\n",
    "\tlibrary_count = 0\n",
    "\tcontext_length_count = 0\n",
    "\tdevelopers_count = 0\n",
    "\topen_source_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuned_count = 0\n",
    "\tcarbon_emission_count = 0\n",
    "\ttokenizer_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'HF entries', 'hf_extracted_json')\n",
    "\n",
    "\tmodels_json = open(os.path.join(result_path, file_name))\n",
    "\tmodels_data_json = json.load(models_json)\n",
    "\n",
    "\tmodels_df = pd.DataFrame(models_data_json) \n",
    "\n",
    "\t# TODO: add more attributes (?)\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['version'] is not None:\n",
    "\t\t\tversion_count += 1\n",
    "\t\tif item['numberOfParameters'] is not None:\n",
    "\t\t\tnumber_of_parameters_count += 1\n",
    "\t\tif item['quantization'] is not None:\n",
    "\t\t\tquantization_count += 1\n",
    "\t\tif item['architecture'] is not None:\n",
    "\t\t\tarchitecture_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\n",
    "\t\tif item['modelCreator'] is not None:\n",
    "\t\t\tmodel_creator_count += 1\n",
    "\t\tif item['licenseToUse'] is not None:\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['libraryFramework']) > 0:\n",
    "\t\t\tlibrary_count += 1\n",
    "\t\tif item['contextLength'] is not None:\n",
    "\t\t\tcontext_length_count += 1\n",
    "\t\tif len(item['developers']) > 0:\n",
    "\t\t\tdevelopers_count += 1\n",
    "\t\tif item['openSource'] is not None:\n",
    "\t\t\topen_source_count += 1\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuned'] is not None:\n",
    "\t\t\tfinetuned_count += 1\n",
    "\t\tif item['carbonEmission [CO2eq tons]'] is not None:\n",
    "\t\t\tcarbon_emission_count += 1\n",
    "\t\tif item['tokenizer'] is not None:\n",
    "\t\t\ttokenizer_count += 1\n",
    "\t\n",
    "\ttotal_models = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed models: {total_models}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Version: {version_count} ({(version_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Number of Parameters: {number_of_parameters_count} ({(number_of_parameters_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Quantization: {quantization_count} ({(quantization_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Architecture: {architecture_count} ({(architecture_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Model creator: {model_creator_count} ({(model_creator_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Library: {library_count} ({(library_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Context Length: {context_length_count} ({(context_length_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Developers: {developers_count} ({(developers_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Open Source: {open_source_count} ({(open_source_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuned: {finetuned_count} ({(finetuned_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Carbon emission: {carbon_emission_count} ({(carbon_emission_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Tokenizer: {tokenizer_count} ({(tokenizer_count / total_models) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tllm_attributes = models_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tmodel_name = item['name']\n",
    "\t\tfor attr in llm_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed models: 64377\n",
      "    Name: 64377 (100.00%)\n",
      "    Version: 0 (0.00%)\n",
      "    Number of Parameters: 0 (0.00%)\n",
      "    Quantization: 3 (0.00%)\n",
      "    Architecture: 391 (0.61%)\n",
      "    Languages: 15426 (23.96%)\n",
      "    Model creator: 386 (0.60%)\n",
      "    License to use: 19956 (31.00%)\n",
      "    Library: 46662 (72.48%)\n",
      "    Context Length: 0 (0.00%)\n",
      "    Developers: 64377 (100.00%)\n",
      "    Open Source: 64377 (100.00%)\n",
      "    URI: 5247 (8.15%)\n",
      "    Fine-tuned: 391 (0.61%)\n",
      "    Carbon emission: 489 (0.76%)\n",
      "    Tokenizer: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "models_statistics('ChatIMPACT.LargeLanguageModel.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>version</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>numberOfParameters</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>quantization</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>modelCreator</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>libraryFramework</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>contextLength</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>developers</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>openSource</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>fineTuned</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>carbonEmission [CO2eq tons]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>tokenizer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>version</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>numberOfParameters</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>quantization</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>modelCreator</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>libraryFramework</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>contextLength</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>developers</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>openSource</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>fineTuned</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id entity name               attribute name available API  \\\n",
       "0   albert-base-v1         LLM                         name          True   \n",
       "1   albert-base-v1         LLM                      version         False   \n",
       "2   albert-base-v1         LLM           numberOfParameters         False   \n",
       "3   albert-base-v1         LLM                 quantization         False   \n",
       "4   albert-base-v1         LLM                 architecture         False   \n",
       "5   albert-base-v1         LLM                    languages          True   \n",
       "6   albert-base-v1         LLM                 modelCreator         False   \n",
       "7   albert-base-v1         LLM                 licenseToUse          True   \n",
       "8   albert-base-v1         LLM             libraryFramework          True   \n",
       "9   albert-base-v1         LLM                contextLength         False   \n",
       "10  albert-base-v1         LLM                   developers          True   \n",
       "11  albert-base-v1         LLM                   openSource          True   \n",
       "12  albert-base-v1         LLM                          uri          True   \n",
       "13  albert-base-v1         LLM                    fineTuned         False   \n",
       "14  albert-base-v1         LLM  carbonEmission [CO2eq tons]         False   \n",
       "15  albert-base-v1         LLM                    tokenizer         False   \n",
       "16  albert-base-v2         LLM                         name          True   \n",
       "17  albert-base-v2         LLM                      version         False   \n",
       "18  albert-base-v2         LLM           numberOfParameters         False   \n",
       "19  albert-base-v2         LLM                 quantization         False   \n",
       "20  albert-base-v2         LLM                 architecture         False   \n",
       "21  albert-base-v2         LLM                    languages          True   \n",
       "22  albert-base-v2         LLM                 modelCreator         False   \n",
       "23  albert-base-v2         LLM                 licenseToUse          True   \n",
       "24  albert-base-v2         LLM             libraryFramework          True   \n",
       "25  albert-base-v2         LLM                contextLength         False   \n",
       "26  albert-base-v2         LLM                   developers          True   \n",
       "27  albert-base-v2         LLM                   openSource          True   \n",
       "28  albert-base-v2         LLM                          uri          True   \n",
       "29  albert-base-v2         LLM                    fineTuned         False   \n",
       "\n",
       "   available scraping  \n",
       "0               False  \n",
       "1               False  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  \n",
       "5               False  \n",
       "6               False  \n",
       "7               False  \n",
       "8               False  \n",
       "9               False  \n",
       "10              False  \n",
       "11              False  \n",
       "12              False  \n",
       "13              False  \n",
       "14              False  \n",
       "15              False  \n",
       "16              False  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  \n",
       "20              False  \n",
       "21              False  \n",
       "22              False  \n",
       "23              False  \n",
       "24              False  \n",
       "25              False  \n",
       "26              False  \n",
       "27              False  \n",
       "28              False  \n",
       "29              False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>paperswithcode_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>card_data</th>\n",
       "      <th>siblings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amirveyseh/acronym_identification</td>\n",
       "      <td>amirveyseh</td>\n",
       "      <td>15ef643450d589d5883e289ffadeb03563e80a9e</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:39:57+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>19</td>\n",
       "      <td>acronym-identification</td>\n",
       "      <td>[task_categories:token-classification, annotat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ade-benchmark-corpus/ade_corpus_v2</td>\n",
       "      <td>ade-benchmark-corpus</td>\n",
       "      <td>4ba01c71687dd7c996597042449448ea312126cf</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:42:58+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_cat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLNLP/adversarial_qa</td>\n",
       "      <td>UCLNLP</td>\n",
       "      <td>c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 14:20:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>139</td>\n",
       "      <td>32</td>\n",
       "      <td>adversarialqa</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yale-LILY/aeslc</td>\n",
       "      <td>Yale-LILY</td>\n",
       "      <td>2305f2e63b68056f9b9037a3805c8c196e0d5581</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:49:13+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>124</td>\n",
       "      <td>12</td>\n",
       "      <td>aeslc</td>\n",
       "      <td>[task_categories:summarization, annotations_cr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nwu-ctext/afrikaans_ner_corpus</td>\n",
       "      <td>nwu-ctext</td>\n",
       "      <td>445834a997dce8b40e1d108638064381de80c497</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:51:47+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:token-classification, task_id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fancyzhx/ag_news</td>\n",
       "      <td>fancyzhx</td>\n",
       "      <td>eb185aade064a813bc0b7f42de02595523103ca4</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 12:02:37+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7297</td>\n",
       "      <td>123</td>\n",
       "      <td>ag-news</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>allenai</td>\n",
       "      <td>210d026faf9955653af8916fad021475a3f00453</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 15:09:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>785162</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google/air_dialogue</td>\n",
       "      <td>google</td>\n",
       "      <td>dbdbe7bcef8d344bc3c68a05600f3d95917d6898</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 15:22:15+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-generation, task_categor...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>komari6/ajgt_twitter_ar</td>\n",
       "      <td>komari6</td>\n",
       "      <td>af3f2fa5462ac461b696cb300d66e07ad366057f</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:58:01+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>legacy-datasets/allegro_reviews</td>\n",
       "      <td>legacy-datasets</td>\n",
       "      <td>71593d1379934286885c53d147bc863ffe830745</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:59:39+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>allegro-reviews</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                author  \\\n",
       "0   amirveyseh/acronym_identification            amirveyseh   \n",
       "1  ade-benchmark-corpus/ade_corpus_v2  ade-benchmark-corpus   \n",
       "2               UCLNLP/adversarial_qa                UCLNLP   \n",
       "3                     Yale-LILY/aeslc             Yale-LILY   \n",
       "4      nwu-ctext/afrikaans_ner_corpus             nwu-ctext   \n",
       "5                    fancyzhx/ag_news              fancyzhx   \n",
       "6                     allenai/ai2_arc               allenai   \n",
       "7                 google/air_dialogue                google   \n",
       "8             komari6/ajgt_twitter_ar               komari6   \n",
       "9     legacy-datasets/allegro_reviews       legacy-datasets   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  15ef643450d589d5883e289ffadeb03563e80a9e 2022-03-02 23:29:22+00:00   \n",
       "1  4ba01c71687dd7c996597042449448ea312126cf 2022-03-02 23:29:22+00:00   \n",
       "2  c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9 2022-03-02 23:29:22+00:00   \n",
       "3  2305f2e63b68056f9b9037a3805c8c196e0d5581 2022-03-02 23:29:22+00:00   \n",
       "4  445834a997dce8b40e1d108638064381de80c497 2022-03-02 23:29:22+00:00   \n",
       "5  eb185aade064a813bc0b7f42de02595523103ca4 2022-03-02 23:29:22+00:00   \n",
       "6  210d026faf9955653af8916fad021475a3f00453 2022-03-02 23:29:22+00:00   \n",
       "7  dbdbe7bcef8d344bc3c68a05600f3d95917d6898 2022-03-02 23:29:22+00:00   \n",
       "8  af3f2fa5462ac461b696cb300d66e07ad366057f 2022-03-02 23:29:22+00:00   \n",
       "9  71593d1379934286885c53d147bc863ffe830745 2022-03-02 23:29:22+00:00   \n",
       "\n",
       "              last_modified  private  gated  disabled  downloads  likes  \\\n",
       "0 2024-01-09 11:39:57+00:00    False  False     False        180     19   \n",
       "1 2024-01-09 11:42:58+00:00    False  False     False        241     25   \n",
       "2 2023-12-21 14:20:00+00:00    False  False     False        139     32   \n",
       "3 2024-01-09 11:49:13+00:00    False  False     False        124     12   \n",
       "4 2024-01-09 11:51:47+00:00    False  False     False        112      6   \n",
       "5 2024-03-07 12:02:37+00:00    False  False     False       7297    123   \n",
       "6 2023-12-21 15:09:48+00:00    False  False     False     785162    111   \n",
       "7 2024-03-07 15:22:15+00:00    False  False     False         75     15   \n",
       "8 2024-01-09 11:58:01+00:00    False  False     False        119      4   \n",
       "9 2024-01-09 11:59:39+00:00    False  False     False         84      4   \n",
       "\n",
       "        paperswithcode_id                                               tags  \\\n",
       "0  acronym-identification  [task_categories:token-classification, annotat...   \n",
       "1                    None  [task_categories:text-classification, task_cat...   \n",
       "2           adversarialqa  [task_categories:question-answering, task_ids:...   \n",
       "3                   aeslc  [task_categories:summarization, annotations_cr...   \n",
       "4                    None  [task_categories:token-classification, task_id...   \n",
       "5                 ag-news  [task_categories:text-classification, task_ids...   \n",
       "6                    None  [task_categories:question-answering, task_ids:...   \n",
       "7                    None  [task_categories:text-generation, task_categor...   \n",
       "8                    None  [task_categories:text-classification, task_ids...   \n",
       "9         allegro-reviews  [task_categories:text-classification, task_ids...   \n",
       "\n",
       "  card_data siblings  \n",
       "0        {}     None  \n",
       "1        {}     None  \n",
       "2        {}     None  \n",
       "3        {}     None  \n",
       "4        {}     None  \n",
       "5        {}     None  \n",
       "6        {}     None  \n",
       "7        {}     None  \n",
       "8        {}     None  \n",
       "9        {}     None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO run for all datasets\n",
    "datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "datasets_df = pd.DataFrame(datasets)\n",
    "datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
       "       'gated', 'disabled', 'downloads', 'likes', 'paperswithcode_id', 'tags',\n",
       "       'card_data', 'siblings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                   amirveyseh/acronym_identification\n",
       "author                                                      amirveyseh\n",
       "sha                           15ef643450d589d5883e289ffadeb03563e80a9e\n",
       "created_at                                   2022-03-02 23:29:22+00:00\n",
       "last_modified                                2024-01-09 11:39:57+00:00\n",
       "private                                                          False\n",
       "gated                                                            False\n",
       "disabled                                                         False\n",
       "downloads                                                          180\n",
       "likes                                                               19\n",
       "paperswithcode_id                               acronym-identification\n",
       "tags                 [task_categories:token-classification, annotat...\n",
       "card_data                                                           {}\n",
       "siblings                                                          None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_categories:question-answering',\n",
       " 'task_ids:extractive-qa',\n",
       " 'task_ids:open-domain-qa',\n",
       " 'annotations_creators:crowdsourced',\n",
       " 'language_creators:found',\n",
       " 'multilinguality:monolingual',\n",
       " 'source_datasets:original',\n",
       " 'language:en',\n",
       " 'license:cc-by-sa-4.0',\n",
       " 'size_categories:10K<n<100K',\n",
       " 'format:parquet',\n",
       " 'modality:text',\n",
       " 'library:datasets',\n",
       " 'library:pandas',\n",
       " 'library:mlcroissant',\n",
       " 'library:polars',\n",
       " 'arxiv:2002.00293',\n",
       " 'arxiv:1606.05250',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[2]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset_idx):\n",
    "\n",
    "\tdataset = datasets_df.loc[dataset_idx]\n",
    "\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset['id'])\n",
    "\tdataset_attributes['size [GB]'] = None\n",
    "\n",
    "\turl = \"https://huggingface.co/datasets/\" + dataset['id']\n",
    "\tfile_size_str = extract_file_size(url)\n",
    "\tif file_size_str:\n",
    "\t\tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t\tif file_size_gb:\n",
    "\t\t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_json(datasets_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for dataset_idx in range(datasets_df.shape[0]):\n",
    "        output.append(extract_datasets_attributes(dataset_idx))\n",
    "        if dataset_idx % 10 == 0:\n",
    "            print(f'Processed {dataset_idx} datasets, elapsed time: {time.time() - start_time:.2f} seconds')\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Dataset.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 datasets, elapsed time: 1.00 seconds\n",
      "Processed 10 datasets, elapsed time: 9.65 seconds\n",
      "Processed 20 datasets, elapsed time: 17.75 seconds\n",
      "Processed 30 datasets, elapsed time: 25.48 seconds\n",
      "Processed 40 datasets, elapsed time: 33.46 seconds\n",
      "Processed 50 datasets, elapsed time: 42.07 seconds\n",
      "Processed 60 datasets, elapsed time: 51.79 seconds\n",
      "Processed 70 datasets, elapsed time: 58.55 seconds\n",
      "Processed 80 datasets, elapsed time: 65.34 seconds\n",
      "Processed 90 datasets, elapsed time: 73.20 seconds\n",
      "Processed 100 datasets, elapsed time: 81.37 seconds\n",
      "Processed 110 datasets, elapsed time: 89.41 seconds\n",
      "Processed 120 datasets, elapsed time: 98.28 seconds\n",
      "Processed 130 datasets, elapsed time: 105.30 seconds\n",
      "Processed 140 datasets, elapsed time: 113.13 seconds\n",
      "Processed 150 datasets, elapsed time: 119.39 seconds\n",
      "Processed 160 datasets, elapsed time: 125.61 seconds\n",
      "Processed 170 datasets, elapsed time: 133.69 seconds\n",
      "Processed 180 datasets, elapsed time: 140.15 seconds\n",
      "Processed 190 datasets, elapsed time: 146.29 seconds\n",
      "Processed 200 datasets, elapsed time: 153.17 seconds\n",
      "Processed 210 datasets, elapsed time: 160.06 seconds\n",
      "Processed 220 datasets, elapsed time: 167.29 seconds\n",
      "Processed 230 datasets, elapsed time: 174.49 seconds\n",
      "Processed 240 datasets, elapsed time: 181.96 seconds\n",
      "Processed 250 datasets, elapsed time: 188.86 seconds\n",
      "Processed 260 datasets, elapsed time: 195.17 seconds\n",
      "Processed 270 datasets, elapsed time: 201.14 seconds\n",
      "Processed 280 datasets, elapsed time: 206.82 seconds\n",
      "Processed 290 datasets, elapsed time: 212.84 seconds\n",
      "Processed 300 datasets, elapsed time: 218.66 seconds\n",
      "Processed 310 datasets, elapsed time: 225.04 seconds\n",
      "Processed 320 datasets, elapsed time: 232.35 seconds\n",
      "Processed 330 datasets, elapsed time: 239.15 seconds\n",
      "Processed 340 datasets, elapsed time: 245.25 seconds\n",
      "Processed 350 datasets, elapsed time: 250.93 seconds\n",
      "Processed 360 datasets, elapsed time: 257.13 seconds\n",
      "Processed 370 datasets, elapsed time: 264.18 seconds\n",
      "Processed 380 datasets, elapsed time: 269.91 seconds\n",
      "Processed 390 datasets, elapsed time: 275.97 seconds\n",
      "Processed 400 datasets, elapsed time: 281.88 seconds\n",
      "Processed 410 datasets, elapsed time: 287.41 seconds\n",
      "Processed 420 datasets, elapsed time: 294.33 seconds\n",
      "Processed 430 datasets, elapsed time: 301.62 seconds\n",
      "Processed 440 datasets, elapsed time: 310.20 seconds\n",
      "Processed 450 datasets, elapsed time: 317.36 seconds\n",
      "Processed 460 datasets, elapsed time: 325.05 seconds\n",
      "Processed 470 datasets, elapsed time: 332.20 seconds\n",
      "Processed 480 datasets, elapsed time: 338.33 seconds\n",
      "Processed 490 datasets, elapsed time: 346.54 seconds\n",
      "Processed 500 datasets, elapsed time: 356.02 seconds\n",
      "Processed 510 datasets, elapsed time: 363.41 seconds\n",
      "Processed 520 datasets, elapsed time: 371.07 seconds\n",
      "Processed 530 datasets, elapsed time: 376.83 seconds\n",
      "Processed 540 datasets, elapsed time: 384.30 seconds\n",
      "Processed 550 datasets, elapsed time: 390.66 seconds\n",
      "Processed 560 datasets, elapsed time: 397.23 seconds\n",
      "Processed 570 datasets, elapsed time: 403.73 seconds\n",
      "Processed 580 datasets, elapsed time: 411.70 seconds\n",
      "Processed 590 datasets, elapsed time: 419.07 seconds\n",
      "Processed 600 datasets, elapsed time: 426.37 seconds\n",
      "Processed 610 datasets, elapsed time: 433.19 seconds\n",
      "Processed 620 datasets, elapsed time: 438.24 seconds\n",
      "Processed 630 datasets, elapsed time: 444.84 seconds\n",
      "Processed 640 datasets, elapsed time: 451.92 seconds\n",
      "Processed 650 datasets, elapsed time: 458.82 seconds\n",
      "Processed 660 datasets, elapsed time: 466.19 seconds\n",
      "Processed 670 datasets, elapsed time: 473.30 seconds\n",
      "Processed 680 datasets, elapsed time: 480.77 seconds\n",
      "Processed 690 datasets, elapsed time: 488.38 seconds\n",
      "Processed 700 datasets, elapsed time: 495.65 seconds\n",
      "Processed 710 datasets, elapsed time: 504.77 seconds\n",
      "Processed 720 datasets, elapsed time: 512.01 seconds\n",
      "Processed 730 datasets, elapsed time: 521.36 seconds\n",
      "Processed 740 datasets, elapsed time: 527.86 seconds\n",
      "Processed 750 datasets, elapsed time: 536.91 seconds\n",
      "Processed 760 datasets, elapsed time: 544.58 seconds\n",
      "Processed 770 datasets, elapsed time: 551.46 seconds\n",
      "Processed 780 datasets, elapsed time: 557.88 seconds\n",
      "Processed 790 datasets, elapsed time: 565.73 seconds\n",
      "Processed 800 datasets, elapsed time: 571.26 seconds\n",
      "Processed 810 datasets, elapsed time: 576.84 seconds\n",
      "Processed 820 datasets, elapsed time: 582.55 seconds\n",
      "Processed 830 datasets, elapsed time: 588.46 seconds\n",
      "Processed 840 datasets, elapsed time: 596.36 seconds\n",
      "Processed 850 datasets, elapsed time: 602.47 seconds\n",
      "Processed 860 datasets, elapsed time: 608.91 seconds\n",
      "Processed 870 datasets, elapsed time: 614.91 seconds\n",
      "Processed 880 datasets, elapsed time: 621.03 seconds\n",
      "Processed 890 datasets, elapsed time: 627.99 seconds\n",
      "Processed 900 datasets, elapsed time: 635.31 seconds\n",
      "Processed 910 datasets, elapsed time: 644.85 seconds\n",
      "Processed 920 datasets, elapsed time: 654.47 seconds\n",
      "Processed 930 datasets, elapsed time: 664.13 seconds\n",
      "Processed 940 datasets, elapsed time: 670.25 seconds\n",
      "Processed 950 datasets, elapsed time: 675.47 seconds\n",
      "Processed 960 datasets, elapsed time: 682.08 seconds\n",
      "Processed 970 datasets, elapsed time: 689.75 seconds\n",
      "Processed 980 datasets, elapsed time: 696.21 seconds\n",
      "Processed 990 datasets, elapsed time: 703.07 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_to_json(datasets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes_optimized(dataset):\n",
    "\n",
    "\tdataset_tags = dataset.tags\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset.id)\n",
    "\tdataset_attributes['size [GB]'] = match_size(dataset_tags)\n",
    "\n",
    "\t# url = \"https://huggingface.co/datasets/\" + dataset.id\n",
    "\t# file_size_str = extract_file_size(url)\n",
    "\t# if file_size_str:\n",
    "\t# \tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t# \tif file_size_gb:\n",
    "\t# \t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 datasets processed, 2.122116804122925 seconds elapsed, estimated time remaining: 421.54 seconds\n",
      "2000 datasets processed, 3.3351528644561768 seconds elapsed, estimated time remaining: 329.58 seconds\n",
      "3000 datasets processed, 4.077770948410034 seconds elapsed, estimated time remaining: 267.29 seconds\n",
      "4000 datasets processed, 4.908613920211792 seconds elapsed, estimated time remaining: 240.08 seconds\n",
      "5000 datasets processed, 5.631502866744995 seconds elapsed, estimated time remaining: 219.23 seconds\n",
      "6000 datasets processed, 6.8947179317474365 seconds elapsed, estimated time remaining: 222.52 seconds\n",
      "7000 datasets processed, 7.563861846923828 seconds elapsed, estimated time remaining: 208.16 seconds\n",
      "8000 datasets processed, 8.125163793563843 seconds elapsed, estimated time remaining: 194.64 seconds\n",
      "9000 datasets processed, 8.944549798965454 seconds elapsed, estimated time remaining: 189.47 seconds\n",
      "10000 datasets processed, 9.60464882850647 seconds elapsed, estimated time remaining: 182.14 seconds\n",
      "11000 datasets processed, 10.610082626342773 seconds elapsed, estimated time remaining: 181.96 seconds\n",
      "12000 datasets processed, 11.249409675598145 seconds elapsed, estimated time remaining: 175.91 seconds\n",
      "13000 datasets processed, 11.898953914642334 seconds elapsed, estimated time remaining: 170.83 seconds\n",
      "14000 datasets processed, 12.693379878997803 seconds elapsed, estimated time remaining: 168.32 seconds\n",
      "15000 datasets processed, 13.439291715621948 seconds elapsed, estimated time remaining: 165.43 seconds\n",
      "16000 datasets processed, 14.698966026306152 seconds elapsed, estimated time remaining: 168.71 seconds\n",
      "17000 datasets processed, 15.484087705612183 seconds elapsed, estimated time remaining: 166.36 seconds\n",
      "18000 datasets processed, 16.036244869232178 seconds elapsed, estimated time remaining: 161.83 seconds\n",
      "19000 datasets processed, 16.688336849212646 seconds elapsed, estimated time remaining: 158.66 seconds\n",
      "20000 datasets processed, 17.31124472618103 seconds elapsed, estimated time remaining: 155.49 seconds\n",
      "21000 datasets processed, 17.968323707580566 seconds elapsed, estimated time remaining: 152.85 seconds\n",
      "22000 datasets processed, 18.81830382347107 seconds elapsed, estimated time remaining: 151.95 seconds\n",
      "23000 datasets processed, 19.56389570236206 seconds elapsed, estimated time remaining: 150.25 seconds\n",
      "24000 datasets processed, 20.313493013381958 seconds elapsed, estimated time remaining: 148.66 seconds\n",
      "25000 datasets processed, 21.026275873184204 seconds elapsed, estimated time remaining: 146.88 seconds\n",
      "26000 datasets processed, 21.66336989402771 seconds elapsed, estimated time remaining: 144.68 seconds\n",
      "27000 datasets processed, 22.24871277809143 seconds elapsed, estimated time remaining: 142.26 seconds\n",
      "28000 datasets processed, 22.82092785835266 seconds elapsed, estimated time remaining: 139.89 seconds\n",
      "29000 datasets processed, 23.64312767982483 seconds elapsed, estimated time remaining: 139.12 seconds\n",
      "30000 datasets processed, 24.22741198539734 seconds elapsed, estimated time remaining: 137.00 seconds\n",
      "31000 datasets processed, 25.019900798797607 seconds elapsed, estimated time remaining: 136.11 seconds\n",
      "32000 datasets processed, 25.557586908340454 seconds elapsed, estimated time remaining: 133.89 seconds\n",
      "33000 datasets processed, 26.105581760406494 seconds elapsed, estimated time remaining: 131.83 seconds\n",
      "34000 datasets processed, 26.997462034225464 seconds elapsed, estimated time remaining: 131.53 seconds\n",
      "35000 datasets processed, 27.542242765426636 seconds elapsed, estimated time remaining: 129.56 seconds\n",
      "36000 datasets processed, 28.10082197189331 seconds elapsed, estimated time remaining: 127.74 seconds\n",
      "37000 datasets processed, 28.615059852600098 seconds elapsed, estimated time remaining: 125.78 seconds\n",
      "38000 datasets processed, 29.448315858840942 seconds elapsed, estimated time remaining: 125.27 seconds\n",
      "39000 datasets processed, 30.080533981323242 seconds elapsed, estimated time remaining: 123.90 seconds\n",
      "40000 datasets processed, 30.73962378501892 seconds elapsed, estimated time remaining: 122.68 seconds\n",
      "41000 datasets processed, 31.449213981628418 seconds elapsed, estimated time remaining: 121.69 seconds\n",
      "42000 datasets processed, 31.986824989318848 seconds elapsed, estimated time remaining: 120.06 seconds\n",
      "43000 datasets processed, 32.49457883834839 seconds elapsed, estimated time remaining: 118.37 seconds\n",
      "44000 datasets processed, 33.06981301307678 seconds elapsed, estimated time remaining: 116.98 seconds\n",
      "45000 datasets processed, 33.6497757434845 seconds elapsed, estimated time remaining: 115.64 seconds\n",
      "46000 datasets processed, 34.20798587799072 seconds elapsed, estimated time remaining: 114.26 seconds\n",
      "47000 datasets processed, 34.76920175552368 seconds elapsed, estimated time remaining: 112.92 seconds\n",
      "48000 datasets processed, 35.60355091094971 seconds elapsed, estimated time remaining: 112.48 seconds\n",
      "49000 datasets processed, 36.17472171783447 seconds elapsed, estimated time remaining: 111.21 seconds\n",
      "50000 datasets processed, 36.69904088973999 seconds elapsed, estimated time remaining: 109.83 seconds\n",
      "51000 datasets processed, 37.942800760269165 seconds elapsed, estimated time remaining: 110.59 seconds\n",
      "52000 datasets processed, 47.4497447013855 seconds elapsed, estimated time remaining: 134.72 seconds\n",
      "53000 datasets processed, 48.65088677406311 seconds elapsed, estimated time remaining: 134.61 seconds\n",
      "54000 datasets processed, 51.521758794784546 seconds elapsed, estimated time remaining: 138.96 seconds\n",
      "55000 datasets processed, 54.53571367263794 seconds elapsed, estimated time remaining: 143.42 seconds\n",
      "56000 datasets processed, 56.3685827255249 seconds elapsed, estimated time remaining: 144.59 seconds\n",
      "57000 datasets processed, 57.212470054626465 seconds elapsed, estimated time remaining: 143.17 seconds\n",
      "58000 datasets processed, 58.80527591705322 seconds elapsed, estimated time remaining: 143.61 seconds\n",
      "59000 datasets processed, 60.45535683631897 seconds elapsed, estimated time remaining: 144.11 seconds\n",
      "60000 datasets processed, 61.565786838531494 seconds elapsed, estimated time remaining: 143.29 seconds\n",
      "61000 datasets processed, 62.435425758361816 seconds elapsed, estimated time remaining: 141.90 seconds\n",
      "62000 datasets processed, 63.04928779602051 seconds elapsed, estimated time remaining: 139.97 seconds\n",
      "63000 datasets processed, 64.31148171424866 seconds elapsed, estimated time remaining: 139.49 seconds\n",
      "64000 datasets processed, 66.64395380020142 seconds elapsed, estimated time remaining: 141.25 seconds\n",
      "65000 datasets processed, 67.56693887710571 seconds elapsed, estimated time remaining: 139.96 seconds\n",
      "66000 datasets processed, 68.91160082817078 seconds elapsed, estimated time remaining: 139.54 seconds\n",
      "67000 datasets processed, 70.51137089729309 seconds elapsed, estimated time remaining: 139.59 seconds\n",
      "68000 datasets processed, 71.08591175079346 seconds elapsed, estimated time remaining: 137.62 seconds\n",
      "69000 datasets processed, 71.83659076690674 seconds elapsed, estimated time remaining: 136.01 seconds\n",
      "70000 datasets processed, 72.6812059879303 seconds elapsed, estimated time remaining: 134.61 seconds\n",
      "71000 datasets processed, 73.76525068283081 seconds elapsed, estimated time remaining: 133.65 seconds\n",
      "72000 datasets processed, 74.39313888549805 seconds elapsed, estimated time remaining: 131.88 seconds\n",
      "73000 datasets processed, 74.97298097610474 seconds elapsed, estimated time remaining: 130.06 seconds\n",
      "74000 datasets processed, 75.81936001777649 seconds elapsed, estimated time remaining: 128.73 seconds\n",
      "75000 datasets processed, 76.95166778564453 seconds elapsed, estimated time remaining: 127.89 seconds\n",
      "76000 datasets processed, 77.69212889671326 seconds elapsed, estimated time remaining: 126.39 seconds\n",
      "77000 datasets processed, 79.06292462348938 seconds elapsed, estimated time remaining: 125.93 seconds\n",
      "78000 datasets processed, 80.13140678405762 seconds elapsed, estimated time remaining: 124.97 seconds\n",
      "79000 datasets processed, 81.36207365989685 seconds elapsed, estimated time remaining: 124.25 seconds\n",
      "80000 datasets processed, 81.91061186790466 seconds elapsed, estimated time remaining: 122.50 seconds\n",
      "81000 datasets processed, 83.81559681892395 seconds elapsed, estimated time remaining: 122.77 seconds\n",
      "82000 datasets processed, 85.03667974472046 seconds elapsed, estimated time remaining: 122.00 seconds\n",
      "83000 datasets processed, 87.30819892883301 seconds elapsed, estimated time remaining: 122.70 seconds\n",
      "84000 datasets processed, 88.57164978981018 seconds elapsed, estimated time remaining: 121.94 seconds\n",
      "85000 datasets processed, 90.47069787979126 seconds elapsed, estimated time remaining: 122.02 seconds\n",
      "86000 datasets processed, 91.34876894950867 seconds elapsed, estimated time remaining: 120.71 seconds\n",
      "87000 datasets processed, 93.9034857749939 seconds elapsed, estimated time remaining: 121.58 seconds\n",
      "88000 datasets processed, 97.04180574417114 seconds elapsed, estimated time remaining: 123.11 seconds\n",
      "89000 datasets processed, 99.23932480812073 seconds elapsed, estimated time remaining: 123.37 seconds\n",
      "90000 datasets processed, 104.81996870040894 seconds elapsed, estimated time remaining: 127.70 seconds\n",
      "91000 datasets processed, 106.37638187408447 seconds elapsed, estimated time remaining: 127.00 seconds\n",
      "92000 datasets processed, 109.31831765174866 seconds elapsed, estimated time remaining: 127.90 seconds\n",
      "93000 datasets processed, 112.42541170120239 seconds elapsed, estimated time remaining: 128.92 seconds\n",
      "94000 datasets processed, 114.63503575325012 seconds elapsed, estimated time remaining: 128.83 seconds\n",
      "95000 datasets processed, 116.06817889213562 seconds elapsed, estimated time remaining: 127.85 seconds\n",
      "96000 datasets processed, 118.65729784965515 seconds elapsed, estimated time remaining: 128.10 seconds\n",
      "97000 datasets processed, 120.61657667160034 seconds elapsed, estimated time remaining: 127.63 seconds\n",
      "98000 datasets processed, 123.10335278511047 seconds elapsed, estimated time remaining: 127.68 seconds\n",
      "99000 datasets processed, 124.98136186599731 seconds elapsed, estimated time remaining: 127.05 seconds\n",
      "100000 datasets processed, 125.93803596496582 seconds elapsed, estimated time remaining: 125.49 seconds\n",
      "101000 datasets processed, 129.22076892852783 seconds elapsed, estimated time remaining: 126.20 seconds\n",
      "102000 datasets processed, 130.6019377708435 seconds elapsed, estimated time remaining: 125.02 seconds\n",
      "103000 datasets processed, 131.4106786251068 seconds elapsed, estimated time remaining: 123.30 seconds\n",
      "104000 datasets processed, 134.5907588005066 seconds elapsed, estimated time remaining: 123.77 seconds\n",
      "105000 datasets processed, 136.62128686904907 seconds elapsed, estimated time remaining: 123.14 seconds\n",
      "106000 datasets processed, 137.97003388404846 seconds elapsed, estimated time remaining: 121.88 seconds\n",
      "107000 datasets processed, 140.18420481681824 seconds elapsed, estimated time remaining: 121.37 seconds\n",
      "108000 datasets processed, 141.9468936920166 seconds elapsed, estimated time remaining: 120.45 seconds\n",
      "109000 datasets processed, 143.2104127407074 seconds elapsed, estimated time remaining: 119.09 seconds\n",
      "110000 datasets processed, 144.0999937057495 seconds elapsed, estimated time remaining: 117.43 seconds\n",
      "111000 datasets processed, 145.85968589782715 seconds elapsed, estimated time remaining: 116.48 seconds\n",
      "112000 datasets processed, 148.926109790802 seconds elapsed, estimated time remaining: 116.54 seconds\n",
      "113000 datasets processed, 150.71394896507263 seconds elapsed, estimated time remaining: 115.56 seconds\n",
      "114000 datasets processed, 151.7510735988617 seconds elapsed, estimated time remaining: 114.00 seconds\n",
      "115000 datasets processed, 153.88023281097412 seconds elapsed, estimated time remaining: 113.26 seconds\n",
      "116000 datasets processed, 155.86638164520264 seconds elapsed, estimated time remaining: 112.39 seconds\n",
      "117000 datasets processed, 157.36700177192688 seconds elapsed, estimated time remaining: 111.15 seconds\n",
      "118000 datasets processed, 158.66950297355652 seconds elapsed, estimated time remaining: 109.78 seconds\n",
      "119000 datasets processed, 159.2974967956543 seconds elapsed, estimated time remaining: 107.95 seconds\n",
      "120000 datasets processed, 162.974871635437 seconds elapsed, estimated time remaining: 108.16 seconds\n",
      "121000 datasets processed, 165.1095826625824 seconds elapsed, estimated time remaining: 107.31 seconds\n",
      "122000 datasets processed, 166.6437587738037 seconds elapsed, estimated time remaining: 106.05 seconds\n",
      "123000 datasets processed, 168.48945379257202 seconds elapsed, estimated time remaining: 104.99 seconds\n",
      "124000 datasets processed, 169.70462274551392 seconds elapsed, estimated time remaining: 103.52 seconds\n",
      "125000 datasets processed, 171.67992091178894 seconds elapsed, estimated time remaining: 102.52 seconds\n",
      "126000 datasets processed, 173.7884407043457 seconds elapsed, estimated time remaining: 101.57 seconds\n",
      "127000 datasets processed, 176.2727916240692 seconds elapsed, estimated time remaining: 100.83 seconds\n",
      "128000 datasets processed, 178.2054579257965 seconds elapsed, estimated time remaining: 99.74 seconds\n",
      "129000 datasets processed, 179.24225997924805 seconds elapsed, estimated time remaining: 98.16 seconds\n",
      "130000 datasets processed, 182.15525579452515 seconds elapsed, estimated time remaining: 97.58 seconds\n",
      "131000 datasets processed, 183.7547287940979 seconds elapsed, estimated time remaining: 96.28 seconds\n",
      "132000 datasets processed, 185.68241691589355 seconds elapsed, estimated time remaining: 95.15 seconds\n",
      "133000 datasets processed, 187.49032187461853 seconds elapsed, estimated time remaining: 93.95 seconds\n",
      "134000 datasets processed, 189.20620965957642 seconds elapsed, estimated time remaining: 92.69 seconds\n",
      "135000 datasets processed, 189.9552628993988 seconds elapsed, estimated time remaining: 90.96 seconds\n",
      "136000 datasets processed, 190.73320388793945 seconds elapsed, estimated time remaining: 89.25 seconds\n",
      "137000 datasets processed, 191.63635683059692 seconds elapsed, estimated time remaining: 87.62 seconds\n",
      "138000 datasets processed, 192.5123279094696 seconds elapsed, estimated time remaining: 85.99 seconds\n",
      "139000 datasets processed, 193.4695019721985 seconds elapsed, estimated time remaining: 84.41 seconds\n",
      "140000 datasets processed, 194.50009179115295 seconds elapsed, estimated time remaining: 82.86 seconds\n",
      "141000 datasets processed, 195.61935472488403 seconds elapsed, estimated time remaining: 81.36 seconds\n",
      "142000 datasets processed, 196.53833770751953 seconds elapsed, estimated time remaining: 79.78 seconds\n",
      "143000 datasets processed, 197.85641074180603 seconds elapsed, estimated time remaining: 78.37 seconds\n",
      "144000 datasets processed, 198.83163595199585 seconds elapsed, estimated time remaining: 76.83 seconds\n",
      "145000 datasets processed, 199.91191291809082 seconds elapsed, estimated time remaining: 75.34 seconds\n",
      "146000 datasets processed, 201.01634097099304 seconds elapsed, estimated time remaining: 73.86 seconds\n",
      "147000 datasets processed, 202.15424990653992 seconds elapsed, estimated time remaining: 72.39 seconds\n",
      "148000 datasets processed, 203.45923686027527 seconds elapsed, estimated time remaining: 70.99 seconds\n",
      "149000 datasets processed, 204.9504907131195 seconds elapsed, estimated time remaining: 69.66 seconds\n",
      "150000 datasets processed, 205.95238399505615 seconds elapsed, estimated time remaining: 68.16 seconds\n",
      "151000 datasets processed, 207.25736093521118 seconds elapsed, estimated time remaining: 66.76 seconds\n",
      "152000 datasets processed, 208.26903986930847 seconds elapsed, estimated time remaining: 65.28 seconds\n",
      "153000 datasets processed, 209.5877068042755 seconds elapsed, estimated time remaining: 63.89 seconds\n",
      "154000 datasets processed, 210.51502680778503 seconds elapsed, estimated time remaining: 62.39 seconds\n",
      "155000 datasets processed, 212.17121267318726 seconds elapsed, estimated time remaining: 61.11 seconds\n",
      "156000 datasets processed, 213.0911808013916 seconds elapsed, estimated time remaining: 59.61 seconds\n",
      "157000 datasets processed, 214.40409088134766 seconds elapsed, estimated time remaining: 58.23 seconds\n",
      "158000 datasets processed, 215.9018349647522 seconds elapsed, estimated time remaining: 56.90 seconds\n",
      "159000 datasets processed, 217.3663468360901 seconds elapsed, estimated time remaining: 55.56 seconds\n",
      "160000 datasets processed, 218.31348586082458 seconds elapsed, estimated time remaining: 54.09 seconds\n",
      "161000 datasets processed, 221.26681661605835 seconds elapsed, estimated time remaining: 53.11 seconds\n",
      "162000 datasets processed, 224.44454979896545 seconds elapsed, estimated time remaining: 52.15 seconds\n",
      "163000 datasets processed, 226.4940948486328 seconds elapsed, estimated time remaining: 50.92 seconds\n",
      "164000 datasets processed, 228.03164291381836 seconds elapsed, estimated time remaining: 49.56 seconds\n",
      "165000 datasets processed, 229.06607484817505 seconds elapsed, estimated time remaining: 48.09 seconds\n",
      "166000 datasets processed, 230.48825073242188 seconds elapsed, estimated time remaining: 46.71 seconds\n",
      "167000 datasets processed, 231.60846996307373 seconds elapsed, estimated time remaining: 45.27 seconds\n",
      "168000 datasets processed, 232.8323938846588 seconds elapsed, estimated time remaining: 43.85 seconds\n",
      "169000 datasets processed, 233.67502164840698 seconds elapsed, estimated time remaining: 42.37 seconds\n",
      "170000 datasets processed, 234.7619707584381 seconds elapsed, estimated time remaining: 40.93 seconds\n",
      "171000 datasets processed, 235.91339492797852 seconds elapsed, estimated time remaining: 39.51 seconds\n",
      "172000 datasets processed, 237.3015058040619 seconds elapsed, estimated time remaining: 38.14 seconds\n",
      "173000 datasets processed, 238.07863092422485 seconds elapsed, estimated time remaining: 36.66 seconds\n",
      "174000 datasets processed, 239.1590118408203 seconds elapsed, estimated time remaining: 35.24 seconds\n",
      "175000 datasets processed, 239.8514859676361 seconds elapsed, estimated time remaining: 33.77 seconds\n",
      "176000 datasets processed, 240.91669392585754 seconds elapsed, estimated time remaining: 32.36 seconds\n",
      "177000 datasets processed, 241.9160897731781 seconds elapsed, estimated time remaining: 30.95 seconds\n",
      "178000 datasets processed, 243.4183328151703 seconds elapsed, estimated time remaining: 29.60 seconds\n",
      "179000 datasets processed, 244.7566478252411 seconds elapsed, estimated time remaining: 28.22 seconds\n",
      "180000 datasets processed, 246.29094076156616 seconds elapsed, estimated time remaining: 26.88 seconds\n",
      "181000 datasets processed, 247.21201276779175 seconds elapsed, estimated time remaining: 25.46 seconds\n",
      "182000 datasets processed, 248.61947965621948 seconds elapsed, estimated time remaining: 24.10 seconds\n",
      "183000 datasets processed, 250.49937677383423 seconds elapsed, estimated time remaining: 22.78 seconds\n",
      "184000 datasets processed, 251.873694896698 seconds elapsed, estimated time remaining: 21.41 seconds\n",
      "185000 datasets processed, 253.12114787101746 seconds elapsed, estimated time remaining: 20.03 seconds\n",
      "186000 datasets processed, 254.3921709060669 seconds elapsed, estimated time remaining: 18.66 seconds\n",
      "187000 datasets processed, 255.4076578617096 seconds elapsed, estimated time remaining: 17.27 seconds\n",
      "188000 datasets processed, 256.3338990211487 seconds elapsed, estimated time remaining: 15.87 seconds\n",
      "189000 datasets processed, 258.03834867477417 seconds elapsed, estimated time remaining: 14.53 seconds\n",
      "190000 datasets processed, 259.8644380569458 seconds elapsed, estimated time remaining: 13.19 seconds\n",
      "191000 datasets processed, 261.2272729873657 seconds elapsed, estimated time remaining: 11.82 seconds\n",
      "192000 datasets processed, 262.65996074676514 seconds elapsed, estimated time remaining: 10.45 seconds\n",
      "193000 datasets processed, 265.84408593177795 seconds elapsed, estimated time remaining: 9.15 seconds\n",
      "194000 datasets processed, 266.7928566932678 seconds elapsed, estimated time remaining: 7.76 seconds\n",
      "195000 datasets processed, 267.9725868701935 seconds elapsed, estimated time remaining: 6.38 seconds\n",
      "196000 datasets processed, 269.1977288722992 seconds elapsed, estimated time remaining: 5.00 seconds\n",
      "197000 datasets processed, 270.56678581237793 seconds elapsed, estimated time remaining: 3.63 seconds\n",
      "198000 datasets processed, 271.5700206756592 seconds elapsed, estimated time remaining: 2.25 seconds\n",
      "199000 datasets processed, 272.7336976528168 seconds elapsed, estimated time remaining: 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'datasets_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for dataset in datasets:\n",
    "    dataset_attributes = extract_datasets_attributes_optimized(dataset)\n",
    "    add_to_json_file(dataset_attributes, file_path)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} datasets processed, {time.time() - start_time} seconds elapsed, estimated time remaining: {(time.time() - start_time) / count * (199642 - count):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def datasets_statistics(file_name):\n",
    "\n",
    "\tname_count = 0\n",
    "\tsize_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tlicense_count = 0\n",
    "\tdomain_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuning_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "\tprint()\n",
    "\tdatasets_json = open(os.path.join(result_path, file_name))\n",
    "\tdatasets_data_json = json.load(datasets_json)\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['size [rows]'] is not None: # can be size [GB] or size [rows]\n",
    "\t\t\tsize_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\t\n",
    "\t\tif item['licenseToUse'] is not None:\t\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['domain']) > 0:\n",
    "\t\t\tdomain_count += 1\t\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuning'] is not None:\n",
    "\t\t\tfinetuning_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\t\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Size: {size_count} ({(size_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Domain: {domain_count} ({(domain_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuning: {finetuning_count} ({(finetuning_count / total_datasets) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tdatasets_attributes = datasets_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tdataset_name = item['name']\n",
    "\t\tfor attr in datasets_attributes:\n",
    "\t\t\tif item[attr] is not None and attr == 'size [GB]':\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of processed datasets: 199654\n",
      "    Name: 199654 (100.00%)\n",
      "    Size: 150976 (75.62%)\n",
      "    Languages: 22559 (11.30%)\n",
      "    License to use: 54002 (27.05%)\n",
      "    Domain: 7319 (3.67%)\n",
      "    URI: 7032 (3.52%)\n",
      "    Fine-tuning: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = datasets_statistics('datasets_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>fineTuning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>fineTuning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id entity name attribute name available API  \\\n",
       "0   acronym_identification     Dataset           name          True   \n",
       "1   acronym_identification     Dataset      size [GB]         False   \n",
       "2   acronym_identification     Dataset      languages          True   \n",
       "3   acronym_identification     Dataset   licenseToUse          True   \n",
       "4   acronym_identification     Dataset         domain         False   \n",
       "5   acronym_identification     Dataset            uri          True   \n",
       "6   acronym_identification     Dataset     fineTuning         False   \n",
       "7            ade_corpus_v2     Dataset           name          True   \n",
       "8            ade_corpus_v2     Dataset      size [GB]         False   \n",
       "9            ade_corpus_v2     Dataset      languages          True   \n",
       "10           ade_corpus_v2     Dataset   licenseToUse          True   \n",
       "11           ade_corpus_v2     Dataset         domain         False   \n",
       "12           ade_corpus_v2     Dataset            uri         False   \n",
       "13           ade_corpus_v2     Dataset     fineTuning         False   \n",
       "14          adversarial_qa     Dataset           name          True   \n",
       "15          adversarial_qa     Dataset      size [GB]         False   \n",
       "16          adversarial_qa     Dataset      languages          True   \n",
       "17          adversarial_qa     Dataset   licenseToUse          True   \n",
       "18          adversarial_qa     Dataset         domain         False   \n",
       "19          adversarial_qa     Dataset            uri          True   \n",
       "\n",
       "   available scraping  \n",
       "0               False  \n",
       "1                True  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  \n",
       "5               False  \n",
       "6               False  \n",
       "7               False  \n",
       "8                True  \n",
       "9               False  \n",
       "10              False  \n",
       "11              False  \n",
       "12              False  \n",
       "13              False  \n",
       "14              False  \n",
       "15               True  \n",
       "16              False  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description, # TODO: text2text generation has no description\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "\n",
    "    with open(result_path + '/ChatIMPACT.DownstreamTask.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tasks_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def tasks_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tsub_task_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\ttask_json = open(os.path.join(result_path, 'ChatIMPACT.DownstreamTask.json'))\n",
    "\ttask_data_json = json.load(task_json)\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\t\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif len(item['sub-task']) > 0:\n",
    "\t\t\tsub_task_count += 1\n",
    "\t\n",
    "\ttask_count = idx + 1\n",
    "\tprint(f'Number of processed task: {idx + 1}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Sub-task: {sub_task_count} ({(sub_task_count / task_count) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\ttask_attributes = task_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in task_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed task: 11\n",
      "    Name: 11 (100.00%)\n",
      "    Description: 11 (100.00%)\n",
      "    Sub-task: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = tasks_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id     entity name attribute name available API  \\\n",
       "0        text-classification  DownstreamTask           name         False   \n",
       "1        text-classification  DownstreamTask    description         False   \n",
       "2        text-classification  DownstreamTask       sub-task         False   \n",
       "3       token-classification  DownstreamTask           name         False   \n",
       "4       token-classification  DownstreamTask    description         False   \n",
       "5       token-classification  DownstreamTask       sub-task         False   \n",
       "6   table-question-answering  DownstreamTask           name         False   \n",
       "7   table-question-answering  DownstreamTask    description         False   \n",
       "8   table-question-answering  DownstreamTask       sub-task         False   \n",
       "9         question-answering  DownstreamTask           name         False   \n",
       "10        question-answering  DownstreamTask    description         False   \n",
       "11        question-answering  DownstreamTask       sub-task         False   \n",
       "12  zero-shot-classification  DownstreamTask           name         False   \n",
       "13  zero-shot-classification  DownstreamTask    description         False   \n",
       "14  zero-shot-classification  DownstreamTask       sub-task         False   \n",
       "15               translation  DownstreamTask           name         False   \n",
       "16               translation  DownstreamTask    description         False   \n",
       "17               translation  DownstreamTask       sub-task         False   \n",
       "18             summarization  DownstreamTask           name         False   \n",
       "19             summarization  DownstreamTask    description         False   \n",
       "20             summarization  DownstreamTask       sub-task         False   \n",
       "21        feature-extraction  DownstreamTask           name         False   \n",
       "22        feature-extraction  DownstreamTask    description         False   \n",
       "23        feature-extraction  DownstreamTask       sub-task         False   \n",
       "24           text-generation  DownstreamTask           name         False   \n",
       "25           text-generation  DownstreamTask    description         False   \n",
       "26           text-generation  DownstreamTask       sub-task         False   \n",
       "27                 fill-mask  DownstreamTask           name         False   \n",
       "28                 fill-mask  DownstreamTask    description         False   \n",
       "29                 fill-mask  DownstreamTask       sub-task         False   \n",
       "30       sentence-similarity  DownstreamTask           name         False   \n",
       "31       sentence-similarity  DownstreamTask    description         False   \n",
       "32       sentence-similarity  DownstreamTask       sub-task         False   \n",
       "\n",
       "   available scraping  \n",
       "0                True  \n",
       "1                True  \n",
       "2               False  \n",
       "3                True  \n",
       "4                True  \n",
       "5               False  \n",
       "6                True  \n",
       "7                True  \n",
       "8               False  \n",
       "9                True  \n",
       "10               True  \n",
       "11              False  \n",
       "12               True  \n",
       "13               True  \n",
       "14              False  \n",
       "15               True  \n",
       "16               True  \n",
       "17              False  \n",
       "18               True  \n",
       "19               True  \n",
       "20              False  \n",
       "21               True  \n",
       "22               True  \n",
       "23              False  \n",
       "24               True  \n",
       "25               True  \n",
       "26              False  \n",
       "27               True  \n",
       "28               True  \n",
       "29              False  \n",
       "30               True  \n",
       "31               True  \n",
       "32              False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "url_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "response = requests.get(url_metrics)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "h4_tags = soup.find_all('h4')\n",
    "metrics = [h4_tag.get_text(strip=True) for h4_tag in h4_tags]\n",
    "# print(metrics)\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "descriptions = [p_tag.get_text() for p_tag in p_tags]\n",
    "descriptions = descriptions[2:] # drop first lines\n",
    "# print(descriptions)\n",
    "\n",
    "# remove from the list the metrics withoud description (not useful for our purpose)\n",
    "metrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "metrics.remove('Aye10032/loss_metric')\n",
    "metrics.remove('giulio98/code_eval_outputs')\n",
    "metrics.remove('maysonma/lingo_judge_metric')\n",
    "metrics.remove('lvwerra/test')\n",
    "metrics.remove('sma2023/wil')\n",
    "\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "# From the lists, remove the descriptions and then the relative metric in the same index that have in the description 'TODO: add a description here\\n\\t\\t\\t\\t\\t\\t'ArithmeticError\n",
    "\n",
    "for i, description in enumerate(descriptions):\n",
    "    if 'TODO: add a description here' in description:\n",
    "        metrics.pop(i)\n",
    "        descriptions.pop(i)\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json(metrics, descriptions):\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "    \n",
    "        metrics_data.append(metric_attributes)\n",
    "        \n",
    "\n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Metric.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json(metrics, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def metric_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tcontext_count = 0\n",
    "\tfeatureBased_endToEnd_count = 0\n",
    "\tgranularity_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tmetric_json = open(os.path.join(result_path, 'ChatIMPACT.Metric.json'))\n",
    "\tmetric_data_json = json.load(metric_json)\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif item['context'] is not None:\n",
    "\t\t\tcontext_count += 1\n",
    "\t\tif item['featureBased/endToEnd'] is not None:\n",
    "\t\t\tfeatureBased_endToEnd_count += 1\n",
    "\t\tif item['granularity'] is not None:\t\n",
    "\t\t\tgranularity_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Context: {context_count} ({(context_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    FeatureBased/endToEnd: {featureBased_endToEnd_count} ({(featureBased_endToEnd_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Granularity: {granularity_count} ({(granularity_count / total_datasets) * 100:.2f}%)')\n",
    "\t\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tmetric_attributes = metric_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in metric_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed datasets: 218\n",
      "    Name: 218 (100.00%)\n",
      "    Description: 218 (100.00%)\n",
      "    Context: 0 (0.00%)\n",
      "    FeatureBased/endToEnd: 0 (0.00%)\n",
      "    Granularity: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = metric_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>featureBased/endToEnd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>granularity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>featureBased/endToEnd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>granularity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id entity name         attribute name available API  \\\n",
       "0   accuracy      Metric                   name         False   \n",
       "1   accuracy      Metric            description         False   \n",
       "2   accuracy      Metric                context         False   \n",
       "3   accuracy      Metric  featureBased/endToEnd         False   \n",
       "4   accuracy      Metric            granularity         False   \n",
       "5  bertscore      Metric                   name         False   \n",
       "6  bertscore      Metric            description         False   \n",
       "7  bertscore      Metric                context         False   \n",
       "8  bertscore      Metric  featureBased/endToEnd         False   \n",
       "9  bertscore      Metric            granularity         False   \n",
       "\n",
       "  available scraping  \n",
       "0               True  \n",
       "1               True  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  \n",
       "5               True  \n",
       "6               True  \n",
       "7              False  \n",
       "8              False  \n",
       "9              False  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_relationship():\n",
    "    \n",
    "    train_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        datasets = match_dataset(model_tags)\n",
    "        \n",
    "        if not datasets:\n",
    "            continue\n",
    "        else:\n",
    "            train_relationship = dict()\n",
    "            train_relationship['Models'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            train_relationship['Datasets'] = datasets # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            train_relationship_list.append(train_relationship)\n",
    "    \n",
    "    return train_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\ttrain_relationship = extract_train_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.TrainRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(train_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_suited_for_relationship():\n",
    "    \n",
    "    suited_for_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        \n",
    "        tasks = []\n",
    "        for t in model_tags:\n",
    "            if t in TAG_DOWNSTREAM_TASK:\n",
    "                tasks.append(t)\n",
    "\n",
    "        if not tasks:\n",
    "            continue\n",
    "        else:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            suited_for_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            suited_for_relationship_list.append(suited_for_relationship)\n",
    "    \n",
    "    return suited_for_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tsuited_for_relationship = extract_suited_for_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.SuitedForRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(suited_for_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enable_relationship():\n",
    "\t\n",
    "\tenable_relationship_list = []\n",
    "\t\n",
    "\tfor dataset_idx in range(len(datasets_df)):\n",
    "\t\t\n",
    "\t\tdataset = datasets_df.loc[dataset_idx]\n",
    "\t\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\t\t\n",
    "\t\ttasks = match_tasks(dataset_tags)\n",
    "\n",
    "\t\tif not tasks:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset['id']) # {\"$oid\": <...>} for MongoDB\n",
    "\t\t\tenable_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "\t\t\tenable_relationship_list.append(enable_relationship)\n",
    "\t\n",
    "\treturn enable_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tenable_relationship = extract_enable_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EnableRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(enable_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enable_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: here https://huggingface.co/tasks some tasks have associated metrics, we could scrape the tasks one by one\n",
    "\n",
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\\n\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "    return assess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.AssessRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task: text-classification\n",
      "\n",
      "Processing task: token-classification\n",
      "\n",
      "Processing task: table-question-answering\n",
      "\n",
      "Processing task: question-answering\n",
      "\n",
      "Processing task: zero-shot-classification\n",
      "\n",
      "Processing task: translation\n",
      "\n",
      "Processing task: summarization\n",
      "\n",
      "Processing task: feature-extraction\n",
      "\n",
      "Processing task: text-generation\n",
      "\n",
      "Processing task: fill-mask\n",
      "\n",
      "Processing task: sentence-similarity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def extract_evaluate_relationship():\n",
    "\n",
    "\tevaluate_relationship_list = []\n",
    "\n",
    "\tfor model_idx in range(len(models_df)):\n",
    "\n",
    "\t\tmodel = models_df.loc[model_idx]\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tprint('No card data available for this model')\n",
    "\t\t\n",
    "\t\tmetrics = []\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\n",
    "\t\tif not metrics:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tevaluate_relationship = dict()\n",
    "\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model['id'])\n",
    "\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\tevaluate_relationship_list.append(evaluate_relationship)\n",
    "\t\n",
    "\treturn evaluate_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluate_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tevaluate_relationship = extract_evaluate_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EvaluateRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(evaluate_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

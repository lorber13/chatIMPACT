{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frabet/miniconda3/envs/hf-api-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tags import * # tags.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>mask_token</th>\n",
       "      <th>card_data</th>\n",
       "      <th>widget_data</th>\n",
       "      <th>model_index</th>\n",
       "      <th>config</th>\n",
       "      <th>transformers_info</th>\n",
       "      <th>siblings</th>\n",
       "      <th>spaces</th>\n",
       "      <th>safetensors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert/albert-base-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>082438ba120d36b97b9288772a41144e941705b9</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:57:35+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>14192</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert/albert-base-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>8e2f239c5f8a2c0f253781ca60135db913e5c80c</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:14+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2324178</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert/albert-large-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>94fd741fb5d6cb5bc578fc154837016c583bafef</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1853</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert/albert-large-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>dfed3a5ef4499fb3351c4ebbcf487375d1e942c8</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>16062</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert/albert-xlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>ed6f87d14403b3c459a458fa6aa9dc5c51c517c1</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:28+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1299</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert/albert-xlarge-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>4fd2c2aa9aeb305f87704a7e595be7bfffa3db88</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-04-10 09:57:46+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>3739</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert/albert-xxlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>43129068ee5f6a481c148daeac11cc593b8ff440</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:42+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>4787</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert/albert-xxlarge-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>97d3e58863d3a41dc581882f73b34d110b18f1f8</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:02:09+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>8349</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google-bert/bert-base-cased-finetuned-mrpc</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>f150c1d609d1e50dd5e2e5408661cfac8339277c</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:03:21+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>52041</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google-bert/bert-base-cased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>cd5ef92a9fb2f889e972770a36d4ed042daf221e</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:02:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>5829586</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'rfilename': '.gitattributes', 'size': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id       author  \\\n",
       "0                       albert/albert-base-v1       albert   \n",
       "1                       albert/albert-base-v2       albert   \n",
       "2                      albert/albert-large-v1       albert   \n",
       "3                      albert/albert-large-v2       albert   \n",
       "4                     albert/albert-xlarge-v1       albert   \n",
       "5                     albert/albert-xlarge-v2       albert   \n",
       "6                    albert/albert-xxlarge-v1       albert   \n",
       "7                    albert/albert-xxlarge-v2       albert   \n",
       "8  google-bert/bert-base-cased-finetuned-mrpc  google-bert   \n",
       "9                 google-bert/bert-base-cased  google-bert   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  082438ba120d36b97b9288772a41144e941705b9 2022-03-02 23:29:04+00:00   \n",
       "1  8e2f239c5f8a2c0f253781ca60135db913e5c80c 2022-03-02 23:29:04+00:00   \n",
       "2  94fd741fb5d6cb5bc578fc154837016c583bafef 2022-03-02 23:29:04+00:00   \n",
       "3  dfed3a5ef4499fb3351c4ebbcf487375d1e942c8 2022-03-02 23:29:04+00:00   \n",
       "4  ed6f87d14403b3c459a458fa6aa9dc5c51c517c1 2022-03-02 23:29:04+00:00   \n",
       "5  4fd2c2aa9aeb305f87704a7e595be7bfffa3db88 2022-03-02 23:29:04+00:00   \n",
       "6  43129068ee5f6a481c148daeac11cc593b8ff440 2022-03-02 23:29:04+00:00   \n",
       "7  97d3e58863d3a41dc581882f73b34d110b18f1f8 2022-03-02 23:29:04+00:00   \n",
       "8  f150c1d609d1e50dd5e2e5408661cfac8339277c 2022-03-02 23:29:04+00:00   \n",
       "9  cd5ef92a9fb2f889e972770a36d4ed042daf221e 2022-03-02 23:29:04+00:00   \n",
       "\n",
       "              last_modified  private  gated disabled  downloads  likes  ...  \\\n",
       "0 2024-02-19 10:57:35+00:00    False  False     None      14192      8  ...   \n",
       "1 2024-02-19 10:58:14+00:00    False  False     None    2324178     99  ...   \n",
       "2 2024-02-19 10:58:26+00:00    False  False     None       1853      3  ...   \n",
       "3 2024-02-19 10:58:48+00:00    False  False     None      16062     15  ...   \n",
       "4 2024-02-19 11:01:28+00:00    False  False     None       1299      4  ...   \n",
       "5 2024-04-10 09:57:46+00:00    False  False     None       3739      8  ...   \n",
       "6 2024-02-19 11:01:42+00:00    False  False     None       4787      5  ...   \n",
       "7 2024-02-19 11:02:09+00:00    False  False     None       8349     19  ...   \n",
       "8 2024-02-19 11:03:21+00:00    False  False     None      52041      1  ...   \n",
       "9 2024-02-19 11:02:26+00:00    False  False     None    5829586    246  ...   \n",
       "\n",
       "  pipeline_tag mask_token card_data widget_data model_index config  \\\n",
       "0    fill-mask       None      None        None        None   None   \n",
       "1    fill-mask       None      None        None        None   None   \n",
       "2    fill-mask       None      None        None        None   None   \n",
       "3    fill-mask       None      None        None        None   None   \n",
       "4    fill-mask       None      None        None        None   None   \n",
       "5    fill-mask       None      None        None        None   None   \n",
       "6    fill-mask       None      None        None        None   None   \n",
       "7    fill-mask       None      None        None        None   None   \n",
       "8    fill-mask       None      None        None        None   None   \n",
       "9    fill-mask       None      None        None        None   None   \n",
       "\n",
       "  transformers_info                                           siblings spaces  \\\n",
       "0              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "1              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "2              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "3              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "4              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "5              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "6              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "7              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "8              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "9              None  [{'rfilename': '.gitattributes', 'size': None,...   None   \n",
       "\n",
       "  safetensors  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  \n",
       "6        None  \n",
       "7        None  \n",
       "8        None  \n",
       "9        None  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = itertools.islice(models, 0, 1000)\n",
    "models_df = pd.DataFrame(model)\n",
    "models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
      "       'gated', 'disabled', 'downloads', 'likes', 'library_name', 'tags',\n",
      "       'pipeline_tag', 'mask_token', 'card_data', 'widget_data', 'model_index',\n",
      "       'config', 'transformers_info', 'siblings', 'spaces', 'safetensors'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(models_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers',\n",
       " 'pytorch',\n",
       " 'tf',\n",
       " 'safetensors',\n",
       " 'albert',\n",
       " 'fill-mask',\n",
       " 'exbert',\n",
       " 'en',\n",
       " 'dataset:bookcorpus',\n",
       " 'dataset:wikipedia',\n",
       " 'arxiv:1909.11942',\n",
       " 'license:apache-2.0',\n",
       " 'autotrain_compatible',\n",
       " 'endpoints_compatible',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag examples\n",
    "models_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers',\n",
       " 'pytorch',\n",
       " 'tensorboard',\n",
       " 'encoder-decoder',\n",
       " 'text2text-generation',\n",
       " 'generated_from_trainer',\n",
       " 'dataset:cnn_dailymail',\n",
       " 'autotrain_compatible',\n",
       " 'endpoints_compatible',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# richer tag information example\n",
    "example_model = api.list_models(model_name='albert_bert_summarization_cnn_dailymail')\n",
    "example_df = pd.DataFrame(example_model)\n",
    "example_df.loc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mca', 'amc', 'leh', 'kli', 'bui', 'bix', 'bsf', 'tce', 'hah', 'lar', 'mxy', 'oci', 'enm', 'sdo', 'kxx', 'wja', 'mii', 'njs', 'chb', 'kst', 'krs', 'waj', 'kwu', 'are', 'kgo', 'cov', 'pom', 'szg', 'ald', 'dsb', 'lky', 'tuv', 'auu', 'rat', 'grv', 'pym', 'bba', 'tpq', 'kxv', 'fy', 'bdd', 'bjt', 'xkv', 'spo', 'cav', 'kna', 'ogc', 'mfi', 'psw', 'tkp', 'pmx', 'mwt', 'ydd', 'enq', 'kwg', 'ugo', 'hmz', 'zpy', 'ssg', 'mmh', 'jav', 'pud', 'lgq', 'lus', 'ase', 'pbt', 'nng', 'mor', 'bpa', 'bxa', 'mnu', 'okr', 'ngt', 'nir', 'kha', 'nxr', 'keb', 'swl', 'tyy', 'jmn', 'ach', 'lud', 'atj', 'bqc', 'kbx', 'ncq', 'gv', 'mmc', 'maf', 'kmi', 'ymm', 'kej', 'fas', 'ces', 'pai', 'wbl', 'awi', 'trf', 'qxu', 'bqr', 'xtm', 'mfd', 'sjl', 'fue', 'kel', 'kvb', 'urh', 'huh', 'qum', 'ngz', 'mme', 'kud', 'szl', 'quf', 'esk', 'bbf', 'jub', 'srx', 'kur', 'avu', 'luo', 'mnp', 'yot', 'idi', 'bjo', 'kps', 'lhl', 'kqj', 'zro', 'koi', 'bwu', 'vmk', 'cob', 'sms', 'kev', 'mgf', 'ktn', 'gni', 'wnu', 'sme', 'kdc', 'bft', 'bea', 'myu', 'mej', 'ida', 'ts', 'nyk', 'lhi', 'kyq', 'ngu', 'mto', 'qxh', 'njn', 'roo', 'th', 'gor', 'kwj', 'hsb', 'rmn', 'moj', 'krv', 'st', 'cux', 'xra', 'mql', 'ctd', 'kcs', 'jrt', 'diq', 'dni', 'prf', 'oyb', 'nja', 'mls', 'pku', 'pga', 'zpp', 'its', 'klv', 'mbh', 'emi', 'agw', 'anx', 'bvw', 'bfs', 'dbm', 'tpi', 'qvw', 'dje', 'nmb', 'am', 'jam', 'cym', 'szb', 'acf', 'mkn', 'snq', 'dww', 'dbn', 'doa', 'bzh', 'mbj', 'ada', 'ttq', 'kpk', 'bgc', 'jnj', 'asa', 'wbf', 'tvk', 'siu', 'hno', 'haa', 'zpj', 'ncf', 'os', 'kwt', 'deu', 'dez', 'sas', 'kcl', 'aix', 'pab', 'csh', 'cjp', 'zzj', 'mlk', 'tww', 'tlp', 'ty', 'arr', 'frm', 'itv', 'wlw', 'yno', 'stp', 'stj', 'vnk', 'mxv', 'scw', 'kqk', 'qus', 'xem', 'nyh', 'gwi', 'vif', 'bib', 'kms', 'cnc', 'wuu', 'fuh', 'zos', 'umu', 'lpo', 'khw', 'gis', 'kmo', 'pch', 'kmu', 'hbo', 'wos', 'wod', 'lez', 'hna', 'kjb', 'inb', 'lme', 'kgq', 'klk', 'bst', 'pma', 'kcg', 'prs', 'aty', 'hu', 'ayo', 'ttv', 'kwn', 'oia', 'kfe', 'xns', 'mng', 'aaf', 'mrv', 'ksn', 'udm', 'cna', 'pid', 'gux', 'mgb', 'mzr', 'mik', 'guu', 'dtb', 'bdb', 'wca', 'end', 'nof', 'yuq', 'rwo', 'boj', 'cag', 'trp', 'wog', 'sey', 'okv', 'ce', 'hld', 'the', 'ijs', 'mnb', 'mvz', 'boz', 'jeh', 'zaj', 'zsr', 'cli', 'caf', 'lra', 'pad', 'sbg', 'kzm', 'mxh', 'btm', 'lip', 'bbw', 'af', 'dmw', 'xsi', 'nrf', 'acr', 'bss', 'rim', 'com', 'pfl', 'lae', 'syr', 'xsb', 'ths', 'ngs', 'iba', 'yrl', 'nmf', 'yiu', 'mip', 'mpg', 'tzj', 'swo', 'gud', 'trq', 'cul', 'iow', 'kvw', 'nto', 'war', 'heb', 'bcr', 'kqp', 'fr', 'nhe', 'pse', 'shj', 'tli', 'shq', 'rml', 'sah', 'xkn', 'toi', 'liu', 'pei', 'pcf', 'cri', 'mbc', 'ppl', 'dik', 'laa', 'srz', 'tny', 'zxx', 'mio', 'tue', 'ifk', 'xbr', 'duq', 'dao', 'kqc', 'kip', 'asm', 'klr', 'bum', 'ijc', 'ifb', 'cat', 'bjc', 'bts', 'nen', 'gax', 'khq', 'tyz', 'prg', 'nud', 'des', 'gbv', 'ztg', 'nnm', 'ayn', 'gri', 'twp', 'beh', 'ong', 'koh', 'oub', 'jv', 'jiy', 'mrp', 'clj', 'lal', 'mve', 'fo', 'slu', 'wbq', 'ukp', 'kxb', 'hms', 'jdt', 'bmf', 'mke', 'rir', 'cbg', 'nqt', 'gbn', 'aaz', 'tnn', 'ruf', 'bin', 'ral', 'loy', 'bdl', 'got', 'skg', 'bpe', 'ht', 'for', 'goh', 'aki', 'mbx', 'jdg', 'aon', 'ign', 'hmn', 'zpi', 'kpe', 'cax', 'bbb', 'kck', 'zbc', 'twh', 'moc', 'zaw', 'txu', 'biu', 'nzi', 'ncu', 'lbu', 'lim', 'csb', 'opm', 'mag', 'mle', 'mus', 'iru', 'kkk', 'tdy', 'mmd', 'giw', 'zyb', 'ryu', 'luz', 'pwa', 'unk', 'bzx', 'nti', 'kio', 'amb', 'jid', 'thd', 'aqm', 'plk', 'nim', 'pot', 'huv', 'alh', 'psa', 'lil', 'jvn', 'mpj', 'run', 'mza', 'mxl', 'dbd', 'ood', 'kwc', 'nes', 'miu', 'suq', 'ykm', 'pbi', 'lyg', 'bzs', 'zyn', 'asr', 'nyb', 'cbn', 'twr', 'rdb', 'mkf', 'one', 'tiq', 'gl', 'klq', 'gwr', 'gcr', 'uth', 'yon', 'tap', 'ors', 'phr', 'mek', 'hup', 'lir', 'yut', 'shu', 'igs', 'nmh', 'smj', 'agx', 'jei', 'oki', 'mhk', 'npy', 'naj', 'sun', 'tra', 'glk', 'dgd', 'aze', 'mpd', 'mtg', 'flr', 'lmd', 'van', 'mjg', 'kbr', 'myv', 'ogb', 'kpm', 'gya', 'bif', 'tw', 'coc', 'dor', 'ass', 'ben', 'frd', 'wba', 'gla', 'ikt', 'uig', 'rtm', 'maa', 'dan', 'nkh', 'bxl', 'bbp', 'hux', 'anf', 'kkn', 'nzm', 'cog', 'not', 'ahg', 'ado', 'apj', 'ppt', 'lrc', 'crm', 'som', 'blr', 'wuv', 'gmz', 'suk', 'kqi', 'avk', 'mep', 'pri', 'cvg', 'gnw', 'lea', 'ie', 'hak', 'lti', 'cpa', 'ndo', 'bpx', 'da', 'knd', 'jow', 'dim', 'vrs', 'kjo', 'mib', 'tml', 'tnv', 'mij', 'ldp', 'bzv', 'slx', 'ono', 'cfa', 'mvg', 'bep', 'ung', 'fkv', 'quw', 'ktv', 'nla', 'dyo', 'kik', 'vai', 'czn', 'fj', 'pan', 'cot', 'gop', 'sav', 'lua', 'ckb', 'sqq', 'shr', 'awe', 'bmm', 'kiu', 'drs', 'pcn', 'yba', 'mwv', 'tvs', 'wgi', 'bsn', 'gox', 'smt', 'tok', 'anl', 'zu', 'guw', 'twi', 'mnk', 'ngn', 'mrg', 'kjr', 'mhp', 'etc', 'buz', 'bqt', 'bap', 'kze', 'fra', 'sop', 'zak', 'log', 'blo', 'sjp', 'xnz', 'mxj', 'caz', 'jnl', 'lrl', 'wnc', 'due', 'bvm', 'eng', 'slk', 'bdu', 'nhd', 'ang', 'mie', 'vut', 'ori', 'pir', 'dib', 'ilp', 'bwe', 'mfa', 'ebo', 'sdg', 'way', 'wob', 'mzv', 'byj', 'tab', 'ccp', 'xrw', 'saz', 'bpp', 'bhz', 'si', 'sgc', 'mds', 'aoj', 'lol', 'ckx', 'tcn', 'tcc', 'hla', 'sst', 'mnz', 'tan', 'mkk', 'ria', 'kde', 'zpc', 'huc', 'pwm', 'arb', 'tdc', 'etr', 'wlo', 'cco', 'yoy', 'yey', 'maq', 'jge', 'oyd', 'hmb', 'tnb', 'gof', 'ssw', 'hui', 'rar', 'wrm', 'tdo', 'sgh', 'max', 'shs', 'ky', 'rab', 'tgk', 'yas', 'mul', 'jat', 'pst', 'kdr', 'aso', 'yal', 'ndc', 'ysn', 'anp', 'tmq', 'nfd', 'plg', 'yaw', 'ian', 'bis', 'nhr', 'mho', 'kmk', 'der', 'tk', 'bao', 'weh', 'tcy', 'iyx', 'gnm', 'zlj', 'cpu', 'bhi', 'har', 'syw', 'aly', 'zab', 'qxl', 'cje', 'mpr', 'nwi', 'sat', 'ish', 'dza', 'tay', 'won', 'urt', 'bth', 'arw', 'haz', 'xub', 'et', 'tiv', 'agf', 'miy', 'tsu', 'zlm', 'spa', 'kvr', 'srb', 'hwc', 'bal', 'kpq', 'aji', 'zoh', 'zgb', 'jna', 'dz', 'zza', 'aab', 'whk', 'tlf', 'ney', 'vol', 'sgs', 'soq', 'ms', 'mrf', 'azm', 'yrb', 'kgb', 'uzb', 'pbl', 'cjv', 'ons', 'bek', 'kmq', 'kdd', 'tfi', 'mdk', 'cr', 'smy', 'kdi', 'lmx', 'so', 'mvn', 'lsi', 'cho', 'ssn', 'sip', 'mir', 'bim', 'ru', 'xer', 'goa', 'ghk', 'fro', 'kit', 'eja', 'nld', 'hmd', 'dij', 'jns', 'kv', 'abs', 'knw', 'bfz', 'ntm', 'acd', 'kmt', 'dbq', 'dcr', 'ksi', 'kmr', 'nhv', 'xkj', 'rei', 'vro', 'lbo', 'srl', 'tcf', 'lul', 'ywl', 'buo', 'ayr', 'kzq', 'mal', 'bhg', 'bqx', 'knf', 'acq', 'rgu', 'eu', 'ktc', 'mgh', 'bon', 'msj', 'tva', 'gew', 'ndv', 'ksu', 'sct', 'oni', 'jun', 'bxb', 'ig', 'soa', 'kfa', 'mcc', 'piv', 'lch', 'itz', 'nza', 'mdt', 'xkb', 'gyr', 'ogg', 'job', 'sbd', 'doi', 'trs', 'msh', 'mfn', 'lan', 'sil', 'scg', 'crv', 'dgc', 'kdl', 'nyq', 'ert', 'sk', 'pau', 'xmh', 'mjv', 'gom', 'ets', 'cns', 'bsk', 'hia', 'wmo', 'tum', 'vra', 'ykg', 'sry', 'nau', 'iyo', 'env', 'km', 'jul', 'ilb', 'rmq', 'cko', 'lse', 'nus', 'khr', 'cni', 'gbi', 'raf', 'kwx', 'kji', 'ife', 'gmh', 'ape', 'orx', 'sbc', 'xwl', 'kqa', 'nag', 'itt', 'aau', 'bhf', 'be', 'baa', 'dws', 'bzt', 'bwf', 'nup', 'gnd', 'qud', 'mqj', 'mcn', 'gmv', 'dms', 'mma', 'saj', 'kvu', 'pcb', 'xmt', 'uuu', 'bav', 'bam', 'ksp', 'acw', 'yan', 'mkb', 'lhm', 'rol', 'ta', 'zpg', 'far', 'dby', 'gqr', 'kcv', 'abq', 'otd', 'nhb', 'tzl', 'oj', 'twu', 'hz', 'nsa', 'uar', 'sen', 'tlr', 'bsb', 'fuy', 'bgn', 'sl', 'jbu', 'pcj', 'fll', 'tbg', 'dv', 'lcc', 'tul', 'atp', 'org', 'sxn', 'ffm', 'kpg', 'yak', 'ndm', 'rey', 'dsh', 'bfu', 'bbo', 'zin', 'sot', 'grn', 'gul', 'tbj', 'nop', 'yin', 'sef', 'all', 'vag', 'yea', 'mnf', 'ayg', 'kid', 'mvo', 'ldl', 'mch', 'coh', 'fat', 'apy', 'gil', 'aww', 'kot', 'bwi', 'xed', 'ags', 'ken', 'yue', 'bwx', 'ltz', 'mxm', 'mqg', 'bvu', 'jab', 'cy', 'top', 'tcx', 'nuf', 'amt', 'pum', 'sdc', 'bzi', 'pru', 'mns', 'xnr', 'lee', 'pay', 'hoe', 'lmg', 'uya', 'kvj', 'gkn', 'rn', 'tod', 'tpx', 'dgr', 'kij', 'bbi', 'mzi', 'abi', 'nia', 'txa', 'bvc', 'cta', 'bxr', 'ydg', 'qxq', 'mde', 'snp', 'cbd', 'auk', 'bhp', 'jru', 'bks', 'iso', 'aaa', 'kge', 'sda', 'ne', 'tdx', 'msg', 'rao', 'gbz', 'shn', 'tvl', 'sok', 'quc', 'gos', 'ato', 'doo', 'buh', 'cjm', 'ido', 'frp', 'por', 'blb', 'bgt', 'nab', 'gbm', 'zpw', 'hne', 'tah', 'sro', 'xdo', 'kif', 'qvc', 'otk', 'ory', 'xsr', 'gfk', 'hdy', 'abz', 'bmb', 'kin', 'aoz', 'faa', 'luc', 'mua', 'nlo', 'zmq', 'kqf', 'bzz', 'vmm', 'syc', 'dhg', 'qxa', 'oke', 'sbu', 'shc', 'ekp', 'etx', 'yuy', 'pam', 'smo', 'ium', 'nch', 'mif', 'enn', 'rom', 'mzl', 'yua', 'bgj', 'nmk', 'chl', 'brl', 'nri', 'tiy', 'nse', 'sfm', 'yer', 'nsm', 'tly', 'udg', 'zaz', 'cde', 'mrz', 'ba', 'mgp', 'fun', 'atg', 'itl', 'roh', 'mdu', 'gwd', 'wbm', 'hbb', 'cnt', 'mhu', 'tsj', 'ggu', 'pbo', 'tuf', 'dgx', 'jpn', 'ymb', 'wls', 'kxm', 'sns', 'bjj', 'ayb', 'mfy', 'kqo', 'sgy', 'kaq', 'glg', 'ril', 'lyn', 'tbt', 'mbu', 'smf', 'kml', 'bg', 'att', 'old', 'mgu', 'mgi', 'dof', 'lna', 'knz', 'nl', 'mur', 'niu', 'tar', 'tgc', 'diz', 'vsl', 'gyz', 'ybe', 'cod', 'ifa', 'erh', 'sn', 'mdb', 'hil', 'gbe', 'xbi', 'guz', 'hum', 'maj', 'trd', 'mcp', 'nzy', 'kca', 'lgm', 'liv', 'yre', 'mxd', 'xtl', 'arg', 'sbe', 'biv', 'cdi', 'tso', 'ojs', 'crh', 'ljp', 'rus', 'avl', 'knc', 'sna', 'lbx', 'auq', 'fut', 'pta', 'deg', 'acm', 'tnc', 'kcd', 'bvi', 'tri', 'mcq', 'aba', 'pwb', 'vmj', 'san', 'tow', 'bbk', 'pkb', 'kom', 'wlx', 'sgz', 'ia', 'ebk', 'hni', 'sei', 'nlk', 'asg', 'fai', 'bqo', 'glo', 'rhp', 'caa', 'gvs', 'ace', 'tdh', 'yaq', 'kjs', 'kal', 'tom', 'tfn', 'sax', 'mqu', 'kzi', 'awx', 'plv', 'zpo', 'ijn', 'rmy', 'qux', 'pcc', 'kjp', 'sly', 'cmi', 'msa', 'slr', 'grj', 'akq', 'kyb', 'mog', 'def', 'kpf', 'naf', 'juk', 'zap', 'tqu', 'ahr', 'kby', 'nmz', 'mjt', 'cdo', 'lit', 'scv', 'brq', 'vie', 'adx', 'lun', 'mlx', 'chz', 'nkx', 'ddn', 'khn', 'bxg', 'sys', 'xgu', 'kkl', 'lld', 'duv', 'txo', 'mum', 'krf', 'nao', 'jib', 'ee', 'blm', 'bee', 'gdx', 'to', 'gia', 'gaa', 'dnw', 'sre', 'klo', 'asb', 'lef', 'hub', 'rro', 'uba', 'mna', 'nyd', 'azb', 'muk', 'kyh', 'ss', 'mfl', 'tmn', 'bev', 'ckm', 'kie', 'tac', 'txy', 'tet', 'ngb', 'kjc', 'kwe', 'pms', 'ijj', 'kpc', 'olo', 'izz', 'ena', 'tts', 'siw', 'kem', 'mjw', 'eit', 'poi', 'ssd', 'ain', 'gww', 'dus', 'jeb', 'pio', 'djm', 'ibm', 'gae', 'kee', 'lhu', 'ruz', 'tob', 'czt', 'ybi', 'haq', 'izh', 'kph', 'mnl', 'jmb', 'khy', 'zoc', 'wsa', 'guo', 'shw', 'qxs', 'shk', 'dil', 'kln', 'wro', 'njb', 'byo', 'kuh', 'aym', 'xmf', 'ptu', 'wiu', 'bwt', 'dhm', 'saw', 'bhd', 'lmi', 'keu', 'nhp', 'iki', 'ipo', 'non', 'hot', 'wno', 'see', 'qva', 'aez', 'gle', 'emb', 'tgp', 'kza', 'acu', 'ata', 'chn', 'bho', 'gra', 'mgd', 'dyg', 'oka', 'hin', 'bse', 'say', 'aqt', 'nhn', 'fur', 'nyn', 'zas', 'itd', 'bot', 'psh', 'csk', 'crq', 'amh', 'ksm', 'ygw', 'dmr', 'loz', 'pcg', 'bfd', 'bkk', 'pne', 'bak', 'nav', 'ige', 'tdd', 'jgk', 'mbp', 'aak', 'slz', 'dcc', 'wni', 'zae', 'myk', 'pcl', 'cuc', 'ina', 'tkd', 'dee', 'akl', 'zwa', 'nni', 'gsw', 'bcq', 'gna', 'vaf', 'kjq', 'que', 'odk', 'apu', 'zne', 'bey', 'bjp', 'bmk', 'ahp', 'lel', 'kbd', 'nmw', 'skv', 'kod', 'kks', 'kdm', 'skx', 'tdn', 'tpr', 'zdj', 'cgg', 'pxm', 'dhv', 'sus', 'lui', 'swi', 'kht', 'dei', 'msw', 'mfq', 'mzk', 'pis', 'yao', 'zrs', 'ngj', 'hoy', 'xom', 'wof', 'pls', 'bil', 'cs', 'frr', 'esg', 'aui', 'kxh', 'kuy', 'jbm', 'tov', 'tuc', 'cox', 'aer', 'mer', 'ktu', 'aap', 'kam', 'niy', 'bgg', 'sym', 'nbq', 'lmu', 'gua', 'ram', 'nhu', 'yur', 'cle', 'aaw', 'jao', 'atk', 'bhy', 'bjg', 'njm', 'tth', 'loa', 'kvd', 'aog', 'poe', 'bji', 'kdh', 'shm', 'ktp', 'rmz', 'ich', 'zpf', 'jac', 'cou', 'thl', 'lu', 'kih', 'dhi', 'anc', 'svb', 'kac', 'kla', 'tvn', 'tpa', 'jog', 'swr', 'sjg', 'stt', 'zty', 'thq', 'xte', 'apt', 'ifu', 'hmr', 'app', 'nin', 'tlj', 'brg', 'xky', 'sua', 'nif', 'spm', 'hix', 'mrr', 'aar', 'amk', 'ldi', 'tlq', 'apw', 'tvd', 'bmh', 'wgb', 'nrm', 'mgc', 'xmg', 'ceb', 'shh', 'bet', 'npo', 'sfw', 'otx', 'evn', 'lum', 'bei', 'ajg', 'nnp', 'dun', 'hye', 'ewe', 'pbb', 'ae', 'brd', 'gok', 'agb', 'qup', 'yui', 'apr', 'hve', 'wau', 'iai', 'rwk', 'mji', 'gag', 'ndy', 'dgz', 'qvh', 'vmw', 'bgf', 'bbu', 'sts', 'eky', 'bzu', 'zms', 'wrs', 'lek', 'gpa', 'tby', 'pgg', 'mad', 'knu', 'soj', 'swv', 'vig', 'bxq', 'kyk', 'aha', 'vot', 'tft', 'bjh', 'akc', 'amp', 'irr', 'vo', 'bte', 'xmw', 'mxt', 'chy', 'ojb', 'glv', 'khj', 'kxj', 'poy', 'mva', 'ndd', 'unx', 'qwa', 'kjd', 'ch', 'mhs', 'atq', 'adi', 'sbh', 'tkg', 'nmi', 'loh', 'dnj', 'djk', 'vaj', 'leu', 'ibg', 'dag', 'rop', 'krr', 'mop', 'bhr', 'ycl', 'gbk', 'dav', 'sdr', 'mzj', 'seg', 'aoa', 'ell', 'bos', 'mfk', 'gau', 'as', 'tus', 'bzk', 'kyg', 'lin', 'bza', 'pht', 'bra', 'buf', 'uis', 'toq', 'bws', 'ask', 'wbi', 'kgf', 'gu', 'kmd', 'beu', 'mhz', 'wew', 'smk', 'twe', 'tuo', 'pic', 'nma', 'ldj', 'gig', 'lav', 'bzj', 'mbm', 'pmf', 'gou', 'kog', 'akt', 'bpr', 'msy', 'nms', 'srn', 'mmx', 'bfo', 'brx', 'aee', 'rug', 'hnn', 'wrk', 'anm', 'ksj', 'olu', 'dwa', 'crs', 'szp', 'khc', 'lfn', 'zar', 'gdr', 'tpe', 'yid', 'bva', 'kvt', 'grd', 'dka', 'vkl', 'kxc', 'mbq', 'smn', 'myx', 'nhy', 'box', 'ka', 'kfc', 'blt', 'ems', 'fij', 'inj', 'spn', 'lgg', 'lle', 'bqv', 'liq', 'kwb', 'ykk', 'ayt', 'llg', 'bol', 'vun', 'pbm', 'xti', 'gmb', 'mdw', 'yiq', 'tyn', 'mmn', 'bmu', 'tn', 'mpc', 'ktj', 'tnk', 'mwf', 'cry', 'bex', 'kft', 'rjs', 'ura', 'mat', 'bri', 'mot', 'mmm', 'sdk', 'kbc', 'sez', 'bnm', 'ca', 'lex', 'cfg', 'gde', 'jkp', 'ccg', 'cdh', 'mkl', 'med', 'zau', 'nyg', 'yaa', 'has', 'nnc', 'ven', 'soe', 'ngp', 'oss', 'hag', 'soy', 'bdq', 'byp', 'isu', 'uzn', 'eyo', 'kci', 'tgl', 'arx', 'xuu', 'pa', 'lri', 'tcu', 'mpu', 'mlw', 'apd', 'akb', 'vkn', 'grt', 'juo', 'kpl', 'xtj', 'tsb', 'ncr', 'pil', 'kjt', 'ggb', 'pcm', 'aqz', 'la', 'pnz', 'mgl', 'gaj', 'ito', 'dbi', 'snf', 'hca', 'abr', 'lob', 'umm', 'enl', 'abm', 'agh', 'bjv', 'jya', 'gea', 'jbj', 'zrg', 'aup', 'wkd', 'cpb', 'cnl', 'gui', 'tal', 'tsp', 'nun', 'kai', 'tt', 'mai', 'mim', 'mbo', 'lpn', 'naz', 'ttr', 'otw', 'dwz', 'jwi', 'spt', 'es', 'lrk', 'vi', 'ga', 'gvl', 'skr', 'ztq', 'ahs', 'sha', 'lbf', 'asc', 'ots', 'apz', 'buw', 'lij', 'ulu', 'hed', 'ubr', 'avt', 'tsa', 'bzy', 'ula', 'pcw', 'piu', 'gd', 'lgr', 'gur', 'lbm', 'sid', 'lgt', 'wti', 'mtt', 'kan', 'pll', 'mfs', 'auy', 'kua', 'adn', 'aai', 'neq', 'rmo', 'nbr', 'swc', 'thm', 'ktb', 'rup', 'aoe', 'whg', 'dop', 'mui', 'kxw', 'bcl', 'inh', 'aao', 'nnl', 'kez', 'duo', 'saq', 'hre', 'ndz', 'nxk', 'hr', 'nbb', 'dya', 'ksc', 'kwo', 'dua', 'mos', 'ttj', 'gej', 'snk', 'kff', 'xod', 'huf', 'ted', 'sol', 'sky', 'nzk', 'nbl', 'knm', 'kfp', 'teq', 'crw', 'dde', 'baw', 'mrn', 'sbl', 'met', 'leq', 'mni', 'moa', 'dmg', 'hit', 'lag', 'nyu', 'kr', 'bsp', 'xuj', 'hus', 'bpn', 'ko', 'ncm', 'bka', 'zbu', 'uta', 'bxs', 'les', 'cma', 'mgw', 'sr', 'gjn', 'pwn', 'vay', 'fad', 'adq', 'ahk', 'lax', 'adz', 'afr', 'nna', 'atb', 'diu', 'llc', 'vav', 'vmx', 'kga', 'fal', 'bbv', 'aun', 'bsi', 'cdr', 'fry', 'aeb', 'nbc', 'bwm', 'jnd', 'ikk', 'kat', 'kpv', 'arv', 'sek', 'stf', 'nmc', 'ppi', 'bio', 'dre', 'agm', 'jml', 'afi', 'nde', 'zph', 'ner', 'pib', 'psn', 'blw', 'kmm', 'mfc', 'lb', 'rhg', 'bfh', 'dwy', 'gn', 'sru', 'xok', 'anj', 'ibd', 'jiu', 'pnq', 'ymk', 'als', 'btd', 'brf', 'cha', 'bni', 'miz', 'bpu', 'kky', 'nnz', 'gub', 'bej', 'mr', 'sdq', 'bfr', 'byz', 'cky', 'te', 'prc', 'xkf', 'mel', 'tmf', 'msm', 'mug', 'toj', 'lbn', 'tr', 'qvz', 'pbv', 'yde', 'azd', 'spl', 'nod', 'jup', 'pre', 'tyr', 'pqa', 'dme', 'apc', 'mtb', 'sie', 'cib', 'yiz', 'neb', 'akd', 'mdj', 'nv', 'kao', 'aad', 'gum', 'ttc', 'mzh', 'kcx', 'icr', 'sjo', 'thk', 'otr', 'mbf', 'meh', 'pag', 'lsm', 'yah', 'big', 'xho', 'cak', 'ckl', 'xwe', 'tim', 'cll', 'szy', 'raa', 'tzh', 'mqx', 'peg', 'kye', 'mxb', 'hyw', 'sui', 'krp', 'kfi', 'klg', 'pnu', 'moh', 'fay', 'prk', 'ses', 'lor', 'bwd', 'yra', 'szv', 'gry', 'tek', 'qvm', 'kqw', 'pcd', 'cac', 'cap', 'miq', 'akr', 'su', 'sue', 'cso', 'gez', 'mae', 'dsn', 'tdt', 'nss', 'bca', 'cbv', 'sif', 'chm', 'hbn', 'asu', 'brt', 'brr', 'ess', 'cih', 'bkv', 'zpr', 'sri', 'gut', 'quv', 'hru', 'frc', 'pl', 'aio', 'txt', 'nbh', 'mez', 'xrb', 'kbb', 'wmb', 'deh', 'ita', 'mgr', 'ind', 'bs', 'pnt', 'ssx', 'zsm', 'hrx', 'fod', 'enx', 'lga', 'btu', 'dkx', 'mkz', 'ybb', 'ebu', 'jmr', 'arz', 'cnr', 'pem', 'nkb', 'yup', 'kto', 'bxk', 'bqs', 'ary', 'utr', 'xon', 'yva', 'zun', 'dyu', 'bjk', 'zps', 'zho', 'gbl', 'tat', 'fli', 'pak', 'zyp', 'kei', 'kce', 'thz', 'pav', 'ann', 'moy', 'tgt', 'bh', 'pdc', 'bdv', 'kiw', 'ckt', 'gek', 'moe', 'wbk', 'tef', 'gcn', 'fin', 'is', 'yi', 'dbb', 'kri', 'sll', 'suc', 'nkf', 'qu', 'shi', 'sco', 'maw', 'cto', 'bta', 'ngl', 'ino', 'mrq', 'ldn', 'pbu', 'bht', 'gai', 'mzw', 'oji', 'cmr', 'kex', 'cor', 'ola', 'gvc', 'zim', 'dio', 'co', 'mya', 'tsn', 'ajz', 'sso', 'mwe', 'bec', 'iku', 'nkk', 'glj', 'aka', 'cro', 'mga', 'bpz', 'hul', 'cdj', 'nio', 'fqs', 'oks', 'kcf', 'kdu', 'goj', 'anr', 'vam', 'nga', 'jra', 'gun', 'ghn', 'aim', 'lwg', 'ptp', 'beq', 'kep', 'fud', 'otm', 'wsi', 'efi', 'krl', 'apm', 'mtp', 'bid', 'fvr', 'gol', 'cop', 'mab', 'jmx', 'muy', 'kow', 'sbz', 'sdh', 'gaf', 'ghr', 'nil', 'mxa', 'wsk', 'cnk', 'bsy', 'wol', 'bul', 'mwl', 'soo', 'isi', 'zns', 'tma', 'lml', 'ctp', 'naq', 'akk', 'shg', 'swk', 'mps', 'mro', 'bcf', 'sgp', 'agd', 'nnq', 'pdu', 'elk', 'kra', 'bar', 'eko', 'lop', 'ksf', 'ff', 'vum', 'bfm', 'twb', 'blf', 'pwo', 'led', 'nym', 'nap', 'bru', 'abh', 'njz', 'stn', 'mxp', 'aiw', 'ktz', 'wlc', 'tsz', 'qub', 'chu', 'lid', 'zte', 'yix', 'urd', 'zpu', 'gaw', 'sga', 'lah', 'iqu', 'jae', 'bkd', 'kfz', 'mqz', 'xsq', 'mfv', 'sep', 'tpp', 'rkb', 'coz', 'ncj', 'bku', 'cbo', 'yss', 'sgr', 'kbj', 'rme', 'mdf', 'boq', 'ghc', 'zpk', 'xcl', 'tcp', 'xnn', 'hnj', 'twy', 'ntr', 'zpm', 'mqb', 'iou', 'nan', 'lkr', 'kbo', 'sbb', 'ukr', 'sad', 'jio', 'fil', 'ega', 'seh', 'fie', 'mit', 'toh', 'plu', 'nyy', 'mgg', 'tih', 'wrp', 'hat', 'ybj', 'kvo', 'dbv', 'ks', 'tgj', 'car', 'isn', 'xsm', 'nke', 'tja', 'tdk', 'lbe', 'yli', 'ksw', 'apb', 'bvz', 'rcf', 'jum', 'ncx', 'bmi', 'mlm', 'kg', 'akh', 'xwg', 'ity', 'tmd', 'boa', 'tti', 'czh', 'bvr', 'mks', 'cjo', 'knk', 'lue', 'tel', 'drd', 'aks', 'kwy', 'bgv', 'bgs', 'cwe', 'nbu', 'guq', 'kdy', 'yki', 'kfv', 'buk', 'snc', 'sjr', 'bkl', 'cte', 'pbp', 'ynq', 'trn', 'law', 'gjk', 'ndb', 'buj', 'trv', 'mpm', 'ivv', 'pov', 'gvp', 'brp', 'sbp', 'kpj', 'bch', 'lao', 'mtu', 'xla', 'meu', 'irx', 'ybl', 'mov', 'bfb', 'zao', 'ota', 'nuj', 'awb', 'knn', 'dnd', 'bem', 'rej', 'kmg', 'nep', 'tty', 'bkr', 'mhl', 'zkd', 'nki', 'gvo', 'mwm', 'cce', 'wsg', 'tif', 'sam', 'tog', 'kau', 'mse', 'sja', 'tgw', 'hlb', 'fir', 'din', 'cdz', 'gyd', 'npl', 'fap', 'png', 'mas', 'vmz', 'sjm', 'bzf', 'grh', 'she', 'mnw', 'snl', 'pug', 'koo', 'fip', 'llp', 'kdt', 'rwa', 'clo', 'mjx', 'slc', 'av', 'nux', 'iwm', 'add', 'csw', 'duc', 'ttk', 'mck', 'azg', 'bkm', 'akf', 'toc', 'tjg', 'nps', 'biz', 'kub', 'hei', 'cuk', 'erg', 'taz', 'bqa', 'bua', 'ps', 'dov', 'svs', 'ike', 'sgw', 'jbo', 'krj', 'nli', 'pao', 'urk', 'kul', 'yes', 'wmt', 'bhb', 'bax', 'qug', 'gxx', 'ofu', 'zat', 'qvo', 'lem', 'kcq', 'kib', 'ddg', 'ndj', 'zyj', 'bwo', 'con', 'kma', 'tfr', 'rw', 'smw', 'msk', 'pua', 'aal', 'aul', 'aec', 'tvt', 'ekk', 'beo', 'sbk', 'cld', 'arn', 'mwi', 'hns', 'mef', 'mct', 'bhj', 'tes', 'swh', 'myl', 'dir', 'ael', 'xkk', 'nph', 'bab', 'ug', 'rui', 'ldo', 'vmy', 'dng', 'kqe', 'kyv', 'mbv', 'ank', 'zhi', 'qxp', 'kew', 'bfw', 'lmk', 'ttm', 'xac', 'ilu', 'mbi', 'tqo', 'wes', 'cbc', 'guc', 'chq', 'tba', 'ggg', 'skq', 'qws', 'orz', 'prq', 'nat', 'dry', 'dzg', 'akp', 'zad', 'tgo', 'dur', 'nix', 'wmd', 'pez', 'yuf', 'stq', 'agg', 'pnc', 'mlh', 'phl', 'kfq', 'kof', 'fao', 'mhx', 'qvs', 'fla', 'kzr', 'aom', 'swb', 'nlv', 'bjn', 'msn', 'kcj', 'cqd', 'lbw', 'zpx', 'yam', 'ayz', 'otq', 'mzp', 'kvl', 'odu', 'alj', 'jle', 'plr', 'lmo', 'kle', 'ogo', 'mah', 'nsy', 'ala', 'yif', 'smu', 'aed', 'lsh', 'lla', 'sin', 'kvg', 'nqy', 'lzh', 'cbu', 'akw', 'sea', 'lub', 'nez', 'njj', 'mbl', 'eus', 'qvj', 'ncb', 'aik', 'pca', 'niw', 'io', 'hvn', 'abn', 'lad', 'alu', 'ccj', 'sev', 'mrw', 'sw', 'tll', 'yka', 'erk', 'nfr', 'nxd', 'pes', 'kap', 'sju', 'mcw', 'ckh', 'bwq', 'sd', 'li', 'pwg', 'lro', 'mne', 'bgx', 'git', 'daa', 'pww', 'xtn', 'sag', 'nre', 'spp', 'dtm', 'une', 'adj', 'nst', 'srr', 'tuz', 'quz', 'pmi', 'kgy', 'wut', 'bhx', 'ml', 'byn', 'de', 'duu', 'jmd', 'zmb', 'kuj', 'xuo', 'wle', 'gbg', 'nbe', 'avn', 'ler', 'kef', 'mwp', 'gan', 'tl', 'gab', 'tpt', 'mku', 'plt', 'lht', 'sss', 'taj', 'cin', 'mfm', 'kwf', 'nbm', 'bgw', 'hlt', 'sig', 'jpa', 'az', 'bgr', 'ihp', 'bgd', 'bzd', 'klb', 'upv', 'khu', 'kfd', 'kyy', 'aca', 'nn', 'tem', 'chr', 'tsw', 'any', 'aeu', 'scu', 'mhy', 'drt', 'huu', 'otn', 'xdy', 'dhw', 'bez', 'hae', 'tcd', 'dyi', 'onj', 'emk', 'zpd', 'epi', 'dbj', 'kru', 'cmn', 'pwr', 'cab', 'bou', 'nuz', 'nyo', 'tru', 'tol', 'hoo', 'mpx', 'pny', 'uki', 'usi', 'krh', 'ami', 'amf', 'fub', 'nxq', 'poo', 'had', 'sos', 'tex', 'kia', 'gnu', 'aa', 'afz', 'uk', 'bnx', 'ipi', 'ver', 'yns', 'vaa', 'nca', 'hid', 'nda', 'wnp', 'mvf', 'pps', 'laj', 'udu', 'hig', 'nac', 'mkg', 'wa', 'ksh', 'tur', 'tds', 'ywa', 'scn', 'ng', 'bud', 'dgo', 'hol', 'alk', 'mcs', 'amx', 'brv', 'yim', 'wli', 'jit', 'nhx', 'mhi', 'nb', 'msb', 'iu', 'nlc', 'nnd', 'cku', 'kwi', 'kmh', 'jen', 'myw', 'tsx', 'ppo', 'kqs', 'ha', 'cvn', 'ppk', 'lbr', 'thr', 'ati', 'abk', 'jer', 'xkz', 'bov', 'knx', 'pln', 'ara', 'tga', 'cfm', 'kxf', 'tku', 'ntu', 'cbi', 'pus', 'crl', 'tng', 'zpl', 'lgl', 'lwo', 'sir', 'kmc', 'cch', 'nbv', 'kcr', 'saf', 'mgo', 'txn', 'onp', 'mrm', 'jmi', 'kmj', 'hac', 'pex', 'kwk', 'azz', 'hgm', 'hch', 'mqn', 'xmc', 'fuf', 'nfu', 'mbt', 'ocu', 'peb', 'acv', 'sou', 'tpl', 'tmc', 'njh', 'fab', 'ro', 'mmo', 'tji', 'ifm', 'bby', 'nwm', 'kvy', 'imo', 'nrg', 'kdp', 'bqw', 'xmm', 'zln', 'rav', 'ngw', 'eve', 'klz', 'lbq', 'goz', 'yet', 'bag', 'ale', 'tnp', 'gug', 'bwr', 'bre', 'wdd', 'kvf', 'bdw', 'mxs', 'byd', 'lwl', 'tao', 'sny', 'pmq', 'amo', 'hal', 'kmn', 'bfj', 'bli', 'kis', 'lt', 'zay', 'bgq', 'cya', 'mzq', 'rin', 'djr', 'zh', 'anw', 'kun', 'rag', 'bac', 'koe', 'urb', 'csn', 'yij', 'cos', 'dzo', 'yaz', 'uz', 'iii', 'brh', 'gwa', 'bcj', 'cja', 'zuy', 'bjx', 'bit', 'zul', 'irk', 'nzb', 'omw', 'pbs', 'lup', 'kmy', 'scs', 'kue', 'jic', 'dak', 'aoc', 'zia', 'dln', 'azj', 'tnl', 'yhd', 'khe', 'skj', 'mzm', 'ziw', 'lie', 'tgd', 'bn', 'fit', 'qwh', 'src', 'saa', 'epo', 'lat', 'kqb', 'gas', 'kfr', 'rmc', 'mjz', 'yay', 'nds', 'rnl', 'tos', 'pdn', 'abo', 'qve', 'krx', 'tlh', 'opa', 'dhn', 'ppm', 'soc', 'aby', 'jaq', 'vmh', 'kea', 'gcd', 'kil', 'lcp', 'gaq', 'cao', 'rbb', 'res', 'kaa', 'nqg', 'aia', 'tks', 'sgj', 'bug', 'vec', 'can', 'wbr', 'wry', 'ay', 'jiv', 'bge', 'bgz', 'mak', 'los', 'csg', 'tou', 'dia', 'kmw', 'guh', 'kkf', 'gbo', 'diw', 'bom', 'tte', 'oso', 'bdi', 'kab', 'kor', 'ore', 'lmp', 'pof', 'mri', 'agy', 'uhn', 'egl', 'kxz', 'rif', 'pko', 'phk', 'qvi', 'ab', 'fuq', 'mpt', 'ii', 'sby', 'mvv', 'puc', 'sug', 'aif', 'afe', 'gad', 'ruy', 'ker', 'qun', 'eze', 'axk', 'lew', 'lje', 'blz', 'mpn', 'pbg', 'na', 'zom', 'kxp', 'dgh', 'bus', 'xkl', 'ibl', 'sbr', 'yun', 'mmy', 'was', 'sck', 'igb', 'lgu', 'pkt', 'cbr', 'amm', 'zam', 'tla', 'gwn', 'bnn', 'sor', 'aii', 'swg', 'myb', 'tlx', 'shb', 'cra', 'kkj', 'srp', 'hmw', 'pkg', 'qul', 'gqa', 'loe', 'aac', 'mmp', 'lia', 'prn', 'msc', 'klu', 'bno', 'dih', 'wat', 'lai', 'nih', 'ged', 'sel', 'wom', 'tnt', 'bo', 'bkx', 'ktm', 'yaf', 'dks', 'nnj', 'tir', 'ssk', 'hgw', 'tqb', 'kw', 'dga', 'tkx', 'agi', 'zpq', 'nqo', 'esu', 'mea', 'agn', 'piy', 'bcs', 'cpx', 'nuk', 'mon', 'gru', 'myy', 'isd', 'zpn', 'oku', 'fwe', 'cyo', 'noz', 'omb', 'jms', 'puo', 'wtm', 'ghs', 'tsg', 'lmn', 'zac', 'int', 'lou', 'sps', 'pui', 'mam', 'tcs', 'tbz', 'bnj', 'gno', 'vmc', 'mcd', 'kql', 'loq', 'kon', 'bla', 'kwd', 'bco', 'ycn', 'wap', 'alw', 'zyg', 'uvh', 'ho', 'mnj', 'tld', 'niz', 'wic', 'suy', 'swj', 'pt', 'phq', 'gby', 'lvk', 'mdh', 'ny', 'dwr', 'ncl', 'vem', 'tam', 'mpp', 'bqp', 'orm', 'kl', 'kqn', 'rmt', 'tbp', 'pac', 'mlt', 'waw', 'msl', 'soi', 'pos', 'bob', 'kbp', 'sab', 'idu', 'chw', 'ur', 'bkc', 'dna', 'mny', 'srd', 'khg', 'nsn', 'obo', 'nak', 'mkc', 'coe', 'csy', 'nlu', 'qxr', 'luj', 'cof', 'orh', 'ava', 'gju', 'bww', 'sgi', 'hsn', 'daq', 'ywn', 'sao', 'nct', 'anv', 'xh', 'mcf', 'set', 'wdj', 'pha', 'sle', 'cbk', 'kje', 'qvn', 'adl', 'sce', 'lih', 'alf', 'tio', 'boo', 'eip', 'tg', 'lg', 'qya', 'mkd', 'vid', 'bi', 'bub', 'afh', 'sne', 'key', 'tls', 'ica', 'djo', 'ztp', 'bpw', 'yis', 'bje', 'did', 'ctg', 'ndu', 'dri', 'ztx', 'klx', 'ywq', 'jmc', 'bhu', 'swe', 'kny', 'rji', 'mix', 'pi', 'jqr', 'ade', 'ort', 'onn', 'bbq', 'lnd', 'gnk', 'hau', 'ton', 'sim', 'xtt', 'dty', 'syk', 'ymr', 'hay', 'sv', 'dny', 'nov', 'glr', 'ntk', 'tui', 'mrt', 'usp', 'trc', 'snn', 'sgb', 'swa', 'tuk', 'wms', 'sux', 'bns', 'ctz', 'tro', 'nob', 'ekg', 'quh', 'dge', 'yat', 'dub', 'pjt', 'mzn', 'xty', 'dad', 'iar', 'bqi', 'kvm', 'ldk', 'zpv', 'wal', 'bbr', 'xta', 'byc', 'hav', 'lom', 'tug', 'rki', 'tpn', 'kkh', 'raj', 'kwa', 'tav', 'thp', 'naw', 'nfl', 'gue', 'muh', 'abx', 'xtc', 'bpv', 'nys', 'ti', 'auc', 'blk', 'wlv', 'bor', 'hio', 'bky', 'mk', 'qxn', 'lmy', 'rgs', 'acz', 'cnh', 'it', 'zav', 'wad', 'sbx', 'dot', 'mey', 'asy', 'nlx', 'tbc', 'bys', 'mbb', 'sob', 'cyb', 'se', 'yor', 'daw', 'sjb', 'txq', 'mih', 'mvp', 'kfx', 'kkc', 'mti', 'ful', 'tsr', 'mjs', 'mxx', 'hwo', 'nfa', 'dob', 'wim', 'xtd', 'dah', 'nut', 'mdd', 'yap', 'psi', 'dox', 'tbk', 'kay', 'eo', 'clk', 'ciw', 'sng', 'cdm', 'cla', 'tbl', 'rah', 'ak', 'bof', 'aug', 'skb', 'mdy', 'bvd', 'bun', 'mut', 'bga', 'ghe', 'rau', 'ayp', 'scl', 'bmv', 'spy', 'kwl', 'mwa', 'kfg', 'tha', 'bmd', 'mco', 'sed', 'anu', 'lln', 'lbj', 'kpx', 'alp', 'tig', 'cme', 'mrh', 'sau', 'lns', 'khl', 'gyn', 'dzl', 'asi', 'wow', 'en', 'bik', 'mic', 'kgp', 'mlp', 'mi', 'her', 'duw', 'srm', 'hi', 'xri', 'etn', 'byr', 'fan', 'lv', 'tsc', 'bhq', 'hkk', 'byv', 'gam', 'nge', 'hoj', 'tdg', 'glw', 'kfy', 'lik', 'yuj', 'ndp', 'poc', 'lif', 'qxo', 'tiw', 'plw', 'muv', 'wbp', 'irn', 'suj', 'row', 'dgg', 'avd', 'iby', 'nhg', 'swp', 'mcb', 'yax', 'meq', 'buu', 'xkg', 'uri', 'bbc', 'bvh', 'ldm', 'geb', 'fuc', 'mpe', 'fi', 'ntj', 'cek', 'zea', 'ame', 'agq', 'fuv', 'tpu', 'kls', 'ztl', 'tgs', 'mtq', 'smh', 'muo', 'wwa', 'kus', 'knl', 'ybh', 'min', 'url', 'gdl', 'ndh', 'emn', 'mlq', 'uzs', 'bau', 'mpq', 'kpa', 'cjk', 'tew', 'gso', 'chd', 'nce', 'ja', 'tnm', 'bod', 'nbn', 'teo', 'ext', 'pce', 'iws', 'jig', 'bye', 'lvs', 'ksd', 'las', 'aln', 'lva', 'mee', 'mnv', 'clu', 'boh', 'eme', 'aro', 'dig', 'lis', 'rue', 'khz', 'kmp', 'ahl', 'eri', 'nou', 'tik', 'nsu', 'lki', 'mbd', 'wmw', 'oro', 'kty', 'bcc', 'or', 'suz', 'krn', 'lug', 'bmr', 'kpo', 'eto', 'hbs', 'iri', 'yby', 'mfo', 'hvv', 'cgc', 'ndx', 'sa', 'dtp', 'tna', 'dhd', 'noe', 'rng', 'lma', 'coj', 'bly', 'kvx', 'kwv', 'dai', 'yeu', 'nya', 'gnb', 'iti', 've', 'uvl', 'jaf', 'ano', 'mki', 'skt', 'thf', 'caq', 'ttb', 'kfu', 'fak', 'nor', 'xmv', 'bkg', 'sti', 'wed', 'mda', 'ncg', 'arl', 'prm', 'zpt', 'isl', 'nku', 'raw', 'suv', 'ady', 'quy', 'csa', 'cia', 'bcp', 'igl', 'awu', 'snv', 'rub', 'cu', 'yae', 'nkw', 'clt', 'alq', 'mlf', 'nco', 'tkq', 'mup', 'kne', 'mla', 'sjn', 'nmn', 'kvi', 'uiv', 'xal', 'npb', 'amn', 'gvf', 'an', 'kyc', 'zag', 'tnr', 'hue', 'tzo', 'slv', 'mmz', 'gng', 'guk', 'nuy', 'hut', 'win', 'haw', 'awn', 'mwn', 'shp', 'prt', 'alx', 'mtl', 'ccl', 'hdn', 'duh', 'mkw', 'ctu', 'mfe', 'bfq', 'lbb', 'aof', 'gwt', 'ngc', 'kfk', 'lcm', 'tsi', 'ln', 'sdp', 'ubu', 'kyo', 'zeh', 'pih', 'snw', 'vmp', 'kpb', 'doy', 'sku', 'jda', 'iry', 'cut', 'pck', 'abt', 'dis', 'nmm', 'afu', 'blc', 'mdn', 'und', 'nlg', 'lic', 'ter', 'lam', 'kol', 'zkr', 'agu', 'kyz', 'ndr', 'sge', 'hml', 'skd', 'nwb', 'my', 'lur', 'tsv', 'tlb', 'mh', 'ebr', 'cbs', 'yll', 'ltg', 'bxh', 'pss', 'ik', 'eka', 'kni', 'zkn', 'cas', 'bqh', 'sqi', 'hy', 'aot', 'okx', 'uss', 'fon', 'kfm', 'qui', 'tuq', 'bgp', 'vor', 'srq', 'cdn', 'mfj', 'gym', 'dem', 'gdf', 'hii', 'weo', 'eot', 'bfa', 'pbn', 'tdl', 'azt', 'kch', 'cnw', 'llu', 'muz', 'blq', 'kvq', 'mml', 'kdj', 'biy', 'bmj', 'taw', 'kzc', 'kaz', 'bcy', 'bip', 'rm', 'lec', 'ctl', 'mdr', 'gmm', 'kng', 'xpe', 'bah', 'bma', 'cgk', 'ssb', 'itr', 'tmr', 'cjs', 'vap', 'ilo', 'fse', 'lww', 'ku', 'kws', 'mar', 'oma', 'kfh', 'zng', 'crc', 'ekr', 'msi', 'bsh', 'zpz', 'tkl', 'pow', 'kgk', 'blh', 'urw', 'kaj', 'stv', 'kjg', 'kos', 'gpe', 'dug', 'ar', 'pmy', 'tvu', 'emg', 'ayi', 'xkt', 'too', 'maz', 'mcu', 'bas', 'kbm', 'mta', 'kno', 'kup', 'foi', 'bnv', 'dwu', 'twf', 'cub', 'kbq', 'hto', 'kbz', 'nd', 'dru', 'mph', 'cpc', 'kxn', 'xes', 'bcz', 'guj', 'snd', 'dip', 'bcn', 'kas', 'el', 'pqm', 'gar', 'hoa', 'arh', 'tyv', 'ayu', 'bef', 'kix', 'ban', 'ote', 'awa', 'cik', 'wbj', 'niq', 'ior', 'nka', 'kkz', 'mrd', 'mtk', 'prl', 'etu', 'njx', 'mfz', 'tdb', 'nit', 'pdo', 'nas', 'tis', 'cui', 'crx', 'kdx', 'xmz', 'jkr', 'hnd', 'mhc', 'jma', 'cbj', 'ahb', 'mxq', 'kdq', 'bci', 'gvr', 'tbo', 'wwo', 'kmz', 'nyf', 'ivb', 'alt', 'rai', 'aqg', 'mbz', 'kqy', 'esh', 'lbk', 'njo', 'mux', 'kpz', 'vah', 'sch', 'lkn', 'gbh', 'mwq', 'sya', 'mig', 'xsn', 'rap', 'tkr', 'mnx', 'ilk', 'npi', 'jad', 'kki', 'dts', 'crk', 'btg', 'mrj', 'gim', 'bjz', 'nhz', 'mg', 'fpe', 'gaz', 'rog', 'grc', 'ukw', 'mox', 'kyf', 'kui', 'dva', 'scp', 'avi', 'mhr', 'ruk', 'dgi', 'jni', 'sbn', 'bew', 'ktf', 'siy', 'ott', 'adh', 'lap', 'bcw', 'mye', 'zgh', 'sm', 'sbs', 'rmb', 'nru', 'ron', 'cuv', 'yyu', 'krw', 'kbv', 'zpe', 'lpa', 'fa', 'div', 'taq', 'keo', 'bqj', 'kek', 'ndi', 'yig', 'mdm', 'kbl', 'ksv', 'klt', 'hop', 'ggw', 'iqw', 'kic', 'gga', 'crn', 'pia', 'mzz', 'izr', 'lac', 'geg', 'krc', 'jaa', 'tzm', 'bya', 'nhw', 'ssy', 'giz', 'vls', 'nij', 'cwd', 'dar', 'gow', 'oc', 'ile', 'gog', 'nhi', 'ygr', 'ttw', 'das', 'xvi', 'gkp', 'ixl', 'nos', 'ewo', 'men', 'kir', 'kk', 'kgj', 'gcf', 'iko', 'dma', 'bvy', 'orv', 'xav', 'enb', 'agr', 'mrl', 'zpa', 'lut', 'myp', 'amr', 'prx', 'mt', 'ikx', 'sma', 'ate', 'sc', 'thy', 'wer', 'sde', 'age', 'oym', 'moi', 'knp', 'ppq', 'wem', 'bqg', 'kj', 'pbc', 'gah', 'zcd', 'lep', 'fmp', 'nyi', 'bux', 'var', 'ksg', 'bkq', 'alz', 'spu', 'bel', 'poh', 'hra', 'kgr', 'sxw', 'meo', 'bsl', 'wud', 'ikw', 'amu', 'crt', 'mfb', 'tpz', 'nbi', 'new', 'za', 'ctt', 'zmp', 'kjl', 'hrv', 'he', 'ksr', 'tbf', 'mmg', 'bps', 'lkh', 'mjc', 'mxn', 'kum', 'chj', 'sml', 'yml', 'nr', 'hoc', 'gdn', 'dnn', 'nvm', 'cv', 'xog', 'tau', 'mtr', 'nal', 'mlv', 'tof', 'nog', 'cmo', 'cok', 'zaf', 'nnw', 'br', 'hea', 'afo', 'dta', 'tii', 'okh', 'elm', 'kpr', 'moz', 'ast', 'mln', 'mue', 'pip', 'tye', 'tix', 'pkh', 'om', 'emp', 'gec', 'mev', 'mgm', 'nii', 'mww', 'cpy', 'viv', 'mzb', 'mwg', 'zca', 'rel', 'esi', 'arp', 'ki', 'tuy', 'kkd', 'kfs', 'isk', 'sba', 'khk', 'gup', 'tpj', 'soz', 'pah', 'dmo', 'mqh', 'gbr', 'yuz', 'brb', 'mym', 'bfg', 'haj', 'kak', 'mcr', 'hmj', 'ozm', 'wan', 'tto', 'che', 'grs', 'bdh', 'ntp', 'kbh', 'pdt', 'yev', 'gro', 'ego', 'nar', 'myh', 'bcg', 'pap', 'xks', 'lig', 'ake', 'ekl', 'ute', 'wib', 'hrm', 'wss', 'lrm', 'tke', 'hts', 'mgv', 'khs', 'hif', 'bbj', 'ldb', 'gba', 'wo', 'mau', 'rnd', 'kvv', 'wbb', 'hmo', 'kfo', 'skn', 'lnu', 'acn', 'aol', 'ats', 'heg', 'bgi', 'aoi', 'sac', 'kys', 'syl', 'tdj', 'mxu', 'bfy', 'ssp', 'oru', 'sxb', 'lsr', 'doz', 'yo', 'wyy', 'mxe', 'tcz', 'dax', 'sg', 'knv', 'dos', 'kdz', 'bdm', 'udl', 'nuo', 'ghl', 'atd', 'zai', 'liw', 'mhw', 'nmo', 'bhl', 'owi', 'ble', 'xum', 'lkt', 'mlg', 'ema', 'snm', 'liz', 'bja', 'nxg', 'rad', 'nho', 'chp', 'sq', 'wci', 'ont', 'tgy', 'tak', 'nba', 'mqy', 'ars', 'bzw', 'nci', 'khm', 'cay', 'drg', 'slp', 'cdf', 'vep', 'syb', 'btx', 'kjh', 'grx', 'ury', 'shy', 'chf', 'fuu', 'bfe', 'ibb', 'tca', 'aey', 'pfe', 'tkb', 'ify', 'afb', 'bpy', 'mn', 'noa', 'ysp', 'lhp', 'yuw', 'mgk', 'nso', 'bsc', 'nnu', 'osi', 'cbt', 'mwc', 'jku', 'kzf', 'kvn', 'smq', 'thv', 'guf', 'ums', 'rkm', 'wln', 'hmt', 'kcc', 'kzn', 'knj', 'stk', 'unr', 'ngi', 'sew', 'sgd', 'nid', 'heh', 'kmb', 'yrk', 'mil', 'orc', 'ksb', 'crj', 'nno', 'glh', 'pek', 'mtf', 'mtj', 'btt', 'dow', 'abu', 'kss', 'ite', 'agl', 'chx', 'bsq', 'tpm', 'byx', 'tee', 'est', 'ost', 'sto', 'bde', 'bhs', 'pmm', 'svc', 'gdg', 'kfb', 'bm', 'ese', 'djc', 'bhw', 'ibo', 'tdv', 'sci', 'kqm', 'akg', 'tbw', 'pci', 'mfh', 'cae', 'cfd', 'xkc', 'yle', 'kpw', 'pnb', 'agt', 'bnp', 'dho', 'umb', 'kad', 'vas', 'mlu', 'cwt', 'kku', 'clc', 'kn', 'yad', 'bda', 'chk', 'tmy', 'mbs', 'nko', 'mtd', 'rwr', 'fng', 'gnn', 'bjr', 'tdf', 'nxa', 'twm', 'xsu', 'puu', 'klw', 'kok', 'apn', 'tkt', 'ceg', 'sur', 'gvj', 'pmj', 'pyu', 'bcv', 'twx', 'lev', 'lok', 'zaa', 'nbp', 'sse', 'yom', 'gdu', 'pon', 'bze', 'bhh', 'yum', 'pol', 'ldq', 'wme', 'sld', 'yog', 'arq', 'djn', 'mnm', 'zik', 'gdb', 'cnb', 'lo', 'knt', 'zts', 'mav', 'usa', 'nnh', 'bbt', 'id', 'plc', 'gvn', 'how', 'nyj', 'hur', 'nuq', 'cly', 'ldg', 'bkw', 'no', 'nyw', 'wji', 'lzz', 'hmg', 'ssi', 'tei', 'hun', 'eza', 'ora', 'mjl', 'kzs', 'nev', 'lot', 'mte', 'mfg', 'cua', 'gel', 'nlj', 'cnq', 'gid', 'khb', 'atu', 'sho', 'bha', 'agc', 'bmq', 'chv', 'tic', 'nnb', 'dsq', 'kyu', 'ded'}\n"
     ]
    }
   ],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?\n",
    "\n",
    "tag_language = set(tag_language)\n",
    "print(tag_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    if '/' not in full_name: # check if the name is in the format 'author/model_name'\n",
    "        return full_name\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)')\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model_idx):\n",
    "\n",
    "\tmodel = models_df.loc[model_idx]\n",
    "\tmodel_tags = models_df.loc[model_idx]['tags']\n",
    "\tmodel_card_data = None\n",
    "\ttry:\n",
    "\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\texcept AttributeError:\n",
    "\t\tprint('No card data available for this model')\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model['id'])\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No architecture data available for this model')\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # TODO: if base_model exists, look for 'author' of the base model\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tbase_model = model_card_data['base_model']\n",
    "\t\t\tbase_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t\t\tmodel_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model['author']]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tprint('No base model data available for this model')\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tprint('No emission data available for this model')\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_json(model_idx):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    model_attributes = extract_model_attributes(model_idx)\n",
    "    \n",
    "    with open(os.path.join(result_path, 'test_models.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_attributes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_json(models_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    for model_idx in range(models_df.shape[0]):\n",
    "        output.append(extract_model_attributes(model_idx))\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.LargeLanguageModel.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_json(models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes_optimized(model):\n",
    "\t\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\t\n",
    "\tmodel_attributes = dict()\n",
    "\t# model_to_base_model = None\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = extract_name(model_card_data['base_model'])\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # in a postprocessing phase, if base_model exists (in 'architecture'), look for 'developers' of the base model\n",
    "\t# try:\n",
    "\t# \tif model_card_data is not None:\n",
    "\t# \t\tif 'base_model' in model_card_data:\n",
    "\t# \t\t\tbase_model = extract_name(model_card_data['base_model'])\n",
    "\t# \t\t\tmodel_to_base_model = {model_attributes['name']: base_model}\n",
    "\t# \t\t\t# base_model_data = pd.DataFrame(api.list_models(model_name=base_model, full=True))\n",
    "\t# \t\t\t# model_attributes['modelCreator'] = base_model_data.loc[0]['author']\n",
    "\t# except KeyError:\n",
    "\t# \tpass\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] # TODO: change type into list(str) in our model\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\t# return model_attributes, model_to_base_model\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(full=True, cardData=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 models processed, 126.49671840667725 seconds elapsed\n",
      "20000 models processed, 234.77703642845154 seconds elapsed\n",
      "30000 models processed, 367.68634724617004 seconds elapsed\n",
      "40000 models processed, 516.6834456920624 seconds elapsed\n",
      "50000 models processed, 624.806972026825 seconds elapsed\n",
      "60000 models processed, 732.1757125854492 seconds elapsed\n",
      "70000 models processed, 839.5752060413361 seconds elapsed\n",
      "80000 models processed, 943.5083427429199 seconds elapsed\n",
      "90000 models processed, 1042.7329745292664 seconds elapsed\n",
      "100000 models processed, 1147.997989654541 seconds elapsed\n",
      "110000 models processed, 1285.1926712989807 seconds elapsed\n",
      "120000 models processed, 1432.8246834278107 seconds elapsed\n",
      "130000 models processed, 1566.2858626842499 seconds elapsed\n",
      "140000 models processed, 1682.8268156051636 seconds elapsed\n",
      "150000 models processed, 1778.1431176662445 seconds elapsed\n",
      "160000 models processed, 1878.1602005958557 seconds elapsed\n",
      "170000 models processed, 2072.9270672798157 seconds elapsed\n",
      "180000 models processed, 2179.3076722621918 seconds elapsed\n",
      "190000 models processed, 2303.749891281128 seconds elapsed\n",
      "200000 models processed, 2454.4678523540497 seconds elapsed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# model_attributes, model_to_base_model = extract_model_attributes_optimized(model)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     model_attributes \u001b[38;5;241m=\u001b[39m extract_model_attributes_optimized(model)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43madd_to_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# if model_to_base_model is not None:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#     add_to_json_file(model_to_base_model, file_path_map)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36madd_to_json_file\u001b[0;34m(data, file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mSEEK_END)\n\u001b[1;32m      6\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, os\u001b[38;5;241m.\u001b[39mSEEK_SET)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'models_data.json')\n",
    "# file_path_map = os.path.join(result_path, 'models_to_base_models.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for model in models:\n",
    "    # model_attributes, model_to_base_model = extract_model_attributes_optimized(model)\n",
    "    model_attributes = extract_model_attributes_optimized(model)\n",
    "    add_to_json_file(model_attributes, file_path)\n",
    "    # if model_to_base_model is not None:\n",
    "    #     add_to_json_file(model_to_base_model, file_path_map)\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import saved data as pandas dataframe\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path_reloaded = os.path.join(result_path, 'models_data_200k_no_modelCreator.json')\n",
    "models_df_reloaded = pd.read_json(file_path_reloaded)\n",
    "# models_df_reloaded = pd.read_json(open(file_path_reloaded, \"r\", encoding=\"utf8\"), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>numberOfParameters</th>\n",
       "      <th>quantization</th>\n",
       "      <th>architecture</th>\n",
       "      <th>languages</th>\n",
       "      <th>modelCreator</th>\n",
       "      <th>licenseToUse</th>\n",
       "      <th>libraryFramework</th>\n",
       "      <th>contextLength</th>\n",
       "      <th>developers</th>\n",
       "      <th>openSource</th>\n",
       "      <th>uri</th>\n",
       "      <th>fineTuned</th>\n",
       "      <th>carbonEmission [CO2eq tons]</th>\n",
       "      <th>tokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[albert]</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.11942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, safeten...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[albert]</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.11942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-large-v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[albert]</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.11942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[albert]</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.11942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-xlarge-v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[en]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, tf, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[albert]</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.11942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200683</th>\n",
       "      <td>kogpt_chitchat_data_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sejongmate]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200684</th>\n",
       "      <td>bloom-3b-squad-qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[thirtyninetythree]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200685</th>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[micro-me]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200686</th>\n",
       "      <td>bart-large-xsum-samsum-finetuned-multi_news_en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Evangeliaa]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200687</th>\n",
       "      <td>Lucas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mit</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CEvs92]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200688 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name  version  \\\n",
       "0                                       albert-base-v1      NaN   \n",
       "1                                       albert-base-v2      NaN   \n",
       "2                                      albert-large-v1      NaN   \n",
       "3                                      albert-large-v2      NaN   \n",
       "4                                     albert-xlarge-v1      NaN   \n",
       "...                                                ...      ...   \n",
       "200683                       kogpt_chitchat_data_input      NaN   \n",
       "200684                               bloom-3b-squad-qa      NaN   \n",
       "200685                                            test      NaN   \n",
       "200686  bart-large-xsum-samsum-finetuned-multi_news_en      NaN   \n",
       "200687                                           Lucas      NaN   \n",
       "\n",
       "        numberOfParameters quantization architecture languages  modelCreator  \\\n",
       "0                      NaN         None         None      [en]           NaN   \n",
       "1                      NaN         None         None      [en]           NaN   \n",
       "2                      NaN         None         None      [en]           NaN   \n",
       "3                      NaN         None         None      [en]           NaN   \n",
       "4                      NaN         None         None      [en]           NaN   \n",
       "...                    ...          ...          ...       ...           ...   \n",
       "200683                 NaN         None         None        []           NaN   \n",
       "200684                 NaN         None         None        []           NaN   \n",
       "200685                 NaN         None         None        []           NaN   \n",
       "200686                 NaN         None         None        []           NaN   \n",
       "200687                 NaN         None         None        []           NaN   \n",
       "\n",
       "       licenseToUse                                   libraryFramework  \\\n",
       "0        apache-2.0           [transformers, pytorch, tf, safetensors]   \n",
       "1        apache-2.0  [transformers, pytorch, tf, jax, rust, safeten...   \n",
       "2        apache-2.0                        [transformers, pytorch, tf]   \n",
       "3        apache-2.0           [transformers, pytorch, tf, safetensors]   \n",
       "4        apache-2.0           [transformers, pytorch, tf, safetensors]   \n",
       "...             ...                                                ...   \n",
       "200683         None                                                 []   \n",
       "200684         None                                                 []   \n",
       "200685         None                                                 []   \n",
       "200686         None                                                 []   \n",
       "200687          mit                                                 []   \n",
       "\n",
       "        contextLength           developers  openSource         uri  fineTuned  \\\n",
       "0                 NaN             [albert]        True  1909.11942        NaN   \n",
       "1                 NaN             [albert]        True  1909.11942        NaN   \n",
       "2                 NaN             [albert]        True  1909.11942        NaN   \n",
       "3                 NaN             [albert]        True  1909.11942        NaN   \n",
       "4                 NaN             [albert]        True  1909.11942        NaN   \n",
       "...               ...                  ...         ...         ...        ...   \n",
       "200683            NaN         [sejongmate]        True        None        NaN   \n",
       "200684            NaN  [thirtyninetythree]        True        None        NaN   \n",
       "200685            NaN           [micro-me]        True        None        NaN   \n",
       "200686            NaN         [Evangeliaa]        True        None        NaN   \n",
       "200687            NaN             [CEvs92]        True        None        NaN   \n",
       "\n",
       "       carbonEmission [CO2eq tons]  tokenizer  \n",
       "0                             None        NaN  \n",
       "1                             None        NaN  \n",
       "2                             None        NaN  \n",
       "3                             None        NaN  \n",
       "4                             None        NaN  \n",
       "...                            ...        ...  \n",
       "200683                        None        NaN  \n",
       "200684                        None        NaN  \n",
       "200685                        None        NaN  \n",
       "200686                        None        NaN  \n",
       "200687                        None        NaN  \n",
       "\n",
       "[200688 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>numberOfParameters</th>\n",
       "      <th>quantization</th>\n",
       "      <th>architecture</th>\n",
       "      <th>languages</th>\n",
       "      <th>modelCreator</th>\n",
       "      <th>licenseToUse</th>\n",
       "      <th>libraryFramework</th>\n",
       "      <th>contextLength</th>\n",
       "      <th>developers</th>\n",
       "      <th>openSource</th>\n",
       "      <th>uri</th>\n",
       "      <th>fineTuned</th>\n",
       "      <th>carbonEmission [CO2eq tons]</th>\n",
       "      <th>tokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>it5-summarization-fanpage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>it5-base</td>\n",
       "      <td>[it]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>it5-summarization-ilpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>it5-base</td>\n",
       "      <td>[it]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, tensorboard, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>it5-summarization-mlsum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>it5-base</td>\n",
       "      <td>[it]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>mbart-summarization-fanpage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>mbart-large-cc25</td>\n",
       "      <td>[it]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>mbart-summarization-ilpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>mbart-large-cc25</td>\n",
       "      <td>[it]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200386</th>\n",
       "      <td>thesis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[peft, tensorboard, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aleph-null]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200528</th>\n",
       "      <td>fruits-and-vegetables-detector-36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>resnet-50</td>\n",
       "      <td>[en]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[jazzmacedo]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200540</th>\n",
       "      <td>hubert-large-speech-emotion-recognition-russia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>hubert-large-ls960-ft</td>\n",
       "      <td>[ru]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[xbgoose]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200577</th>\n",
       "      <td>ruBert-base-russian-emotion-detection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>ruBert-base</td>\n",
       "      <td>[ru]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[MaxKazak]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200638</th>\n",
       "      <td>token_classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cc-by-sa-4.0</td>\n",
       "      <td>[transformers, pytorch, tensorboard, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Seogmin]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1645 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  version  \\\n",
       "173                             it5-summarization-fanpage      NaN   \n",
       "174                              it5-summarization-ilpost      NaN   \n",
       "175                               it5-summarization-mlsum      NaN   \n",
       "176                           mbart-summarization-fanpage      NaN   \n",
       "177                            mbart-summarization-ilpost      NaN   \n",
       "...                                                   ...      ...   \n",
       "200386                                             thesis      NaN   \n",
       "200528                  fruits-and-vegetables-detector-36      NaN   \n",
       "200540  hubert-large-speech-emotion-recognition-russia...      NaN   \n",
       "200577              ruBert-base-russian-emotion-detection      NaN   \n",
       "200638                               token_classification      NaN   \n",
       "\n",
       "        numberOfParameters quantization              architecture languages  \\\n",
       "173                    NaN         None                  it5-base      [it]   \n",
       "174                    NaN         None                  it5-base      [it]   \n",
       "175                    NaN         None                  it5-base      [it]   \n",
       "176                    NaN         None          mbart-large-cc25      [it]   \n",
       "177                    NaN         None          mbart-large-cc25      [it]   \n",
       "...                    ...          ...                       ...       ...   \n",
       "200386                 NaN         None  Mistral-7B-Instruct-v0.2        []   \n",
       "200528                 NaN         None                 resnet-50      [en]   \n",
       "200540                 NaN         None     hubert-large-ls960-ft      [ru]   \n",
       "200577                 NaN         None               ruBert-base      [ru]   \n",
       "200638                 NaN         None                 bert-base        []   \n",
       "\n",
       "        modelCreator  licenseToUse  \\\n",
       "173              NaN          None   \n",
       "174              NaN          None   \n",
       "175              NaN          None   \n",
       "176              NaN          None   \n",
       "177              NaN          None   \n",
       "...              ...           ...   \n",
       "200386           NaN    apache-2.0   \n",
       "200528           NaN    apache-2.0   \n",
       "200540           NaN    apache-2.0   \n",
       "200577           NaN    apache-2.0   \n",
       "200638           NaN  cc-by-sa-4.0   \n",
       "\n",
       "                                         libraryFramework  contextLength  \\\n",
       "173                  [transformers, pytorch, safetensors]            NaN   \n",
       "174     [transformers, pytorch, tensorboard, safetensors]            NaN   \n",
       "175                  [transformers, pytorch, safetensors]            NaN   \n",
       "176                  [transformers, pytorch, safetensors]            NaN   \n",
       "177                  [transformers, pytorch, safetensors]            NaN   \n",
       "...                                                   ...            ...   \n",
       "200386                   [peft, tensorboard, safetensors]            NaN   \n",
       "200528               [transformers, pytorch, safetensors]            NaN   \n",
       "200540               [transformers, pytorch, safetensors]            NaN   \n",
       "200577               [transformers, pytorch, safetensors]            NaN   \n",
       "200638  [transformers, pytorch, tensorboard, safetensors]            NaN   \n",
       "\n",
       "          developers  openSource   uri  fineTuned carbonEmission [CO2eq tons]  \\\n",
       "173        [ARTeLab]        True  None        1.0                        None   \n",
       "174        [ARTeLab]        True  None        1.0                        None   \n",
       "175        [ARTeLab]        True  None        1.0                        None   \n",
       "176        [ARTeLab]        True  None        1.0                        None   \n",
       "177        [ARTeLab]        True  None        1.0                        None   \n",
       "...              ...         ...   ...        ...                         ...   \n",
       "200386  [aleph-null]        True  None        1.0                        None   \n",
       "200528  [jazzmacedo]        True  None        1.0                        None   \n",
       "200540     [xbgoose]        True  None        1.0                        None   \n",
       "200577    [MaxKazak]        True  None        1.0                        None   \n",
       "200638     [Seogmin]        True  None        1.0                        None   \n",
       "\n",
       "        tokenizer  \n",
       "173           NaN  \n",
       "174           NaN  \n",
       "175           NaN  \n",
       "176           NaN  \n",
       "177           NaN  \n",
       "...           ...  \n",
       "200386        NaN  \n",
       "200528        NaN  \n",
       "200540        NaN  \n",
       "200577        NaN  \n",
       "200638        NaN  \n",
       "\n",
       "[1645 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df_reloaded[models_df_reloaded['architecture'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in models_df_reloaded.itertuples():\n",
    "    if not pd.isna(row.architecture) and type(row.architecture) == str:\n",
    "        developers = models_df_reloaded[models_df_reloaded['name'] == row.architecture].developers.values\n",
    "        modelCreators = []\n",
    "        for developer in developers:\n",
    "            modelCreators.append(developer[0])\n",
    "        models_df_reloaded.at[row.Index, 'modelCreator'] = modelCreators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>numberOfParameters</th>\n",
       "      <th>quantization</th>\n",
       "      <th>architecture</th>\n",
       "      <th>languages</th>\n",
       "      <th>modelCreator</th>\n",
       "      <th>licenseToUse</th>\n",
       "      <th>libraryFramework</th>\n",
       "      <th>contextLength</th>\n",
       "      <th>developers</th>\n",
       "      <th>openSource</th>\n",
       "      <th>uri</th>\n",
       "      <th>fineTuned</th>\n",
       "      <th>carbonEmission [CO2eq tons]</th>\n",
       "      <th>tokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>it5-summarization-fanpage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>it5-base</td>\n",
       "      <td>[it]</td>\n",
       "      <td>[gsarti]</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>it5-summarization-ilpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>it5-base</td>\n",
       "      <td>[it]</td>\n",
       "      <td>[gsarti]</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, tensorboard, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>it5-summarization-mlsum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>it5-base</td>\n",
       "      <td>[it]</td>\n",
       "      <td>[gsarti]</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>mbart-summarization-fanpage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>mbart-large-cc25</td>\n",
       "      <td>[it]</td>\n",
       "      <td>[facebook]</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>mbart-summarization-ilpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>mbart-large-cc25</td>\n",
       "      <td>[it]</td>\n",
       "      <td>[facebook]</td>\n",
       "      <td>None</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ARTeLab]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200386</th>\n",
       "      <td>thesis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[peft, tensorboard, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aleph-null]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200528</th>\n",
       "      <td>fruits-and-vegetables-detector-36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>resnet-50</td>\n",
       "      <td>[en]</td>\n",
       "      <td>[microsoft, OWG, VishwanathanR, sallyanndeluci...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[jazzmacedo]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200540</th>\n",
       "      <td>hubert-large-speech-emotion-recognition-russia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>hubert-large-ls960-ft</td>\n",
       "      <td>[ru]</td>\n",
       "      <td>[facebook, optimum]</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[xbgoose]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200577</th>\n",
       "      <td>ruBert-base-russian-emotion-detection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>ruBert-base</td>\n",
       "      <td>[ru]</td>\n",
       "      <td>[ai-forever, YuryCHep]</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[transformers, pytorch, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[MaxKazak]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200638</th>\n",
       "      <td>token_classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>[]</td>\n",
       "      <td>[klue, Seonwhee-Genome, AsterGoldman, Sheza, y...</td>\n",
       "      <td>cc-by-sa-4.0</td>\n",
       "      <td>[transformers, pytorch, tensorboard, safetensors]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Seogmin]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1645 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  version  \\\n",
       "173                             it5-summarization-fanpage      NaN   \n",
       "174                              it5-summarization-ilpost      NaN   \n",
       "175                               it5-summarization-mlsum      NaN   \n",
       "176                           mbart-summarization-fanpage      NaN   \n",
       "177                            mbart-summarization-ilpost      NaN   \n",
       "...                                                   ...      ...   \n",
       "200386                                             thesis      NaN   \n",
       "200528                  fruits-and-vegetables-detector-36      NaN   \n",
       "200540  hubert-large-speech-emotion-recognition-russia...      NaN   \n",
       "200577              ruBert-base-russian-emotion-detection      NaN   \n",
       "200638                               token_classification      NaN   \n",
       "\n",
       "        numberOfParameters quantization              architecture languages  \\\n",
       "173                    NaN         None                  it5-base      [it]   \n",
       "174                    NaN         None                  it5-base      [it]   \n",
       "175                    NaN         None                  it5-base      [it]   \n",
       "176                    NaN         None          mbart-large-cc25      [it]   \n",
       "177                    NaN         None          mbart-large-cc25      [it]   \n",
       "...                    ...          ...                       ...       ...   \n",
       "200386                 NaN         None  Mistral-7B-Instruct-v0.2        []   \n",
       "200528                 NaN         None                 resnet-50      [en]   \n",
       "200540                 NaN         None     hubert-large-ls960-ft      [ru]   \n",
       "200577                 NaN         None               ruBert-base      [ru]   \n",
       "200638                 NaN         None                 bert-base        []   \n",
       "\n",
       "                                             modelCreator  licenseToUse  \\\n",
       "173                                              [gsarti]          None   \n",
       "174                                              [gsarti]          None   \n",
       "175                                              [gsarti]          None   \n",
       "176                                            [facebook]          None   \n",
       "177                                            [facebook]          None   \n",
       "...                                                   ...           ...   \n",
       "200386                                                 []    apache-2.0   \n",
       "200528  [microsoft, OWG, VishwanathanR, sallyanndeluci...    apache-2.0   \n",
       "200540                                [facebook, optimum]    apache-2.0   \n",
       "200577                             [ai-forever, YuryCHep]    apache-2.0   \n",
       "200638  [klue, Seonwhee-Genome, AsterGoldman, Sheza, y...  cc-by-sa-4.0   \n",
       "\n",
       "                                         libraryFramework  contextLength  \\\n",
       "173                  [transformers, pytorch, safetensors]            NaN   \n",
       "174     [transformers, pytorch, tensorboard, safetensors]            NaN   \n",
       "175                  [transformers, pytorch, safetensors]            NaN   \n",
       "176                  [transformers, pytorch, safetensors]            NaN   \n",
       "177                  [transformers, pytorch, safetensors]            NaN   \n",
       "...                                                   ...            ...   \n",
       "200386                   [peft, tensorboard, safetensors]            NaN   \n",
       "200528               [transformers, pytorch, safetensors]            NaN   \n",
       "200540               [transformers, pytorch, safetensors]            NaN   \n",
       "200577               [transformers, pytorch, safetensors]            NaN   \n",
       "200638  [transformers, pytorch, tensorboard, safetensors]            NaN   \n",
       "\n",
       "          developers  openSource   uri  fineTuned carbonEmission [CO2eq tons]  \\\n",
       "173        [ARTeLab]        True  None        1.0                        None   \n",
       "174        [ARTeLab]        True  None        1.0                        None   \n",
       "175        [ARTeLab]        True  None        1.0                        None   \n",
       "176        [ARTeLab]        True  None        1.0                        None   \n",
       "177        [ARTeLab]        True  None        1.0                        None   \n",
       "...              ...         ...   ...        ...                         ...   \n",
       "200386  [aleph-null]        True  None        1.0                        None   \n",
       "200528  [jazzmacedo]        True  None        1.0                        None   \n",
       "200540     [xbgoose]        True  None        1.0                        None   \n",
       "200577    [MaxKazak]        True  None        1.0                        None   \n",
       "200638     [Seogmin]        True  None        1.0                        None   \n",
       "\n",
       "        tokenizer  \n",
       "173           NaN  \n",
       "174           NaN  \n",
       "175           NaN  \n",
       "176           NaN  \n",
       "177           NaN  \n",
       "...           ...  \n",
       "200386        NaN  \n",
       "200528        NaN  \n",
       "200540        NaN  \n",
       "200577        NaN  \n",
       "200638        NaN  \n",
       "\n",
       "[1645 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df_reloaded[models_df_reloaded['architecture'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save postprocessed dataframe to json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def models_statistics(file_name):\n",
    "\n",
    "\tname_count = 0\n",
    "\tversion_count = 0\n",
    "\tnumber_of_parameters_count = 0\n",
    "\tquantization_count = 0\n",
    "\tarchitecture_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tmodel_creator_count = 0\n",
    "\tlicense_count = 0\n",
    "\tlibrary_count = 0\n",
    "\tcontext_length_count = 0\n",
    "\tdevelopers_count = 0\n",
    "\topen_source_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuned_count = 0\n",
    "\tcarbon_emission_count = 0\n",
    "\ttokenizer_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'HF entries', 'hf_extracted_json')\n",
    "\n",
    "\tmodels_json = open(os.path.join(result_path, file_name))\n",
    "\tmodels_data_json = json.load(models_json)\n",
    "\n",
    "\tmodels_df = pd.DataFrame(models_data_json) \n",
    "\n",
    "\t# TODO: add more attributes (?)\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['version'] is not None:\n",
    "\t\t\tversion_count += 1\n",
    "\t\tif item['numberOfParameters'] is not None:\n",
    "\t\t\tnumber_of_parameters_count += 1\n",
    "\t\tif item['quantization'] is not None:\n",
    "\t\t\tquantization_count += 1\n",
    "\t\tif item['architecture'] is not None:\n",
    "\t\t\tarchitecture_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\n",
    "\t\tif item['modelCreator'] is not None:\n",
    "\t\t\tmodel_creator_count += 1\n",
    "\t\tif item['licenseToUse'] is not None:\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['libraryFramework']) > 0:\n",
    "\t\t\tlibrary_count += 1\n",
    "\t\tif item['contextLength'] is not None:\n",
    "\t\t\tcontext_length_count += 1\n",
    "\t\tif len(item['developers']) > 0:\n",
    "\t\t\tdevelopers_count += 1\n",
    "\t\tif item['openSource'] is not None:\n",
    "\t\t\topen_source_count += 1\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuned'] is not None:\n",
    "\t\t\tfinetuned_count += 1\n",
    "\t\tif item['carbonEmission [CO2eq tons]'] is not None:\n",
    "\t\t\tcarbon_emission_count += 1\n",
    "\t\tif item['tokenizer'] is not None:\n",
    "\t\t\ttokenizer_count += 1\n",
    "\t\n",
    "\ttotal_models = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed models: {total_models}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Version: {version_count} ({(version_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Number of Parameters: {number_of_parameters_count} ({(number_of_parameters_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Quantization: {quantization_count} ({(quantization_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Architecture: {architecture_count} ({(architecture_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Model creator: {model_creator_count} ({(model_creator_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Library: {library_count} ({(library_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Context Length: {context_length_count} ({(context_length_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Developers: {developers_count} ({(developers_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Open Source: {open_source_count} ({(open_source_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuned: {finetuned_count} ({(finetuned_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Carbon emission: {carbon_emission_count} ({(carbon_emission_count / total_models) * 100:.2f}%)')\n",
    "\tprint(f'    Tokenizer: {tokenizer_count} ({(tokenizer_count / total_models) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tllm_attributes = models_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(models_data_json):\n",
    "\t\tmodel_name = item['name']\n",
    "\t\tfor attr in llm_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': model_name, 'entity name': 'LLM', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed models: 64377\n",
      "    Name: 64377 (100.00%)\n",
      "    Version: 0 (0.00%)\n",
      "    Number of Parameters: 0 (0.00%)\n",
      "    Quantization: 3 (0.00%)\n",
      "    Architecture: 391 (0.61%)\n",
      "    Languages: 15426 (23.96%)\n",
      "    Model creator: 386 (0.60%)\n",
      "    License to use: 19956 (31.00%)\n",
      "    Library: 46662 (72.48%)\n",
      "    Context Length: 0 (0.00%)\n",
      "    Developers: 64377 (100.00%)\n",
      "    Open Source: 64377 (100.00%)\n",
      "    URI: 5247 (8.15%)\n",
      "    Fine-tuned: 391 (0.61%)\n",
      "    Carbon emission: 489 (0.76%)\n",
      "    Tokenizer: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "models_statistics('ChatIMPACT.LargeLanguageModel.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>version</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>numberOfParameters</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>quantization</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>modelCreator</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>libraryFramework</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>contextLength</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>developers</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>openSource</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>fineTuned</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>carbonEmission [CO2eq tons]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>LLM</td>\n",
       "      <td>tokenizer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>version</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>numberOfParameters</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>quantization</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>modelCreator</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>libraryFramework</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>contextLength</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>developers</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>openSource</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>LLM</td>\n",
       "      <td>fineTuned</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id entity name               attribute name available API  \\\n",
       "0   albert-base-v1         LLM                         name          True   \n",
       "1   albert-base-v1         LLM                      version         False   \n",
       "2   albert-base-v1         LLM           numberOfParameters         False   \n",
       "3   albert-base-v1         LLM                 quantization         False   \n",
       "4   albert-base-v1         LLM                 architecture         False   \n",
       "5   albert-base-v1         LLM                    languages          True   \n",
       "6   albert-base-v1         LLM                 modelCreator         False   \n",
       "7   albert-base-v1         LLM                 licenseToUse          True   \n",
       "8   albert-base-v1         LLM             libraryFramework          True   \n",
       "9   albert-base-v1         LLM                contextLength         False   \n",
       "10  albert-base-v1         LLM                   developers          True   \n",
       "11  albert-base-v1         LLM                   openSource          True   \n",
       "12  albert-base-v1         LLM                          uri          True   \n",
       "13  albert-base-v1         LLM                    fineTuned         False   \n",
       "14  albert-base-v1         LLM  carbonEmission [CO2eq tons]         False   \n",
       "15  albert-base-v1         LLM                    tokenizer         False   \n",
       "16  albert-base-v2         LLM                         name          True   \n",
       "17  albert-base-v2         LLM                      version         False   \n",
       "18  albert-base-v2         LLM           numberOfParameters         False   \n",
       "19  albert-base-v2         LLM                 quantization         False   \n",
       "20  albert-base-v2         LLM                 architecture         False   \n",
       "21  albert-base-v2         LLM                    languages          True   \n",
       "22  albert-base-v2         LLM                 modelCreator         False   \n",
       "23  albert-base-v2         LLM                 licenseToUse          True   \n",
       "24  albert-base-v2         LLM             libraryFramework          True   \n",
       "25  albert-base-v2         LLM                contextLength         False   \n",
       "26  albert-base-v2         LLM                   developers          True   \n",
       "27  albert-base-v2         LLM                   openSource          True   \n",
       "28  albert-base-v2         LLM                          uri          True   \n",
       "29  albert-base-v2         LLM                    fineTuned         False   \n",
       "\n",
       "   available scraping  \n",
       "0               False  \n",
       "1               False  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  \n",
       "5               False  \n",
       "6               False  \n",
       "7               False  \n",
       "8               False  \n",
       "9               False  \n",
       "10              False  \n",
       "11              False  \n",
       "12              False  \n",
       "13              False  \n",
       "14              False  \n",
       "15              False  \n",
       "16              False  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  \n",
       "20              False  \n",
       "21              False  \n",
       "22              False  \n",
       "23              False  \n",
       "24              False  \n",
       "25              False  \n",
       "26              False  \n",
       "27              False  \n",
       "28              False  \n",
       "29              False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>sha</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>paperswithcode_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>card_data</th>\n",
       "      <th>siblings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amirveyseh/acronym_identification</td>\n",
       "      <td>amirveyseh</td>\n",
       "      <td>15ef643450d589d5883e289ffadeb03563e80a9e</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:39:57+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>19</td>\n",
       "      <td>acronym-identification</td>\n",
       "      <td>[task_categories:token-classification, annotat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ade-benchmark-corpus/ade_corpus_v2</td>\n",
       "      <td>ade-benchmark-corpus</td>\n",
       "      <td>4ba01c71687dd7c996597042449448ea312126cf</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:42:58+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_cat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLNLP/adversarial_qa</td>\n",
       "      <td>UCLNLP</td>\n",
       "      <td>c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 14:20:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>139</td>\n",
       "      <td>32</td>\n",
       "      <td>adversarialqa</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yale-LILY/aeslc</td>\n",
       "      <td>Yale-LILY</td>\n",
       "      <td>2305f2e63b68056f9b9037a3805c8c196e0d5581</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:49:13+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>124</td>\n",
       "      <td>12</td>\n",
       "      <td>aeslc</td>\n",
       "      <td>[task_categories:summarization, annotations_cr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nwu-ctext/afrikaans_ner_corpus</td>\n",
       "      <td>nwu-ctext</td>\n",
       "      <td>445834a997dce8b40e1d108638064381de80c497</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:51:47+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:token-classification, task_id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fancyzhx/ag_news</td>\n",
       "      <td>fancyzhx</td>\n",
       "      <td>eb185aade064a813bc0b7f42de02595523103ca4</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 12:02:37+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7297</td>\n",
       "      <td>123</td>\n",
       "      <td>ag-news</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>allenai</td>\n",
       "      <td>210d026faf9955653af8916fad021475a3f00453</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2023-12-21 15:09:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>785162</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:question-answering, task_ids:...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google/air_dialogue</td>\n",
       "      <td>google</td>\n",
       "      <td>dbdbe7bcef8d344bc3c68a05600f3d95917d6898</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-03-07 15:22:15+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-generation, task_categor...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>komari6/ajgt_twitter_ar</td>\n",
       "      <td>komari6</td>\n",
       "      <td>af3f2fa5462ac461b696cb300d66e07ad366057f</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:58:01+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>legacy-datasets/allegro_reviews</td>\n",
       "      <td>legacy-datasets</td>\n",
       "      <td>71593d1379934286885c53d147bc863ffe830745</td>\n",
       "      <td>2022-03-02 23:29:22+00:00</td>\n",
       "      <td>2024-01-09 11:59:39+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>allegro-reviews</td>\n",
       "      <td>[task_categories:text-classification, task_ids...</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                author  \\\n",
       "0   amirveyseh/acronym_identification            amirveyseh   \n",
       "1  ade-benchmark-corpus/ade_corpus_v2  ade-benchmark-corpus   \n",
       "2               UCLNLP/adversarial_qa                UCLNLP   \n",
       "3                     Yale-LILY/aeslc             Yale-LILY   \n",
       "4      nwu-ctext/afrikaans_ner_corpus             nwu-ctext   \n",
       "5                    fancyzhx/ag_news              fancyzhx   \n",
       "6                     allenai/ai2_arc               allenai   \n",
       "7                 google/air_dialogue                google   \n",
       "8             komari6/ajgt_twitter_ar               komari6   \n",
       "9     legacy-datasets/allegro_reviews       legacy-datasets   \n",
       "\n",
       "                                        sha                created_at  \\\n",
       "0  15ef643450d589d5883e289ffadeb03563e80a9e 2022-03-02 23:29:22+00:00   \n",
       "1  4ba01c71687dd7c996597042449448ea312126cf 2022-03-02 23:29:22+00:00   \n",
       "2  c2d5f738db1ad21a4126a144dfbb00cb51e0a4a9 2022-03-02 23:29:22+00:00   \n",
       "3  2305f2e63b68056f9b9037a3805c8c196e0d5581 2022-03-02 23:29:22+00:00   \n",
       "4  445834a997dce8b40e1d108638064381de80c497 2022-03-02 23:29:22+00:00   \n",
       "5  eb185aade064a813bc0b7f42de02595523103ca4 2022-03-02 23:29:22+00:00   \n",
       "6  210d026faf9955653af8916fad021475a3f00453 2022-03-02 23:29:22+00:00   \n",
       "7  dbdbe7bcef8d344bc3c68a05600f3d95917d6898 2022-03-02 23:29:22+00:00   \n",
       "8  af3f2fa5462ac461b696cb300d66e07ad366057f 2022-03-02 23:29:22+00:00   \n",
       "9  71593d1379934286885c53d147bc863ffe830745 2022-03-02 23:29:22+00:00   \n",
       "\n",
       "              last_modified  private  gated  disabled  downloads  likes  \\\n",
       "0 2024-01-09 11:39:57+00:00    False  False     False        180     19   \n",
       "1 2024-01-09 11:42:58+00:00    False  False     False        241     25   \n",
       "2 2023-12-21 14:20:00+00:00    False  False     False        139     32   \n",
       "3 2024-01-09 11:49:13+00:00    False  False     False        124     12   \n",
       "4 2024-01-09 11:51:47+00:00    False  False     False        112      6   \n",
       "5 2024-03-07 12:02:37+00:00    False  False     False       7297    123   \n",
       "6 2023-12-21 15:09:48+00:00    False  False     False     785162    111   \n",
       "7 2024-03-07 15:22:15+00:00    False  False     False         75     15   \n",
       "8 2024-01-09 11:58:01+00:00    False  False     False        119      4   \n",
       "9 2024-01-09 11:59:39+00:00    False  False     False         84      4   \n",
       "\n",
       "        paperswithcode_id                                               tags  \\\n",
       "0  acronym-identification  [task_categories:token-classification, annotat...   \n",
       "1                    None  [task_categories:text-classification, task_cat...   \n",
       "2           adversarialqa  [task_categories:question-answering, task_ids:...   \n",
       "3                   aeslc  [task_categories:summarization, annotations_cr...   \n",
       "4                    None  [task_categories:token-classification, task_id...   \n",
       "5                 ag-news  [task_categories:text-classification, task_ids...   \n",
       "6                    None  [task_categories:question-answering, task_ids:...   \n",
       "7                    None  [task_categories:text-generation, task_categor...   \n",
       "8                    None  [task_categories:text-classification, task_ids...   \n",
       "9         allegro-reviews  [task_categories:text-classification, task_ids...   \n",
       "\n",
       "  card_data siblings  \n",
       "0        {}     None  \n",
       "1        {}     None  \n",
       "2        {}     None  \n",
       "3        {}     None  \n",
       "4        {}     None  \n",
       "5        {}     None  \n",
       "6        {}     None  \n",
       "7        {}     None  \n",
       "8        {}     None  \n",
       "9        {}     None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO run for all datasets\n",
    "datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "datasets_df = pd.DataFrame(datasets)\n",
    "datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
       "       'gated', 'disabled', 'downloads', 'likes', 'paperswithcode_id', 'tags',\n",
       "       'card_data', 'siblings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                   amirveyseh/acronym_identification\n",
       "author                                                      amirveyseh\n",
       "sha                           15ef643450d589d5883e289ffadeb03563e80a9e\n",
       "created_at                                   2022-03-02 23:29:22+00:00\n",
       "last_modified                                2024-01-09 11:39:57+00:00\n",
       "private                                                          False\n",
       "gated                                                            False\n",
       "disabled                                                         False\n",
       "downloads                                                          180\n",
       "likes                                                               19\n",
       "paperswithcode_id                               acronym-identification\n",
       "tags                 [task_categories:token-classification, annotat...\n",
       "card_data                                                           {}\n",
       "siblings                                                          None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_categories:question-answering',\n",
       " 'task_ids:extractive-qa',\n",
       " 'task_ids:open-domain-qa',\n",
       " 'annotations_creators:crowdsourced',\n",
       " 'language_creators:found',\n",
       " 'multilinguality:monolingual',\n",
       " 'source_datasets:original',\n",
       " 'language:en',\n",
       " 'license:cc-by-sa-4.0',\n",
       " 'size_categories:10K<n<100K',\n",
       " 'format:parquet',\n",
       " 'modality:text',\n",
       " 'library:datasets',\n",
       " 'library:pandas',\n",
       " 'library:mlcroissant',\n",
       " 'library:polars',\n",
       " 'arxiv:2002.00293',\n",
       " 'arxiv:1606.05250',\n",
       " 'region:us']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df.loc[2]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset_idx):\n",
    "\n",
    "\tdataset = datasets_df.loc[dataset_idx]\n",
    "\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset['id'])\n",
    "\tdataset_attributes['size [GB]'] = None\n",
    "\n",
    "\turl = \"https://huggingface.co/datasets/\" + dataset['id']\n",
    "\tfile_size_str = extract_file_size(url)\n",
    "\tif file_size_str:\n",
    "\t\tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t\tif file_size_gb:\n",
    "\t\t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_json(datasets_df):\n",
    "    \n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for dataset_idx in range(datasets_df.shape[0]):\n",
    "        output.append(extract_datasets_attributes(dataset_idx))\n",
    "        if dataset_idx % 10 == 0:\n",
    "            print(f'Processed {dataset_idx} datasets, elapsed time: {time.time() - start_time:.2f} seconds')\n",
    "    \n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Dataset.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 datasets, elapsed time: 1.00 seconds\n",
      "Processed 10 datasets, elapsed time: 9.65 seconds\n",
      "Processed 20 datasets, elapsed time: 17.75 seconds\n",
      "Processed 30 datasets, elapsed time: 25.48 seconds\n",
      "Processed 40 datasets, elapsed time: 33.46 seconds\n",
      "Processed 50 datasets, elapsed time: 42.07 seconds\n",
      "Processed 60 datasets, elapsed time: 51.79 seconds\n",
      "Processed 70 datasets, elapsed time: 58.55 seconds\n",
      "Processed 80 datasets, elapsed time: 65.34 seconds\n",
      "Processed 90 datasets, elapsed time: 73.20 seconds\n",
      "Processed 100 datasets, elapsed time: 81.37 seconds\n",
      "Processed 110 datasets, elapsed time: 89.41 seconds\n",
      "Processed 120 datasets, elapsed time: 98.28 seconds\n",
      "Processed 130 datasets, elapsed time: 105.30 seconds\n",
      "Processed 140 datasets, elapsed time: 113.13 seconds\n",
      "Processed 150 datasets, elapsed time: 119.39 seconds\n",
      "Processed 160 datasets, elapsed time: 125.61 seconds\n",
      "Processed 170 datasets, elapsed time: 133.69 seconds\n",
      "Processed 180 datasets, elapsed time: 140.15 seconds\n",
      "Processed 190 datasets, elapsed time: 146.29 seconds\n",
      "Processed 200 datasets, elapsed time: 153.17 seconds\n",
      "Processed 210 datasets, elapsed time: 160.06 seconds\n",
      "Processed 220 datasets, elapsed time: 167.29 seconds\n",
      "Processed 230 datasets, elapsed time: 174.49 seconds\n",
      "Processed 240 datasets, elapsed time: 181.96 seconds\n",
      "Processed 250 datasets, elapsed time: 188.86 seconds\n",
      "Processed 260 datasets, elapsed time: 195.17 seconds\n",
      "Processed 270 datasets, elapsed time: 201.14 seconds\n",
      "Processed 280 datasets, elapsed time: 206.82 seconds\n",
      "Processed 290 datasets, elapsed time: 212.84 seconds\n",
      "Processed 300 datasets, elapsed time: 218.66 seconds\n",
      "Processed 310 datasets, elapsed time: 225.04 seconds\n",
      "Processed 320 datasets, elapsed time: 232.35 seconds\n",
      "Processed 330 datasets, elapsed time: 239.15 seconds\n",
      "Processed 340 datasets, elapsed time: 245.25 seconds\n",
      "Processed 350 datasets, elapsed time: 250.93 seconds\n",
      "Processed 360 datasets, elapsed time: 257.13 seconds\n",
      "Processed 370 datasets, elapsed time: 264.18 seconds\n",
      "Processed 380 datasets, elapsed time: 269.91 seconds\n",
      "Processed 390 datasets, elapsed time: 275.97 seconds\n",
      "Processed 400 datasets, elapsed time: 281.88 seconds\n",
      "Processed 410 datasets, elapsed time: 287.41 seconds\n",
      "Processed 420 datasets, elapsed time: 294.33 seconds\n",
      "Processed 430 datasets, elapsed time: 301.62 seconds\n",
      "Processed 440 datasets, elapsed time: 310.20 seconds\n",
      "Processed 450 datasets, elapsed time: 317.36 seconds\n",
      "Processed 460 datasets, elapsed time: 325.05 seconds\n",
      "Processed 470 datasets, elapsed time: 332.20 seconds\n",
      "Processed 480 datasets, elapsed time: 338.33 seconds\n",
      "Processed 490 datasets, elapsed time: 346.54 seconds\n",
      "Processed 500 datasets, elapsed time: 356.02 seconds\n",
      "Processed 510 datasets, elapsed time: 363.41 seconds\n",
      "Processed 520 datasets, elapsed time: 371.07 seconds\n",
      "Processed 530 datasets, elapsed time: 376.83 seconds\n",
      "Processed 540 datasets, elapsed time: 384.30 seconds\n",
      "Processed 550 datasets, elapsed time: 390.66 seconds\n",
      "Processed 560 datasets, elapsed time: 397.23 seconds\n",
      "Processed 570 datasets, elapsed time: 403.73 seconds\n",
      "Processed 580 datasets, elapsed time: 411.70 seconds\n",
      "Processed 590 datasets, elapsed time: 419.07 seconds\n",
      "Processed 600 datasets, elapsed time: 426.37 seconds\n",
      "Processed 610 datasets, elapsed time: 433.19 seconds\n",
      "Processed 620 datasets, elapsed time: 438.24 seconds\n",
      "Processed 630 datasets, elapsed time: 444.84 seconds\n",
      "Processed 640 datasets, elapsed time: 451.92 seconds\n",
      "Processed 650 datasets, elapsed time: 458.82 seconds\n",
      "Processed 660 datasets, elapsed time: 466.19 seconds\n",
      "Processed 670 datasets, elapsed time: 473.30 seconds\n",
      "Processed 680 datasets, elapsed time: 480.77 seconds\n",
      "Processed 690 datasets, elapsed time: 488.38 seconds\n",
      "Processed 700 datasets, elapsed time: 495.65 seconds\n",
      "Processed 710 datasets, elapsed time: 504.77 seconds\n",
      "Processed 720 datasets, elapsed time: 512.01 seconds\n",
      "Processed 730 datasets, elapsed time: 521.36 seconds\n",
      "Processed 740 datasets, elapsed time: 527.86 seconds\n",
      "Processed 750 datasets, elapsed time: 536.91 seconds\n",
      "Processed 760 datasets, elapsed time: 544.58 seconds\n",
      "Processed 770 datasets, elapsed time: 551.46 seconds\n",
      "Processed 780 datasets, elapsed time: 557.88 seconds\n",
      "Processed 790 datasets, elapsed time: 565.73 seconds\n",
      "Processed 800 datasets, elapsed time: 571.26 seconds\n",
      "Processed 810 datasets, elapsed time: 576.84 seconds\n",
      "Processed 820 datasets, elapsed time: 582.55 seconds\n",
      "Processed 830 datasets, elapsed time: 588.46 seconds\n",
      "Processed 840 datasets, elapsed time: 596.36 seconds\n",
      "Processed 850 datasets, elapsed time: 602.47 seconds\n",
      "Processed 860 datasets, elapsed time: 608.91 seconds\n",
      "Processed 870 datasets, elapsed time: 614.91 seconds\n",
      "Processed 880 datasets, elapsed time: 621.03 seconds\n",
      "Processed 890 datasets, elapsed time: 627.99 seconds\n",
      "Processed 900 datasets, elapsed time: 635.31 seconds\n",
      "Processed 910 datasets, elapsed time: 644.85 seconds\n",
      "Processed 920 datasets, elapsed time: 654.47 seconds\n",
      "Processed 930 datasets, elapsed time: 664.13 seconds\n",
      "Processed 940 datasets, elapsed time: 670.25 seconds\n",
      "Processed 950 datasets, elapsed time: 675.47 seconds\n",
      "Processed 960 datasets, elapsed time: 682.08 seconds\n",
      "Processed 970 datasets, elapsed time: 689.75 seconds\n",
      "Processed 980 datasets, elapsed time: 696.21 seconds\n",
      "Processed 990 datasets, elapsed time: 703.07 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_to_json(datasets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info extraction optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes_optimized(dataset):\n",
    "\n",
    "\tdataset_tags = dataset.tags\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset.id)\n",
    "\tdataset_attributes['size [GB]'] = match_size(dataset_tags)\n",
    "\n",
    "\t# url = \"https://huggingface.co/datasets/\" + dataset.id\n",
    "\t# file_size_str = extract_file_size(url)\n",
    "\t# if file_size_str:\n",
    "\t# \tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t# \tif file_size_gb:\n",
    "\t# \t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags) # TODO: add multiple URIs when available?\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 datasets processed, 2.122116804122925 seconds elapsed, estimated time remaining: 421.54 seconds\n",
      "2000 datasets processed, 3.3351528644561768 seconds elapsed, estimated time remaining: 329.58 seconds\n",
      "3000 datasets processed, 4.077770948410034 seconds elapsed, estimated time remaining: 267.29 seconds\n",
      "4000 datasets processed, 4.908613920211792 seconds elapsed, estimated time remaining: 240.08 seconds\n",
      "5000 datasets processed, 5.631502866744995 seconds elapsed, estimated time remaining: 219.23 seconds\n",
      "6000 datasets processed, 6.8947179317474365 seconds elapsed, estimated time remaining: 222.52 seconds\n",
      "7000 datasets processed, 7.563861846923828 seconds elapsed, estimated time remaining: 208.16 seconds\n",
      "8000 datasets processed, 8.125163793563843 seconds elapsed, estimated time remaining: 194.64 seconds\n",
      "9000 datasets processed, 8.944549798965454 seconds elapsed, estimated time remaining: 189.47 seconds\n",
      "10000 datasets processed, 9.60464882850647 seconds elapsed, estimated time remaining: 182.14 seconds\n",
      "11000 datasets processed, 10.610082626342773 seconds elapsed, estimated time remaining: 181.96 seconds\n",
      "12000 datasets processed, 11.249409675598145 seconds elapsed, estimated time remaining: 175.91 seconds\n",
      "13000 datasets processed, 11.898953914642334 seconds elapsed, estimated time remaining: 170.83 seconds\n",
      "14000 datasets processed, 12.693379878997803 seconds elapsed, estimated time remaining: 168.32 seconds\n",
      "15000 datasets processed, 13.439291715621948 seconds elapsed, estimated time remaining: 165.43 seconds\n",
      "16000 datasets processed, 14.698966026306152 seconds elapsed, estimated time remaining: 168.71 seconds\n",
      "17000 datasets processed, 15.484087705612183 seconds elapsed, estimated time remaining: 166.36 seconds\n",
      "18000 datasets processed, 16.036244869232178 seconds elapsed, estimated time remaining: 161.83 seconds\n",
      "19000 datasets processed, 16.688336849212646 seconds elapsed, estimated time remaining: 158.66 seconds\n",
      "20000 datasets processed, 17.31124472618103 seconds elapsed, estimated time remaining: 155.49 seconds\n",
      "21000 datasets processed, 17.968323707580566 seconds elapsed, estimated time remaining: 152.85 seconds\n",
      "22000 datasets processed, 18.81830382347107 seconds elapsed, estimated time remaining: 151.95 seconds\n",
      "23000 datasets processed, 19.56389570236206 seconds elapsed, estimated time remaining: 150.25 seconds\n",
      "24000 datasets processed, 20.313493013381958 seconds elapsed, estimated time remaining: 148.66 seconds\n",
      "25000 datasets processed, 21.026275873184204 seconds elapsed, estimated time remaining: 146.88 seconds\n",
      "26000 datasets processed, 21.66336989402771 seconds elapsed, estimated time remaining: 144.68 seconds\n",
      "27000 datasets processed, 22.24871277809143 seconds elapsed, estimated time remaining: 142.26 seconds\n",
      "28000 datasets processed, 22.82092785835266 seconds elapsed, estimated time remaining: 139.89 seconds\n",
      "29000 datasets processed, 23.64312767982483 seconds elapsed, estimated time remaining: 139.12 seconds\n",
      "30000 datasets processed, 24.22741198539734 seconds elapsed, estimated time remaining: 137.00 seconds\n",
      "31000 datasets processed, 25.019900798797607 seconds elapsed, estimated time remaining: 136.11 seconds\n",
      "32000 datasets processed, 25.557586908340454 seconds elapsed, estimated time remaining: 133.89 seconds\n",
      "33000 datasets processed, 26.105581760406494 seconds elapsed, estimated time remaining: 131.83 seconds\n",
      "34000 datasets processed, 26.997462034225464 seconds elapsed, estimated time remaining: 131.53 seconds\n",
      "35000 datasets processed, 27.542242765426636 seconds elapsed, estimated time remaining: 129.56 seconds\n",
      "36000 datasets processed, 28.10082197189331 seconds elapsed, estimated time remaining: 127.74 seconds\n",
      "37000 datasets processed, 28.615059852600098 seconds elapsed, estimated time remaining: 125.78 seconds\n",
      "38000 datasets processed, 29.448315858840942 seconds elapsed, estimated time remaining: 125.27 seconds\n",
      "39000 datasets processed, 30.080533981323242 seconds elapsed, estimated time remaining: 123.90 seconds\n",
      "40000 datasets processed, 30.73962378501892 seconds elapsed, estimated time remaining: 122.68 seconds\n",
      "41000 datasets processed, 31.449213981628418 seconds elapsed, estimated time remaining: 121.69 seconds\n",
      "42000 datasets processed, 31.986824989318848 seconds elapsed, estimated time remaining: 120.06 seconds\n",
      "43000 datasets processed, 32.49457883834839 seconds elapsed, estimated time remaining: 118.37 seconds\n",
      "44000 datasets processed, 33.06981301307678 seconds elapsed, estimated time remaining: 116.98 seconds\n",
      "45000 datasets processed, 33.6497757434845 seconds elapsed, estimated time remaining: 115.64 seconds\n",
      "46000 datasets processed, 34.20798587799072 seconds elapsed, estimated time remaining: 114.26 seconds\n",
      "47000 datasets processed, 34.76920175552368 seconds elapsed, estimated time remaining: 112.92 seconds\n",
      "48000 datasets processed, 35.60355091094971 seconds elapsed, estimated time remaining: 112.48 seconds\n",
      "49000 datasets processed, 36.17472171783447 seconds elapsed, estimated time remaining: 111.21 seconds\n",
      "50000 datasets processed, 36.69904088973999 seconds elapsed, estimated time remaining: 109.83 seconds\n",
      "51000 datasets processed, 37.942800760269165 seconds elapsed, estimated time remaining: 110.59 seconds\n",
      "52000 datasets processed, 47.4497447013855 seconds elapsed, estimated time remaining: 134.72 seconds\n",
      "53000 datasets processed, 48.65088677406311 seconds elapsed, estimated time remaining: 134.61 seconds\n",
      "54000 datasets processed, 51.521758794784546 seconds elapsed, estimated time remaining: 138.96 seconds\n",
      "55000 datasets processed, 54.53571367263794 seconds elapsed, estimated time remaining: 143.42 seconds\n",
      "56000 datasets processed, 56.3685827255249 seconds elapsed, estimated time remaining: 144.59 seconds\n",
      "57000 datasets processed, 57.212470054626465 seconds elapsed, estimated time remaining: 143.17 seconds\n",
      "58000 datasets processed, 58.80527591705322 seconds elapsed, estimated time remaining: 143.61 seconds\n",
      "59000 datasets processed, 60.45535683631897 seconds elapsed, estimated time remaining: 144.11 seconds\n",
      "60000 datasets processed, 61.565786838531494 seconds elapsed, estimated time remaining: 143.29 seconds\n",
      "61000 datasets processed, 62.435425758361816 seconds elapsed, estimated time remaining: 141.90 seconds\n",
      "62000 datasets processed, 63.04928779602051 seconds elapsed, estimated time remaining: 139.97 seconds\n",
      "63000 datasets processed, 64.31148171424866 seconds elapsed, estimated time remaining: 139.49 seconds\n",
      "64000 datasets processed, 66.64395380020142 seconds elapsed, estimated time remaining: 141.25 seconds\n",
      "65000 datasets processed, 67.56693887710571 seconds elapsed, estimated time remaining: 139.96 seconds\n",
      "66000 datasets processed, 68.91160082817078 seconds elapsed, estimated time remaining: 139.54 seconds\n",
      "67000 datasets processed, 70.51137089729309 seconds elapsed, estimated time remaining: 139.59 seconds\n",
      "68000 datasets processed, 71.08591175079346 seconds elapsed, estimated time remaining: 137.62 seconds\n",
      "69000 datasets processed, 71.83659076690674 seconds elapsed, estimated time remaining: 136.01 seconds\n",
      "70000 datasets processed, 72.6812059879303 seconds elapsed, estimated time remaining: 134.61 seconds\n",
      "71000 datasets processed, 73.76525068283081 seconds elapsed, estimated time remaining: 133.65 seconds\n",
      "72000 datasets processed, 74.39313888549805 seconds elapsed, estimated time remaining: 131.88 seconds\n",
      "73000 datasets processed, 74.97298097610474 seconds elapsed, estimated time remaining: 130.06 seconds\n",
      "74000 datasets processed, 75.81936001777649 seconds elapsed, estimated time remaining: 128.73 seconds\n",
      "75000 datasets processed, 76.95166778564453 seconds elapsed, estimated time remaining: 127.89 seconds\n",
      "76000 datasets processed, 77.69212889671326 seconds elapsed, estimated time remaining: 126.39 seconds\n",
      "77000 datasets processed, 79.06292462348938 seconds elapsed, estimated time remaining: 125.93 seconds\n",
      "78000 datasets processed, 80.13140678405762 seconds elapsed, estimated time remaining: 124.97 seconds\n",
      "79000 datasets processed, 81.36207365989685 seconds elapsed, estimated time remaining: 124.25 seconds\n",
      "80000 datasets processed, 81.91061186790466 seconds elapsed, estimated time remaining: 122.50 seconds\n",
      "81000 datasets processed, 83.81559681892395 seconds elapsed, estimated time remaining: 122.77 seconds\n",
      "82000 datasets processed, 85.03667974472046 seconds elapsed, estimated time remaining: 122.00 seconds\n",
      "83000 datasets processed, 87.30819892883301 seconds elapsed, estimated time remaining: 122.70 seconds\n",
      "84000 datasets processed, 88.57164978981018 seconds elapsed, estimated time remaining: 121.94 seconds\n",
      "85000 datasets processed, 90.47069787979126 seconds elapsed, estimated time remaining: 122.02 seconds\n",
      "86000 datasets processed, 91.34876894950867 seconds elapsed, estimated time remaining: 120.71 seconds\n",
      "87000 datasets processed, 93.9034857749939 seconds elapsed, estimated time remaining: 121.58 seconds\n",
      "88000 datasets processed, 97.04180574417114 seconds elapsed, estimated time remaining: 123.11 seconds\n",
      "89000 datasets processed, 99.23932480812073 seconds elapsed, estimated time remaining: 123.37 seconds\n",
      "90000 datasets processed, 104.81996870040894 seconds elapsed, estimated time remaining: 127.70 seconds\n",
      "91000 datasets processed, 106.37638187408447 seconds elapsed, estimated time remaining: 127.00 seconds\n",
      "92000 datasets processed, 109.31831765174866 seconds elapsed, estimated time remaining: 127.90 seconds\n",
      "93000 datasets processed, 112.42541170120239 seconds elapsed, estimated time remaining: 128.92 seconds\n",
      "94000 datasets processed, 114.63503575325012 seconds elapsed, estimated time remaining: 128.83 seconds\n",
      "95000 datasets processed, 116.06817889213562 seconds elapsed, estimated time remaining: 127.85 seconds\n",
      "96000 datasets processed, 118.65729784965515 seconds elapsed, estimated time remaining: 128.10 seconds\n",
      "97000 datasets processed, 120.61657667160034 seconds elapsed, estimated time remaining: 127.63 seconds\n",
      "98000 datasets processed, 123.10335278511047 seconds elapsed, estimated time remaining: 127.68 seconds\n",
      "99000 datasets processed, 124.98136186599731 seconds elapsed, estimated time remaining: 127.05 seconds\n",
      "100000 datasets processed, 125.93803596496582 seconds elapsed, estimated time remaining: 125.49 seconds\n",
      "101000 datasets processed, 129.22076892852783 seconds elapsed, estimated time remaining: 126.20 seconds\n",
      "102000 datasets processed, 130.6019377708435 seconds elapsed, estimated time remaining: 125.02 seconds\n",
      "103000 datasets processed, 131.4106786251068 seconds elapsed, estimated time remaining: 123.30 seconds\n",
      "104000 datasets processed, 134.5907588005066 seconds elapsed, estimated time remaining: 123.77 seconds\n",
      "105000 datasets processed, 136.62128686904907 seconds elapsed, estimated time remaining: 123.14 seconds\n",
      "106000 datasets processed, 137.97003388404846 seconds elapsed, estimated time remaining: 121.88 seconds\n",
      "107000 datasets processed, 140.18420481681824 seconds elapsed, estimated time remaining: 121.37 seconds\n",
      "108000 datasets processed, 141.9468936920166 seconds elapsed, estimated time remaining: 120.45 seconds\n",
      "109000 datasets processed, 143.2104127407074 seconds elapsed, estimated time remaining: 119.09 seconds\n",
      "110000 datasets processed, 144.0999937057495 seconds elapsed, estimated time remaining: 117.43 seconds\n",
      "111000 datasets processed, 145.85968589782715 seconds elapsed, estimated time remaining: 116.48 seconds\n",
      "112000 datasets processed, 148.926109790802 seconds elapsed, estimated time remaining: 116.54 seconds\n",
      "113000 datasets processed, 150.71394896507263 seconds elapsed, estimated time remaining: 115.56 seconds\n",
      "114000 datasets processed, 151.7510735988617 seconds elapsed, estimated time remaining: 114.00 seconds\n",
      "115000 datasets processed, 153.88023281097412 seconds elapsed, estimated time remaining: 113.26 seconds\n",
      "116000 datasets processed, 155.86638164520264 seconds elapsed, estimated time remaining: 112.39 seconds\n",
      "117000 datasets processed, 157.36700177192688 seconds elapsed, estimated time remaining: 111.15 seconds\n",
      "118000 datasets processed, 158.66950297355652 seconds elapsed, estimated time remaining: 109.78 seconds\n",
      "119000 datasets processed, 159.2974967956543 seconds elapsed, estimated time remaining: 107.95 seconds\n",
      "120000 datasets processed, 162.974871635437 seconds elapsed, estimated time remaining: 108.16 seconds\n",
      "121000 datasets processed, 165.1095826625824 seconds elapsed, estimated time remaining: 107.31 seconds\n",
      "122000 datasets processed, 166.6437587738037 seconds elapsed, estimated time remaining: 106.05 seconds\n",
      "123000 datasets processed, 168.48945379257202 seconds elapsed, estimated time remaining: 104.99 seconds\n",
      "124000 datasets processed, 169.70462274551392 seconds elapsed, estimated time remaining: 103.52 seconds\n",
      "125000 datasets processed, 171.67992091178894 seconds elapsed, estimated time remaining: 102.52 seconds\n",
      "126000 datasets processed, 173.7884407043457 seconds elapsed, estimated time remaining: 101.57 seconds\n",
      "127000 datasets processed, 176.2727916240692 seconds elapsed, estimated time remaining: 100.83 seconds\n",
      "128000 datasets processed, 178.2054579257965 seconds elapsed, estimated time remaining: 99.74 seconds\n",
      "129000 datasets processed, 179.24225997924805 seconds elapsed, estimated time remaining: 98.16 seconds\n",
      "130000 datasets processed, 182.15525579452515 seconds elapsed, estimated time remaining: 97.58 seconds\n",
      "131000 datasets processed, 183.7547287940979 seconds elapsed, estimated time remaining: 96.28 seconds\n",
      "132000 datasets processed, 185.68241691589355 seconds elapsed, estimated time remaining: 95.15 seconds\n",
      "133000 datasets processed, 187.49032187461853 seconds elapsed, estimated time remaining: 93.95 seconds\n",
      "134000 datasets processed, 189.20620965957642 seconds elapsed, estimated time remaining: 92.69 seconds\n",
      "135000 datasets processed, 189.9552628993988 seconds elapsed, estimated time remaining: 90.96 seconds\n",
      "136000 datasets processed, 190.73320388793945 seconds elapsed, estimated time remaining: 89.25 seconds\n",
      "137000 datasets processed, 191.63635683059692 seconds elapsed, estimated time remaining: 87.62 seconds\n",
      "138000 datasets processed, 192.5123279094696 seconds elapsed, estimated time remaining: 85.99 seconds\n",
      "139000 datasets processed, 193.4695019721985 seconds elapsed, estimated time remaining: 84.41 seconds\n",
      "140000 datasets processed, 194.50009179115295 seconds elapsed, estimated time remaining: 82.86 seconds\n",
      "141000 datasets processed, 195.61935472488403 seconds elapsed, estimated time remaining: 81.36 seconds\n",
      "142000 datasets processed, 196.53833770751953 seconds elapsed, estimated time remaining: 79.78 seconds\n",
      "143000 datasets processed, 197.85641074180603 seconds elapsed, estimated time remaining: 78.37 seconds\n",
      "144000 datasets processed, 198.83163595199585 seconds elapsed, estimated time remaining: 76.83 seconds\n",
      "145000 datasets processed, 199.91191291809082 seconds elapsed, estimated time remaining: 75.34 seconds\n",
      "146000 datasets processed, 201.01634097099304 seconds elapsed, estimated time remaining: 73.86 seconds\n",
      "147000 datasets processed, 202.15424990653992 seconds elapsed, estimated time remaining: 72.39 seconds\n",
      "148000 datasets processed, 203.45923686027527 seconds elapsed, estimated time remaining: 70.99 seconds\n",
      "149000 datasets processed, 204.9504907131195 seconds elapsed, estimated time remaining: 69.66 seconds\n",
      "150000 datasets processed, 205.95238399505615 seconds elapsed, estimated time remaining: 68.16 seconds\n",
      "151000 datasets processed, 207.25736093521118 seconds elapsed, estimated time remaining: 66.76 seconds\n",
      "152000 datasets processed, 208.26903986930847 seconds elapsed, estimated time remaining: 65.28 seconds\n",
      "153000 datasets processed, 209.5877068042755 seconds elapsed, estimated time remaining: 63.89 seconds\n",
      "154000 datasets processed, 210.51502680778503 seconds elapsed, estimated time remaining: 62.39 seconds\n",
      "155000 datasets processed, 212.17121267318726 seconds elapsed, estimated time remaining: 61.11 seconds\n",
      "156000 datasets processed, 213.0911808013916 seconds elapsed, estimated time remaining: 59.61 seconds\n",
      "157000 datasets processed, 214.40409088134766 seconds elapsed, estimated time remaining: 58.23 seconds\n",
      "158000 datasets processed, 215.9018349647522 seconds elapsed, estimated time remaining: 56.90 seconds\n",
      "159000 datasets processed, 217.3663468360901 seconds elapsed, estimated time remaining: 55.56 seconds\n",
      "160000 datasets processed, 218.31348586082458 seconds elapsed, estimated time remaining: 54.09 seconds\n",
      "161000 datasets processed, 221.26681661605835 seconds elapsed, estimated time remaining: 53.11 seconds\n",
      "162000 datasets processed, 224.44454979896545 seconds elapsed, estimated time remaining: 52.15 seconds\n",
      "163000 datasets processed, 226.4940948486328 seconds elapsed, estimated time remaining: 50.92 seconds\n",
      "164000 datasets processed, 228.03164291381836 seconds elapsed, estimated time remaining: 49.56 seconds\n",
      "165000 datasets processed, 229.06607484817505 seconds elapsed, estimated time remaining: 48.09 seconds\n",
      "166000 datasets processed, 230.48825073242188 seconds elapsed, estimated time remaining: 46.71 seconds\n",
      "167000 datasets processed, 231.60846996307373 seconds elapsed, estimated time remaining: 45.27 seconds\n",
      "168000 datasets processed, 232.8323938846588 seconds elapsed, estimated time remaining: 43.85 seconds\n",
      "169000 datasets processed, 233.67502164840698 seconds elapsed, estimated time remaining: 42.37 seconds\n",
      "170000 datasets processed, 234.7619707584381 seconds elapsed, estimated time remaining: 40.93 seconds\n",
      "171000 datasets processed, 235.91339492797852 seconds elapsed, estimated time remaining: 39.51 seconds\n",
      "172000 datasets processed, 237.3015058040619 seconds elapsed, estimated time remaining: 38.14 seconds\n",
      "173000 datasets processed, 238.07863092422485 seconds elapsed, estimated time remaining: 36.66 seconds\n",
      "174000 datasets processed, 239.1590118408203 seconds elapsed, estimated time remaining: 35.24 seconds\n",
      "175000 datasets processed, 239.8514859676361 seconds elapsed, estimated time remaining: 33.77 seconds\n",
      "176000 datasets processed, 240.91669392585754 seconds elapsed, estimated time remaining: 32.36 seconds\n",
      "177000 datasets processed, 241.9160897731781 seconds elapsed, estimated time remaining: 30.95 seconds\n",
      "178000 datasets processed, 243.4183328151703 seconds elapsed, estimated time remaining: 29.60 seconds\n",
      "179000 datasets processed, 244.7566478252411 seconds elapsed, estimated time remaining: 28.22 seconds\n",
      "180000 datasets processed, 246.29094076156616 seconds elapsed, estimated time remaining: 26.88 seconds\n",
      "181000 datasets processed, 247.21201276779175 seconds elapsed, estimated time remaining: 25.46 seconds\n",
      "182000 datasets processed, 248.61947965621948 seconds elapsed, estimated time remaining: 24.10 seconds\n",
      "183000 datasets processed, 250.49937677383423 seconds elapsed, estimated time remaining: 22.78 seconds\n",
      "184000 datasets processed, 251.873694896698 seconds elapsed, estimated time remaining: 21.41 seconds\n",
      "185000 datasets processed, 253.12114787101746 seconds elapsed, estimated time remaining: 20.03 seconds\n",
      "186000 datasets processed, 254.3921709060669 seconds elapsed, estimated time remaining: 18.66 seconds\n",
      "187000 datasets processed, 255.4076578617096 seconds elapsed, estimated time remaining: 17.27 seconds\n",
      "188000 datasets processed, 256.3338990211487 seconds elapsed, estimated time remaining: 15.87 seconds\n",
      "189000 datasets processed, 258.03834867477417 seconds elapsed, estimated time remaining: 14.53 seconds\n",
      "190000 datasets processed, 259.8644380569458 seconds elapsed, estimated time remaining: 13.19 seconds\n",
      "191000 datasets processed, 261.2272729873657 seconds elapsed, estimated time remaining: 11.82 seconds\n",
      "192000 datasets processed, 262.65996074676514 seconds elapsed, estimated time remaining: 10.45 seconds\n",
      "193000 datasets processed, 265.84408593177795 seconds elapsed, estimated time remaining: 9.15 seconds\n",
      "194000 datasets processed, 266.7928566932678 seconds elapsed, estimated time remaining: 7.76 seconds\n",
      "195000 datasets processed, 267.9725868701935 seconds elapsed, estimated time remaining: 6.38 seconds\n",
      "196000 datasets processed, 269.1977288722992 seconds elapsed, estimated time remaining: 5.00 seconds\n",
      "197000 datasets processed, 270.56678581237793 seconds elapsed, estimated time remaining: 3.63 seconds\n",
      "198000 datasets processed, 271.5700206756592 seconds elapsed, estimated time remaining: 2.25 seconds\n",
      "199000 datasets processed, 272.7336976528168 seconds elapsed, estimated time remaining: 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'hf_extracted_json')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "file_path = os.path.join(result_path, 'datasets_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for dataset in datasets:\n",
    "    dataset_attributes = extract_datasets_attributes_optimized(dataset)\n",
    "    add_to_json_file(dataset_attributes, file_path)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} datasets processed, {time.time() - start_time} seconds elapsed, estimated time remaining: {(time.time() - start_time) / count * (199642 - count):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def datasets_statistics(file_name):\n",
    "\n",
    "\tname_count = 0\n",
    "\tsize_count = 0\n",
    "\tlanguages_count = 0\n",
    "\tlicense_count = 0\n",
    "\tdomain_count = 0\n",
    "\turi_count = 0\n",
    "\tfinetuning_count = 0\n",
    "\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "\tprint()\n",
    "\tdatasets_json = open(os.path.join(result_path, file_name))\n",
    "\tdatasets_data_json = json.load(datasets_json)\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['size [rows]'] is not None: # can be size [GB] or size [rows]\n",
    "\t\t\tsize_count += 1\n",
    "\t\tif len(item['languages']) > 0:\n",
    "\t\t\tlanguages_count += 1\t\n",
    "\t\tif item['licenseToUse'] is not None:\t\n",
    "\t\t\tlicense_count += 1\n",
    "\t\tif len(item['domain']) > 0:\n",
    "\t\t\tdomain_count += 1\t\n",
    "\t\tif item['uri'] is not None:\n",
    "\t\t\turi_count += 1\n",
    "\t\tif item['fineTuning'] is not None:\n",
    "\t\t\tfinetuning_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\t\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Size: {size_count} ({(size_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Languages: {languages_count} ({(languages_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    License to use: {license_count} ({(license_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Domain: {domain_count} ({(domain_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    URI: {uri_count} ({(uri_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Fine-tuning: {finetuning_count} ({(finetuning_count / total_datasets) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tdatasets_attributes = datasets_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(datasets_data_json):\n",
    "\t\tdataset_name = item['name']\n",
    "\t\tfor attr in datasets_attributes:\n",
    "\t\t\tif item[attr] is not None and attr == 'size [GB]':\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': True, 'available scraping': False}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': dataset_name, 'entity name': 'Dataset', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of processed datasets: 199654\n",
      "    Name: 199654 (100.00%)\n",
      "    Size: 150976 (75.62%)\n",
      "    Languages: 22559 (11.30%)\n",
      "    License to use: 54002 (27.05%)\n",
      "    Domain: 7319 (3.67%)\n",
      "    URI: 7032 (3.52%)\n",
      "    Fine-tuning: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = datasets_statistics('datasets_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>acronym_identification</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>fineTuning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ade_corpus_v2</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>fineTuning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>name</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>size [GB]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>languages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>licenseToUse</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>domain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adversarial_qa</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>uri</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id entity name attribute name available API  \\\n",
       "0   acronym_identification     Dataset           name          True   \n",
       "1   acronym_identification     Dataset      size [GB]         False   \n",
       "2   acronym_identification     Dataset      languages          True   \n",
       "3   acronym_identification     Dataset   licenseToUse          True   \n",
       "4   acronym_identification     Dataset         domain         False   \n",
       "5   acronym_identification     Dataset            uri          True   \n",
       "6   acronym_identification     Dataset     fineTuning         False   \n",
       "7            ade_corpus_v2     Dataset           name          True   \n",
       "8            ade_corpus_v2     Dataset      size [GB]         False   \n",
       "9            ade_corpus_v2     Dataset      languages          True   \n",
       "10           ade_corpus_v2     Dataset   licenseToUse          True   \n",
       "11           ade_corpus_v2     Dataset         domain         False   \n",
       "12           ade_corpus_v2     Dataset            uri         False   \n",
       "13           ade_corpus_v2     Dataset     fineTuning         False   \n",
       "14          adversarial_qa     Dataset           name          True   \n",
       "15          adversarial_qa     Dataset      size [GB]         False   \n",
       "16          adversarial_qa     Dataset      languages          True   \n",
       "17          adversarial_qa     Dataset   licenseToUse          True   \n",
       "18          adversarial_qa     Dataset         domain         False   \n",
       "19          adversarial_qa     Dataset            uri          True   \n",
       "\n",
       "   available scraping  \n",
       "0               False  \n",
       "1                True  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  \n",
       "5               False  \n",
       "6               False  \n",
       "7               False  \n",
       "8                True  \n",
       "9               False  \n",
       "10              False  \n",
       "11              False  \n",
       "12              False  \n",
       "13              False  \n",
       "14              False  \n",
       "15               True  \n",
       "16              False  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description, # TODO: text2text generation has no description\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "\n",
    "    with open(result_path + '/ChatIMPACT.DownstreamTask.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tasks_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def tasks_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tsub_task_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\ttask_json = open(os.path.join(result_path, 'ChatIMPACT.DownstreamTask.json'))\n",
    "\ttask_data_json = json.load(task_json)\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\t\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif len(item['sub-task']) > 0:\n",
    "\t\t\tsub_task_count += 1\n",
    "\t\n",
    "\ttask_count = idx + 1\n",
    "\tprint(f'Number of processed task: {idx + 1}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / task_count) * 100:.2f}%)')\n",
    "\tprint(f'    Sub-task: {sub_task_count} ({(sub_task_count / task_count) * 100:.2f}%)')\n",
    "\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\ttask_attributes = task_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(task_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in task_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'DownstreamTask', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed task: 11\n",
      "    Name: 11 (100.00%)\n",
      "    Description: 11 (100.00%)\n",
      "    Sub-task: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = tasks_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>translation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>summarization</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>DownstreamTask</td>\n",
       "      <td>sub-task</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id     entity name attribute name available API  \\\n",
       "0        text-classification  DownstreamTask           name         False   \n",
       "1        text-classification  DownstreamTask    description         False   \n",
       "2        text-classification  DownstreamTask       sub-task         False   \n",
       "3       token-classification  DownstreamTask           name         False   \n",
       "4       token-classification  DownstreamTask    description         False   \n",
       "5       token-classification  DownstreamTask       sub-task         False   \n",
       "6   table-question-answering  DownstreamTask           name         False   \n",
       "7   table-question-answering  DownstreamTask    description         False   \n",
       "8   table-question-answering  DownstreamTask       sub-task         False   \n",
       "9         question-answering  DownstreamTask           name         False   \n",
       "10        question-answering  DownstreamTask    description         False   \n",
       "11        question-answering  DownstreamTask       sub-task         False   \n",
       "12  zero-shot-classification  DownstreamTask           name         False   \n",
       "13  zero-shot-classification  DownstreamTask    description         False   \n",
       "14  zero-shot-classification  DownstreamTask       sub-task         False   \n",
       "15               translation  DownstreamTask           name         False   \n",
       "16               translation  DownstreamTask    description         False   \n",
       "17               translation  DownstreamTask       sub-task         False   \n",
       "18             summarization  DownstreamTask           name         False   \n",
       "19             summarization  DownstreamTask    description         False   \n",
       "20             summarization  DownstreamTask       sub-task         False   \n",
       "21        feature-extraction  DownstreamTask           name         False   \n",
       "22        feature-extraction  DownstreamTask    description         False   \n",
       "23        feature-extraction  DownstreamTask       sub-task         False   \n",
       "24           text-generation  DownstreamTask           name         False   \n",
       "25           text-generation  DownstreamTask    description         False   \n",
       "26           text-generation  DownstreamTask       sub-task         False   \n",
       "27                 fill-mask  DownstreamTask           name         False   \n",
       "28                 fill-mask  DownstreamTask    description         False   \n",
       "29                 fill-mask  DownstreamTask       sub-task         False   \n",
       "30       sentence-similarity  DownstreamTask           name         False   \n",
       "31       sentence-similarity  DownstreamTask    description         False   \n",
       "32       sentence-similarity  DownstreamTask       sub-task         False   \n",
       "\n",
       "   available scraping  \n",
       "0                True  \n",
       "1                True  \n",
       "2               False  \n",
       "3                True  \n",
       "4                True  \n",
       "5               False  \n",
       "6                True  \n",
       "7                True  \n",
       "8               False  \n",
       "9                True  \n",
       "10               True  \n",
       "11              False  \n",
       "12               True  \n",
       "13               True  \n",
       "14              False  \n",
       "15               True  \n",
       "16               True  \n",
       "17              False  \n",
       "18               True  \n",
       "19               True  \n",
       "20              False  \n",
       "21               True  \n",
       "22               True  \n",
       "23              False  \n",
       "24               True  \n",
       "25               True  \n",
       "26              False  \n",
       "27               True  \n",
       "28               True  \n",
       "29              False  \n",
       "30               True  \n",
       "31               True  \n",
       "32              False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "url_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "response = requests.get(url_metrics)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "h4_tags = soup.find_all('h4')\n",
    "metrics = [h4_tag.get_text(strip=True) for h4_tag in h4_tags]\n",
    "# print(metrics)\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "descriptions = [p_tag.get_text() for p_tag in p_tags]\n",
    "descriptions = descriptions[2:] # drop first lines\n",
    "# print(descriptions)\n",
    "\n",
    "# remove from the list the metrics withoud description (not useful for our purpose)\n",
    "metrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "metrics.remove('Aye10032/loss_metric')\n",
    "metrics.remove('giulio98/code_eval_outputs')\n",
    "metrics.remove('maysonma/lingo_judge_metric')\n",
    "metrics.remove('lvwerra/test')\n",
    "metrics.remove('sma2023/wil')\n",
    "\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "# From the lists, remove the descriptions and then the relative metric in the same index that have in the description 'TODO: add a description here\\n\\t\\t\\t\\t\\t\\t'ArithmeticError\n",
    "\n",
    "for i, description in enumerate(descriptions):\n",
    "    if 'TODO: add a description here' in description:\n",
    "        metrics.pop(i)\n",
    "        descriptions.pop(i)\n",
    "\n",
    "assert len(metrics) == len(descriptions)\n",
    "print(len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json(metrics, descriptions):\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    \n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "    \n",
    "        metrics_data.append(metric_attributes)\n",
    "        \n",
    "\n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Metric.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json(metrics, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON and count available attributes\n",
    "\n",
    "def metric_statistics():\n",
    "\tname_count = 0\n",
    "\tdescription_count = 0\n",
    "\tcontext_count = 0\n",
    "\tfeatureBased_endToEnd_count = 0\n",
    "\tgranularity_count = 0\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tmetric_json = open(os.path.join(result_path, 'ChatIMPACT.Metric.json'))\n",
    "\tmetric_data_json = json.load(metric_json)\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\tif item['name'] is not None:\n",
    "\t\t\tname_count += 1\n",
    "\t\tif item['description'] is not None:\n",
    "\t\t\tdescription_count += 1\n",
    "\t\tif item['context'] is not None:\n",
    "\t\t\tcontext_count += 1\n",
    "\t\tif item['featureBased/endToEnd'] is not None:\n",
    "\t\t\tfeatureBased_endToEnd_count += 1\n",
    "\t\tif item['granularity'] is not None:\t\n",
    "\t\t\tgranularity_count += 1\n",
    "\t\n",
    "\ttotal_datasets = idx + 1\n",
    "\n",
    "\tprint(f'Number of processed datasets: {total_datasets}')\n",
    "\tprint(f'    Name: {name_count} ({(name_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Description: {description_count} ({(description_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Context: {context_count} ({(context_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    FeatureBased/endToEnd: {featureBased_endToEnd_count} ({(featureBased_endToEnd_count / total_datasets) * 100:.2f}%)')\n",
    "\tprint(f'    Granularity: {granularity_count} ({(granularity_count / total_datasets) * 100:.2f}%)')\n",
    "\t\n",
    "\tavailability = pd.DataFrame(columns=['id', 'entity name', 'attribute name', 'available API', 'available scraping'])\n",
    "\n",
    "\tmetric_attributes = metric_data_json[0].keys()\n",
    "\n",
    "\tfor idx, item in enumerate(metric_data_json):\n",
    "\t\ttask_name = item['name']\n",
    "\t\tfor attr in metric_attributes:\n",
    "\t\t\tif item[attr] is not None and type(item[attr]) != list:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True) # all llm attributes we are able to extract come from API, no attribute is obtained by scraping\n",
    "\t\t\telif item[attr] is not None and type(item[attr]) == list and len(item[attr]) > 0:\n",
    "\t\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': True}])], ignore_index=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tavailability = pd.concat([availability, pd.DataFrame([{'id': task_name, 'entity name': 'Metric', 'attribute name': attr, 'available API': False, 'available scraping': False}])], ignore_index=True)\n",
    "\n",
    "\treturn availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed datasets: 218\n",
      "    Name: 218 (100.00%)\n",
      "    Description: 218 (100.00%)\n",
      "    Context: 0 (0.00%)\n",
      "    FeatureBased/endToEnd: 0 (0.00%)\n",
      "    Granularity: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "availability = metric_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity name</th>\n",
       "      <th>attribute name</th>\n",
       "      <th>available API</th>\n",
       "      <th>available scraping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>featureBased/endToEnd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>Metric</td>\n",
       "      <td>granularity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>name</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>description</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>featureBased/endToEnd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bertscore</td>\n",
       "      <td>Metric</td>\n",
       "      <td>granularity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id entity name         attribute name available API  \\\n",
       "0   accuracy      Metric                   name         False   \n",
       "1   accuracy      Metric            description         False   \n",
       "2   accuracy      Metric                context         False   \n",
       "3   accuracy      Metric  featureBased/endToEnd         False   \n",
       "4   accuracy      Metric            granularity         False   \n",
       "5  bertscore      Metric                   name         False   \n",
       "6  bertscore      Metric            description         False   \n",
       "7  bertscore      Metric                context         False   \n",
       "8  bertscore      Metric  featureBased/endToEnd         False   \n",
       "9  bertscore      Metric            granularity         False   \n",
       "\n",
       "  available scraping  \n",
       "0               True  \n",
       "1               True  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  \n",
       "5               True  \n",
       "6               True  \n",
       "7              False  \n",
       "8              False  \n",
       "9              False  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_relationship():\n",
    "    \n",
    "    train_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        datasets = match_dataset(model_tags)\n",
    "        \n",
    "        if not datasets:\n",
    "            continue\n",
    "        else:\n",
    "            train_relationship = dict()\n",
    "            train_relationship['Models'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            train_relationship['Datasets'] = datasets # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            train_relationship_list.append(train_relationship)\n",
    "    \n",
    "    return train_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\ttrain_relationship = extract_train_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.TrainRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(train_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_suited_for_relationship():\n",
    "    \n",
    "    suited_for_relationship_list = []\n",
    "    \n",
    "    for model_idx in range(len(models_df)):\n",
    "        \n",
    "        model = models_df.loc[model_idx]\n",
    "        model_tags = models_df.loc[model_idx]['tags']\n",
    "        \n",
    "        tasks = []\n",
    "        for t in model_tags:\n",
    "            if t in TAG_DOWNSTREAM_TASK:\n",
    "                tasks.append(t)\n",
    "\n",
    "        if not tasks:\n",
    "            continue\n",
    "        else:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model['id']) # {\"$oid\": <...>} for MongoDB\n",
    "            suited_for_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "            suited_for_relationship_list.append(suited_for_relationship)\n",
    "    \n",
    "    return suited_for_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tsuited_for_relationship = extract_suited_for_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.SuitedForRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(suited_for_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enable_relationship():\n",
    "\t\n",
    "\tenable_relationship_list = []\n",
    "\t\n",
    "\tfor dataset_idx in range(len(datasets_df)):\n",
    "\t\t\n",
    "\t\tdataset = datasets_df.loc[dataset_idx]\n",
    "\t\tdataset_tags = datasets_df.loc[dataset_idx]['tags']\n",
    "\t\t\n",
    "\t\ttasks = match_tasks(dataset_tags)\n",
    "\n",
    "\t\tif not tasks:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset['id']) # {\"$oid\": <...>} for MongoDB\n",
    "\t\t\tenable_relationship['DownstreamTask'] = tasks # [{\"$oid\": <...>}, ..., {\"$oid\": <...>}] for MongoDB\n",
    "\t\t\tenable_relationship_list.append(enable_relationship)\n",
    "\t\n",
    "\treturn enable_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tenable_relationship = extract_enable_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EnableRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(enable_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enable_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: here https://huggingface.co/tasks some tasks have associated metrics, we could scrape the tasks one by one\n",
    "\n",
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\\n\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "    return assess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.AssessRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task: text-classification\n",
      "\n",
      "Processing task: token-classification\n",
      "\n",
      "Processing task: table-question-answering\n",
      "\n",
      "Processing task: question-answering\n",
      "\n",
      "Processing task: zero-shot-classification\n",
      "\n",
      "Processing task: translation\n",
      "\n",
      "Processing task: summarization\n",
      "\n",
      "Processing task: feature-extraction\n",
      "\n",
      "Processing task: text-generation\n",
      "\n",
      "Processing task: fill-mask\n",
      "\n",
      "Processing task: sentence-similarity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def extract_evaluate_relationship():\n",
    "\n",
    "\tevaluate_relationship_list = []\n",
    "\n",
    "\tfor model_idx in range(len(models_df)):\n",
    "\n",
    "\t\tmodel = models_df.loc[model_idx]\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tmodel_card_data = next(api.list_models(model_name=model['id'], full=True, cardData=True)).card_data.to_dict()\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tprint('No card data available for this model')\n",
    "\t\t\n",
    "\t\tmetrics = []\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\n",
    "\t\tif not metrics:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tevaluate_relationship = dict()\n",
    "\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model['id'])\n",
    "\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\tevaluate_relationship_list.append(evaluate_relationship)\n",
    "\t\n",
    "\treturn evaluate_relationship_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluate_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\t\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tevaluate_relationship = extract_evaluate_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.EvaluateRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(evaluate_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

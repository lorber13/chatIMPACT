{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub.utils import logging\n",
    "\n",
    "from tags import * # tags.py\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?\n",
    "\n",
    "tag_language = set(tag_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)') # TODO: use DOI instead of arXiv?\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri\n",
    "\n",
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')\n",
    "\n",
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all models\n",
    "\n",
    "# models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 1000 models\n",
    "\n",
    "# model = itertools.islice(models, 0, 1000)\n",
    "# models_df = pd.DataFrame(model)\n",
    "# models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model):\n",
    "\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['id'] = model.id\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # extracted in a postprocessing step\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] \n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\tmodel_attributes['likes'] = model.likes\n",
    "\tmodel_attributes['downloads'] = model.downloads\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing token-classification models...\n",
      "1000 models processed, 1.0878269672393799 seconds elapsed, estimated time remaining: 757.3165774698257 seconds\n",
      "2000 models processed, 2.522703170776367 seconds elapsed, estimated time remaining: 876.8450167179108 seconds\n",
      "3000 models processed, 3.8365001678466797 seconds elapsed, estimated time remaining: 887.7182051753997 seconds\n",
      "4000 models processed, 5.370304107666016 seconds elapsed, estimated time remaining: 930.623386335969 seconds\n",
      "5000 models processed, 6.618153095245361 seconds elapsed, estimated time remaining: 916.16803772192 seconds\n",
      "6000 models processed, 7.742092132568359 seconds elapsed, estimated time remaining: 891.8405297079087 seconds\n",
      "7000 models processed, 8.940644264221191 seconds elapsed, estimated time remaining: 881.4991540736471 seconds\n",
      "8000 models processed, 10.21143102645874 seconds elapsed, estimated time remaining: 879.66654563421 seconds\n",
      "9000 models processed, 11.39786410331726 seconds elapsed, estimated time remaining: 871.5087877760992 seconds\n",
      "10000 models processed, 12.61611819267273 seconds elapsed, estimated time remaining: 866.9320449989319 seconds\n",
      "11000 models processed, 14.12074613571167 seconds elapsed, estimated time remaining: 880.8293495855766 seconds\n",
      "12000 models processed, 15.089762210845947 seconds elapsed, estimated time remaining: 861.5779102512201 seconds\n",
      "13000 models processed, 16.057276964187622 seconds elapsed, estimated time remaining: 845.0601777650025 seconds\n",
      "14000 models processed, 17.310059070587158 seconds elapsed, estimated time remaining: 844.6840959796565 seconds\n",
      "15000 models processed, 18.881100177764893 seconds elapsed, estimated time remaining: 858.6647882897377 seconds\n",
      "16000 models processed, 20.05589509010315 seconds elapsed, estimated time remaining: 853.8323138625026 seconds\n",
      "17000 models processed, 21.260287046432495 seconds elapsed, estimated time remaining: 850.6142897451346 seconds\n",
      "18000 models processed, 22.570364952087402 seconds elapsed, estimated time remaining: 851.6076535520025 seconds\n",
      "19000 models processed, 24.09822916984558 seconds elapsed, estimated time remaining: 860.1319308260868 seconds\n",
      "Processing feature-extraction models...\n",
      "20000 models processed, 26.329992055892944 seconds elapsed, estimated time remaining: 891.4839480096341 seconds\n",
      "21000 models processed, 27.507529973983765 seconds elapsed, estimated time remaining: 885.6928585180782 seconds\n",
      "22000 models processed, 29.035470247268677 seconds elapsed, estimated time remaining: 891.0749719322595 seconds\n",
      "23000 models processed, 30.94891905784607 seconds elapsed, estimated time remaining: 907.1560237070996 seconds\n",
      "24000 models processed, 32.27117919921875 seconds elapsed, estimated time remaining: 905.1556209368905 seconds\n",
      "25000 models processed, 33.583914041519165 seconds elapsed, estimated time remaining: 902.9533742242241 seconds\n",
      "26000 models processed, 36.35054612159729 seconds elapsed, estimated time remaining: 938.350330631953 seconds\n",
      "27000 models processed, 39.53977918624878 seconds elapsed, estimated time remaining: 981.4097086151088 seconds\n",
      "28000 models processed, 42.56561803817749 seconds elapsed, estimated time remaining: 1017.2606003519297 seconds\n",
      "29000 models processed, 45.022250175476074 seconds elapsed, estimated time remaining: 1037.3158643825302 seconds\n",
      "30000 models processed, 47.38696503639221 seconds elapsed, estimated time remaining: 1053.826169056479 seconds\n",
      "31000 models processed, 49.82292318344116 seconds elapsed, estimated time remaining: 1070.6496844663775 seconds\n",
      "32000 models processed, 53.8104043006897 seconds elapsed, estimated time remaining: 1118.5199588389694 seconds\n",
      "33000 models processed, 55.90863490104675 seconds elapsed, estimated time remaining: 1125.2240683884042 seconds\n",
      "34000 models processed, 59.80326509475708 seconds elapsed, estimated time remaining: 1166.4487073227097 seconds\n",
      "35000 models processed, 63.57608723640442 seconds elapsed, estimated time remaining: 1202.79061721656 seconds\n",
      "Processing fill-mask models...\n",
      "36000 models processed, 68.30889511108398 seconds elapsed, estimated time remaining: 1254.534716886123 seconds\n",
      "37000 models processed, 69.81548118591309 seconds elapsed, estimated time remaining: 1245.6629956365534 seconds\n",
      "38000 models processed, 71.09416508674622 seconds elapsed, estimated time remaining: 1233.225667031464 seconds\n",
      "39000 models processed, 72.33612012863159 seconds elapsed, estimated time remaining: 1220.7407382404988 seconds\n",
      "40000 models processed, 73.59708404541016 seconds elapsed, estimated time remaining: 1209.1302558930518 seconds\n",
      "41000 models processed, 75.14173817634583 seconds elapsed, estimated time remaining: 1202.564838239693 seconds\n",
      "42000 models processed, 76.39849209785461 seconds elapsed, estimated time remaining: 1191.747432382697 seconds\n",
      "43000 models processed, 78.03533816337585 seconds elapsed, estimated time remaining: 1187.1571054656893 seconds\n",
      "44000 models processed, 79.36232209205627 seconds elapsed, estimated time remaining: 1178.1012652189297 seconds\n",
      "45000 models processed, 80.57151222229004 seconds elapsed, estimated time remaining: 1167.6818147480858 seconds\n",
      "46000 models processed, 82.16358709335327 seconds elapsed, estimated time remaining: 1163.082786584087 seconds\n",
      "47000 models processed, 84.2702841758728 seconds elapsed, estimated time remaining: 1165.7306594822153 seconds\n",
      "48000 models processed, 85.57889914512634 seconds elapsed, estimated time remaining: 1157.3869286888837 seconds\n",
      "Processing translation models...\n",
      "49000 models processed, 86.84217309951782 seconds elapsed, estimated time remaining: 1148.7306217117603 seconds\n",
      "50000 models processed, 88.53027319908142 seconds elapsed, estimated time remaining: 1145.8686349994564 seconds\n",
      "51000 models processed, 90.00233197212219 seconds elapsed, estimated time remaining: 1140.3154673433678 seconds\n",
      "52000 models processed, 91.40987396240234 seconds elapsed, estimated time remaining: 1134.118840773995 seconds\n",
      "53000 models processed, 92.31174397468567 seconds elapsed, estimated time remaining: 1121.9570000185965 seconds\n",
      "Processing zero-shot-classification models...\n",
      "Processing question-answering models...\n",
      "54000 models processed, 94.18149995803833 seconds elapsed, estimated time remaining: 1121.7401085721829 seconds\n",
      "55000 models processed, 95.02671122550964 seconds elapsed, estimated time remaining: 1109.5008140224718 seconds\n",
      "56000 models processed, 96.66782307624817 seconds elapsed, estimated time remaining: 1106.7810355245897 seconds\n",
      "57000 models processed, 98.61814308166504 seconds elapsed, estimated time remaining: 1107.5717704666956 seconds\n",
      "58000 models processed, 100.07697629928589 seconds elapsed, estimated time remaining: 1102.8517928054907 seconds\n",
      "59000 models processed, 101.62326002120972 seconds elapsed, estimated time remaining: 1099.1882382508213 seconds\n",
      "60000 models processed, 103.17613315582275 seconds elapsed, estimated time remaining: 1095.6652626542807 seconds\n",
      "61000 models processed, 104.78175806999207 seconds elapsed, estimated time remaining: 1092.7569954232154 seconds\n",
      "62000 models processed, 105.63901019096375 seconds elapsed, estimated time remaining: 1082.2240100861134 seconds\n",
      "63000 models processed, 107.22034096717834 seconds elapsed, estimated time remaining: 1079.286822214664 seconds\n",
      "64000 models processed, 108.3920521736145 seconds elapsed, estimated time remaining: 1072.339567379527 seconds\n",
      "65000 models processed, 109.79198908805847 seconds elapsed, estimated time remaining: 1067.789651147439 seconds\n",
      "66000 models processed, 110.35604310035706 seconds elapsed, estimated time remaining: 1055.3416902945548 seconds\n",
      "Processing summarization models...\n",
      "67000 models processed, 112.14933204650879 seconds elapsed, estimated time remaining: 1054.8096825861646 seconds\n",
      "68000 models processed, 113.41421699523926 seconds elapsed, estimated time remaining: 1049.3517374003704 seconds\n",
      "Processing sentence-similarity models...\n",
      "69000 models processed, 115.93382406234741 seconds elapsed, estimated time remaining: 1055.438048357473 seconds\n",
      "70000 models processed, 118.84960103034973 seconds elapsed, estimated time remaining: 1064.8279517352105 seconds\n",
      "71000 models processed, 128.8849012851715 seconds elapsed, estimated time remaining: 1136.659585130967 seconds\n",
      "72000 models processed, 132.42475128173828 seconds elapsed, estimated time remaining: 1149.8183888937035 seconds\n",
      "73000 models processed, 137.83199214935303 seconds elapsed, estimated time remaining: 1178.4862309611071 seconds\n",
      "74000 models processed, 144.9098620414734 seconds elapsed, estimated time remaining: 1220.3016483140057 seconds\n",
      "Processing text-generation models...\n",
      "75000 models processed, 154.90603113174438 seconds elapsed, estimated time remaining: 1285.021980191307 seconds\n",
      "76000 models processed, 156.0617392063141 seconds elapsed, estimated time remaining: 1275.5213836697403 seconds\n",
      "77000 models processed, 156.81258296966553 seconds elapsed, estimated time remaining: 1262.976731591076 seconds\n",
      "78000 models processed, 157.83272099494934 seconds elapsed, estimated time remaining: 1252.872132008834 seconds\n",
      "79000 models processed, 158.88334107398987 seconds elapsed, estimated time remaining: 1243.2360377213563 seconds\n",
      "80000 models processed, 159.72114515304565 seconds elapsed, estimated time remaining: 1232.1728059368074 seconds\n",
      "81000 models processed, 160.71938514709473 seconds elapsed, estimated time remaining: 1222.5824799505929 seconds\n",
      "82000 models processed, 162.11972522735596 seconds elapsed, estimated time remaining: 1216.2182531337505 seconds\n",
      "83000 models processed, 163.15270924568176 seconds elapsed, estimated time remaining: 1207.2553872807114 seconds\n",
      "84000 models processed, 164.28562426567078 seconds elapsed, estimated time remaining: 1199.2107722591843 seconds\n",
      "85000 models processed, 165.31033205986023 seconds elapsed, estimated time remaining: 1190.5494889340569 seconds\n",
      "86000 models processed, 166.26017808914185 seconds elapsed, estimated time remaining: 1181.5337909450698 seconds\n",
      "87000 models processed, 167.5208821296692 seconds elapsed, estimated time remaining: 1174.8836728385784 seconds\n",
      "88000 models processed, 168.39299893379211 seconds elapsed, estimated time remaining: 1165.6661274055514 seconds\n",
      "89000 models processed, 169.42342829704285 seconds elapsed, estimated time remaining: 1157.717902471237 seconds\n",
      "90000 models processed, 170.41324615478516 seconds elapsed, estimated time remaining: 1149.6494489084932 seconds\n",
      "91000 models processed, 171.75680589675903 seconds elapsed, estimated time remaining: 1144.0928808289832 seconds\n",
      "92000 models processed, 172.78110313415527 seconds elapsed, estimated time remaining: 1136.5278365738134 seconds\n",
      "93000 models processed, 173.6085922718048 seconds elapsed, estimated time remaining: 1127.8249161826564 seconds\n",
      "94000 models processed, 174.78420519828796 seconds elapsed, estimated time remaining: 1121.52333672245 seconds\n",
      "95000 models processed, 175.74104404449463 seconds elapsed, estimated time remaining: 1113.942965498804 seconds\n",
      "96000 models processed, 176.70233130455017 seconds elapsed, estimated time remaining: 1106.528434981977 seconds\n",
      "97000 models processed, 177.63959312438965 seconds elapsed, estimated time remaining: 1099.0983143670633 seconds\n",
      "98000 models processed, 179.5522849559784 seconds elapsed, estimated time remaining: 1097.7643922891275 seconds\n",
      "99000 models processed, 181.76771116256714 seconds elapsed, estimated time remaining: 1098.247885245405 seconds\n",
      "100000 models processed, 183.6686532497406 seconds elapsed, estimated time remaining: 1096.7994315941191 seconds\n",
      "101000 models processed, 186.01388812065125 seconds elapsed, estimated time remaining: 1097.9645005410073 seconds\n",
      "102000 models processed, 187.7582049369812 seconds elapsed, estimated time remaining: 1095.5544247143314 seconds\n",
      "103000 models processed, 189.17024207115173 seconds elapsed, estimated time remaining: 1091.2405081973216 seconds\n",
      "104000 models processed, 190.65094113349915 seconds elapsed, estimated time remaining: 1087.3740103860544 seconds\n",
      "105000 models processed, 192.09659719467163 seconds elapsed, estimated time remaining: 1083.355315741162 seconds\n",
      "106000 models processed, 193.4874119758606 seconds elapsed, estimated time remaining: 1079.0793205232621 seconds\n",
      "107000 models processed, 194.67334818840027 seconds elapsed, estimated time remaining: 1073.7272578880809 seconds\n",
      "108000 models processed, 195.8520700931549 seconds elapsed, estimated time remaining: 1068.4129602607886 seconds\n",
      "109000 models processed, 197.09723210334778 seconds elapsed, estimated time remaining: 1063.533074983238 seconds\n",
      "110000 models processed, 198.32557916641235 seconds elapsed, estimated time remaining: 1058.6295150391447 seconds\n",
      "111000 models processed, 200.1386592388153 seconds elapsed, estimated time remaining: 1056.8799961421685 seconds\n",
      "112000 models processed, 201.32968616485596 seconds elapsed, estimated time remaining: 1051.87932310115 seconds\n",
      "113000 models processed, 202.75387024879456 seconds elapsed, estimated time remaining: 1048.1514082989186 seconds\n",
      "114000 models processed, 204.21114301681519 seconds elapsed, estimated time remaining: 1044.6331710854538 seconds\n",
      "115000 models processed, 205.69391322135925 seconds elapsed, estimated time remaining: 1041.2798446047368 seconds\n",
      "116000 models processed, 207.58711314201355 seconds elapsed, estimated time remaining: 1040.0150410137505 seconds\n",
      "117000 models processed, 208.7036111354828 seconds elapsed, estimated time remaining: 1034.8880969943023 seconds\n",
      "118000 models processed, 209.85679697990417 seconds elapsed, estimated time remaining: 1030.0091973754309 seconds\n",
      "119000 models processed, 211.78048706054688 seconds elapsed, estimated time remaining: 1028.9364059097265 seconds\n",
      "120000 models processed, 213.2398190498352 seconds elapsed, estimated time remaining: 1025.6160289148013 seconds\n",
      "121000 models processed, 214.93835616111755 seconds elapsed, estimated time remaining: 1023.4654218778846 seconds\n",
      "122000 models processed, 216.0334780216217 seconds elapsed, estimated time remaining: 1018.4774558776246 seconds\n",
      "123000 models processed, 217.7061800956726 seconds elapsed, estimated time remaining: 1016.2489321203271 seconds\n",
      "124000 models processed, 219.76651811599731 seconds elapsed, estimated time remaining: 1015.8211284363155 seconds\n",
      "125000 models processed, 221.04776406288147 seconds elapsed, estimated time remaining: 1011.8010691715316 seconds\n",
      "126000 models processed, 222.48086094856262 seconds elapsed, estimated time remaining: 1008.5128239372117 seconds\n",
      "127000 models processed, 223.7447521686554 seconds elapsed, estimated time remaining: 1004.4941593753259 seconds\n",
      "128000 models processed, 224.91838121414185 seconds elapsed, estimated time remaining: 1000.1171750201509 seconds\n",
      "129000 models processed, 226.11752033233643 seconds elapsed, estimated time remaining: 995.9022073986586 seconds\n",
      "130000 models processed, 227.0636260509491 seconds elapsed, estimated time remaining: 990.6297173322421 seconds\n",
      "131000 models processed, 228.4357500076294 seconds elapsed, estimated time remaining: 987.2644574845328 seconds\n",
      "132000 models processed, 229.90312910079956 seconds elapsed, estimated time remaining: 984.3372354432851 seconds\n",
      "133000 models processed, 231.53783202171326 seconds elapsed, estimated time remaining: 982.1417234110869 seconds\n",
      "134000 models processed, 233.14671206474304 seconds elapsed, estimated time remaining: 979.8460558167073 seconds\n",
      "135000 models processed, 234.58395910263062 seconds elapsed, estimated time remaining: 976.845855049483 seconds\n",
      "136000 models processed, 235.7360782623291 seconds elapsed, estimated time remaining: 972.6921457770152 seconds\n",
      "137000 models processed, 236.83934712409973 seconds elapsed, estimated time remaining: 968.3825194769602 seconds\n",
      "138000 models processed, 238.35130500793457 seconds elapsed, estimated time remaining: 965.7753237477904 seconds\n",
      "139000 models processed, 239.81565117835999 seconds elapsed, estimated time remaining: 962.9927023182361 seconds\n",
      "140000 models processed, 241.12047505378723 seconds elapsed, estimated time remaining: 959.5940721932242 seconds\n",
      "141000 models processed, 242.55016922950745 seconds elapsed, estimated time remaining: 956.717658659634 seconds\n",
      "142000 models processed, 243.68629622459412 seconds elapsed, estimated time remaining: 952.7139030412452 seconds\n",
      "143000 models processed, 244.65027904510498 seconds elapsed, estimated time remaining: 948.0831518235407 seconds\n",
      "144000 models processed, 245.7180380821228 seconds elapsed, estimated time remaining: 943.90197327187 seconds\n",
      "145000 models processed, 246.6971571445465 seconds elapsed, estimated time remaining: 939.4261916522619 seconds\n",
      "146000 models processed, 247.93816018104553 seconds elapsed, estimated time remaining: 935.9869520634103 seconds\n",
      "147000 models processed, 249.18636393547058 seconds elapsed, estimated time remaining: 932.6045662665367 seconds\n",
      "148000 models processed, 250.16597604751587 seconds elapsed, estimated time remaining: 928.2543951874649 seconds\n",
      "149000 models processed, 251.2738480567932 seconds elapsed, estimated time remaining: 924.4213278056375 seconds\n",
      "150000 models processed, 252.51043725013733 seconds elapsed, estimated time remaining: 921.0941231715425 seconds\n",
      "151000 models processed, 254.40994119644165 seconds elapsed, estimated time remaining: 920.1923512466853 seconds\n",
      "152000 models processed, 256.7054331302643 seconds elapsed, estimated time remaining: 920.6976935249285 seconds\n",
      "153000 models processed, 258.5297040939331 seconds elapsed, estimated time remaining: 919.4904884399526 seconds\n",
      "154000 models processed, 260.2622711658478 seconds elapsed, estimated time remaining: 917.9518081200154 seconds\n",
      "155000 models processed, 262.0950231552124 seconds elapsed, estimated time remaining: 916.7610931680186 seconds\n",
      "156000 models processed, 263.51282119750977 seconds elapsed, estimated time remaining: 914.1226190414337 seconds\n",
      "157000 models processed, 264.7194950580597 seconds elapsed, estimated time remaining: 910.7733413632448 seconds\n",
      "158000 models processed, 266.5115599632263 seconds elapsed, estimated time remaining: 909.4487840391081 seconds\n",
      "159000 models processed, 267.5236370563507 seconds elapsed, estimated time remaining: 905.4783602921168 seconds\n",
      "160000 models processed, 268.5798192024231 seconds elapsed, estimated time remaining: 901.6929720741838 seconds\n",
      "161000 models processed, 269.6094081401825 seconds elapsed, estimated time remaining: 897.8529327435226 seconds\n",
      "162000 models processed, 270.79345417022705 seconds elapsed, estimated time remaining: 894.5578337808538 seconds\n",
      "163000 models processed, 272.2008662223816 seconds elapsed, estimated time remaining: 892.0206204953749 seconds\n",
      "164000 models processed, 273.2526330947876 seconds elapsed, estimated time remaining: 888.3409941189841 seconds\n",
      "165000 models processed, 274.39358019828796 seconds elapsed, estimated time remaining: 884.9808422001665 seconds\n",
      "166000 models processed, 275.3964641094208 seconds elapsed, estimated time remaining: 881.2056555551161 seconds\n",
      "167000 models processed, 276.55760526657104 seconds elapsed, estimated time remaining: 877.9660817445824 seconds\n",
      "168000 models processed, 277.5526010990143 seconds elapsed, estimated time remaining: 874.2279324809994 seconds\n",
      "169000 models processed, 278.6203520298004 seconds elapsed, estimated time remaining: 870.7496162075939 seconds\n",
      "170000 models processed, 279.46544003486633 seconds elapsed, estimated time remaining: 866.6091944984408 seconds\n",
      "171000 models processed, 280.55095195770264 seconds elapsed, estimated time remaining: 863.2470920689956 seconds\n",
      "172000 models processed, 281.42525601387024 seconds elapsed, estimated time remaining: 859.2665875194211 seconds\n",
      "173000 models processed, 282.30113101005554 seconds elapsed, estimated time remaining: 855.3267524961378 seconds\n",
      "174000 models processed, 283.1826570034027 seconds elapsed, estimated time remaining: 851.4391282844982 seconds\n",
      "175000 models processed, 284.17658400535583 seconds elapsed, estimated time remaining: 847.9212354071617 seconds\n",
      "176000 models processed, 285.0833160877228 seconds elapsed, estimated time remaining: 844.1738283423781 seconds\n",
      "177000 models processed, 286.0139012336731 seconds elapsed, estimated time remaining: 840.5286179312743 seconds\n",
      "178000 models processed, 287.3861792087555 seconds elapsed, estimated time remaining: 838.2021687973778 seconds\n",
      "179000 models processed, 288.61738300323486 seconds elapsed, estimated time remaining: 835.4780028597363 seconds\n",
      "180000 models processed, 289.9430410861969 seconds elapsed, estimated time remaining: 833.0418089085632 seconds\n",
      "181000 models processed, 291.1099452972412 seconds elapsed, estimated time remaining: 830.1651577542341 seconds\n",
      "182000 models processed, 292.3200809955597 seconds elapsed, estimated time remaining: 827.4296684277242 seconds\n",
      "183000 models processed, 293.7653388977051 seconds elapsed, estimated time remaining: 825.3714583344746 seconds\n",
      "184000 models processed, 295.3923752307892 seconds elapsed, estimated time remaining: 823.8268723104854 seconds\n",
      "185000 models processed, 296.62557315826416 seconds elapsed, estimated time remaining: 821.1910743439236 seconds\n",
      "186000 models processed, 297.7208790779114 seconds elapsed, estimated time remaining: 818.1914089409792 seconds\n",
      "187000 models processed, 298.7068910598755 seconds elapsed, estimated time remaining: 814.9139445603978 seconds\n",
      "188000 models processed, 299.8153212070465 seconds elapsed, estimated time remaining: 811.9923989590203 seconds\n",
      "189000 models processed, 300.80251026153564 seconds elapsed, estimated time remaining: 808.7640616012048 seconds\n",
      "190000 models processed, 302.26520705223083 seconds elapsed, estimated time remaining: 806.828576201459 seconds\n",
      "191000 models processed, 303.8686821460724 seconds elapsed, estimated time remaining: 805.2711121815038 seconds\n",
      "192000 models processed, 305.2706401348114 seconds elapsed, estimated time remaining: 803.182963743863 seconds\n",
      "193000 models processed, 306.4081082344055 seconds elapsed, estimated time remaining: 800.411019207779 seconds\n",
      "194000 models processed, 307.6522891521454 seconds elapsed, estimated time remaining: 797.9326966687921 seconds\n",
      "195000 models processed, 308.7638771533966 seconds elapsed, estimated time remaining: 795.1255794599998 seconds\n",
      "196000 models processed, 309.99534797668457 seconds elapsed, estimated time remaining: 792.642302098931 seconds\n",
      "197000 models processed, 310.9815969467163 seconds elapsed, estimated time remaining: 789.5491346156246 seconds\n",
      "198000 models processed, 311.94142603874207 seconds elapsed, estimated time remaining: 786.4106495128785 seconds\n",
      "199000 models processed, 313.51490902900696 seconds elapsed, estimated time remaining: 784.8302342005063 seconds\n",
      "200000 models processed, 314.49674701690674 seconds elapsed, estimated time remaining: 781.7791717406893 seconds\n",
      "201000 models processed, 316.00105810165405 seconds elapsed, estimated time remaining: 780.0384053433428 seconds\n",
      "202000 models processed, 317.0974190235138 seconds elapsed, estimated time remaining: 777.2999742363821 seconds\n",
      "203000 models processed, 318.568941116333 seconds elapsed, estimated time remaining: 775.4909702709742 seconds\n",
      "204000 models processed, 319.6637830734253 seconds elapsed, estimated time remaining: 772.7746718490943 seconds\n",
      "205000 models processed, 320.56754302978516 seconds elapsed, estimated time remaining: 769.6154401207575 seconds\n",
      "206000 models processed, 321.57077622413635 seconds elapsed, estimated time remaining: 766.7152812322436 seconds\n",
      "207000 models processed, 323.71192717552185 seconds elapsed, estimated time remaining: 766.5279595888478 seconds\n",
      "208000 models processed, 325.23046827316284 seconds elapsed, estimated time remaining: 764.8576355165472 seconds\n",
      "209000 models processed, 326.1406509876251 seconds elapsed, estimated time remaining: 761.7678231001562 seconds\n",
      "Processing text-classification models...\n",
      "210000 models processed, 326.89676213264465 seconds elapsed, estimated time remaining: 758.3413420373575 seconds\n",
      "211000 models processed, 327.8920929431915 seconds elapsed, estimated time remaining: 755.4913611198036 seconds\n",
      "212000 models processed, 328.6994471549988 seconds elapsed, estimated time remaining: 752.228689179182 seconds\n",
      "213000 models processed, 329.5710241794586 seconds elapsed, estimated time remaining: 749.1350569588768 seconds\n",
      "214000 models processed, 330.3204040527344 seconds elapsed, estimated time remaining: 745.786299531217 seconds\n",
      "215000 models processed, 331.01088428497314 seconds elapsed, estimated time remaining: 742.3296337354948 seconds\n",
      "216000 models processed, 332.2152409553528 seconds elapsed, estimated time remaining: 740.0432928698747 seconds\n",
      "217000 models processed, 333.25324392318726 seconds elapsed, estimated time remaining: 737.3988276353892 seconds\n",
      "218000 models processed, 334.7205410003662 seconds elapsed, estimated time remaining: 735.7126851006975 seconds\n",
      "219000 models processed, 335.92639422416687 seconds elapsed, estimated time remaining: 733.4577046257307 seconds\n",
      "220000 models processed, 336.96650290489197 seconds elapsed, estimated time remaining: 730.8527815365445 seconds\n",
      "221000 models processed, 337.9109890460968 seconds elapsed, estimated time remaining: 728.0559881805445 seconds\n",
      "222000 models processed, 338.9142789840698 seconds elapsed, estimated time remaining: 725.4017482133577 seconds\n",
      "223000 models processed, 340.056449174881 seconds elapsed, estimated time remaining: 723.0576122120956 seconds\n",
      "224000 models processed, 341.7279131412506 seconds elapsed, estimated time remaining: 721.842255387336 seconds\n",
      "225000 models processed, 342.8899042606354 seconds elapsed, estimated time remaining: 719.5537081172286 seconds\n",
      "226000 models processed, 343.96992015838623 seconds elapsed, estimated time remaining: 717.1042344336489 seconds\n",
      "227000 models processed, 344.94174695014954 seconds elapsed, estimated time remaining: 714.4427429005921 seconds\n",
      "228000 models processed, 345.8419840335846 seconds elapsed, estimated time remaining: 711.6487628064783 seconds\n",
      "229000 models processed, 346.7937443256378 seconds elapsed, estimated time remaining: 708.9766557152094 seconds\n",
      "230000 models processed, 348.295282125473 seconds elapsed, estimated time remaining: 707.4361822820063 seconds\n",
      "231000 models processed, 349.16836619377136 seconds elapsed, estimated time remaining: 704.6278149580997 seconds\n",
      "232000 models processed, 350.09043407440186 seconds elapsed, estimated time remaining: 701.9343445545661 seconds\n",
      "233000 models processed, 351.04620003700256 seconds elapsed, estimated time remaining: 699.3232091854857 seconds\n",
      "234000 models processed, 352.02298402786255 seconds elapsed, estimated time remaining: 696.7678299119656 seconds\n",
      "235000 models processed, 352.8885200023651 seconds elapsed, estimated time remaining: 694.0070877182008 seconds\n",
      "236000 models processed, 355.6443898677826 seconds elapsed, estimated time remaining: 694.9562692777585 seconds\n",
      "237000 models processed, 356.9222662448883 seconds elapsed, estimated time remaining: 693.0044944992167 seconds\n",
      "238000 models processed, 357.771870136261 seconds elapsed, estimated time remaining: 690.2321399014396 seconds\n",
      "239000 models processed, 359.09089708328247 seconds elapsed, estimated time remaining: 688.3757527208488 seconds\n",
      "240000 models processed, 359.86780405044556 seconds elapsed, estimated time remaining: 685.4911935510695 seconds\n",
      "241000 models processed, 360.72454619407654 seconds elapsed, estimated time remaining: 682.7752359121569 seconds\n",
      "242000 models processed, 361.6688041687012 seconds elapsed, estimated time remaining: 680.2392442611032 seconds\n",
      "243000 models processed, 362.673406124115 seconds elapsed, estimated time remaining: 677.8291371950024 seconds\n",
      "244000 models processed, 363.58619594573975 seconds elapsed, estimated time remaining: 675.2600356538668 seconds\n",
      "245000 models processed, 364.364627122879 seconds elapsed, estimated time remaining: 672.4564912726538 seconds\n",
      "246000 models processed, 365.0633502006531 seconds elapsed, estimated time remaining: 669.5232254556931 seconds\n",
      "247000 models processed, 365.8144881725311 seconds elapsed, estimated time remaining: 666.7035780213063 seconds\n",
      "248000 models processed, 366.58064699172974 seconds elapsed, estimated time remaining: 663.9278160669554 seconds\n",
      "249000 models processed, 367.55982422828674 seconds elapsed, estimated time remaining: 661.5515952931695 seconds\n",
      "250000 models processed, 368.5382421016693 seconds elapsed, estimated time remaining: 659.1851964818173 seconds\n",
      "251000 models processed, 369.5057532787323 seconds elapsed, estimated time remaining: 656.81046851123 seconds\n",
      "252000 models processed, 371.1852219104767 seconds elapsed, estimated time remaining: 655.7045955995208 seconds\n",
      "253000 models processed, 372.50757002830505 seconds elapsed, estimated time remaining: 653.9672313942212 seconds\n",
      "254000 models processed, 373.8789072036743 seconds elapsed, estimated time remaining: 652.3186099041184 seconds\n",
      "255000 models processed, 375.0061960220337 seconds elapsed, estimated time remaining: 650.248982722632 seconds\n",
      "256000 models processed, 376.7875609397888 seconds elapsed, estimated time remaining: 649.313887994824 seconds\n",
      "257000 models processed, 377.7100040912628 seconds elapsed, estimated time remaining: 646.9011369069059 seconds\n",
      "258000 models processed, 378.69264101982117 seconds elapsed, estimated time remaining: 644.6023991345102 seconds\n",
      "259000 models processed, 379.7375831604004 seconds elapsed, estimated time remaining: 642.4192300547596 seconds\n",
      "260000 models processed, 381.5746681690216 seconds elapsed, estimated time remaining: 641.576717081015 seconds\n",
      "261000 models processed, 382.5827651023865 seconds elapsed, estimated time remaining: 639.3412497178495 seconds\n",
      "262000 models processed, 383.6106870174408 seconds elapsed, estimated time remaining: 637.1480746093342 seconds\n",
      "263000 models processed, 384.7359039783478 seconds elapsed, estimated time remaining: 635.1243795449344 seconds\n",
      "264000 models processed, 386.1913731098175 seconds elapsed, estimated time remaining: 633.649355029341 seconds\n",
      "265000 models processed, 387.2641432285309 seconds elapsed, estimated time remaining: 631.5503709980965 seconds\n",
      "266000 models processed, 388.6150860786438 seconds elapsed, estimated time remaining: 629.9099949962913 seconds\n",
      "267000 models processed, 389.9363090991974 seconds elapsed, estimated time remaining: 628.2239113791212 seconds\n",
      "268000 models processed, 390.94273614883423 seconds elapsed, estimated time remaining: 626.0364483889146 seconds\n",
      "269000 models processed, 391.9217622280121 seconds elapsed, estimated time remaining: 623.8141546494633 seconds\n",
      "270000 models processed, 393.4570770263672 seconds elapsed, estimated time remaining: 622.4811617336079 seconds\n",
      "271000 models processed, 395.7949450016022 seconds elapsed, estimated time remaining: 622.4087361087693 seconds\n",
      "272000 models processed, 396.6976079940796 seconds elapsed, estimated time remaining: 620.0762891178025 seconds\n",
      "273000 models processed, 397.7700822353363 seconds elapsed, estimated time remaining: 618.0181525409039 seconds\n",
      "274000 models processed, 399.5934331417084 seconds elapsed, estimated time remaining: 617.1268539027245 seconds\n",
      "275000 models processed, 402.15261602401733 seconds elapsed, estimated time remaining: 617.3583796254643 seconds\n",
      "276000 models processed, 403.97930908203125 seconds elapsed, estimated time remaining: 616.4519397761304 seconds\n",
      "277000 models processed, 405.14861607551575 seconds elapsed, estimated time remaining: 614.5417145530101 seconds\n",
      "Processing table-question-answering models...\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(result_path, 'models_data_no_modelCreator.json')\n",
    "\n",
    "# Total: 697,162 models\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} models...')\n",
    "    models = api.list_models(filter=task, full=True, cardData=True)\n",
    "    for model in models:\n",
    "        model_attributes = extract_model_attributes(model)\n",
    "        add_to_json_file(model_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'models_data_no_modelCreator.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "models_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before removing duplicates: 277497\n",
      "len after removing duplicates: 269335\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(f'len before removing duplicates: {  len(models_df) }')\n",
    "models_df = models_df.loc[models_df.astype(str).drop_duplicates().index]\n",
    "print(f'len after removing duplicates: {  len(models_df) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 rows processed (1.8459380134015098 %), elapsed time: 23.079020977020264 seconds, estimated time remaining: 1227.1808838305474 seconds\n",
      "2000 rows processed (3.6918760268030195 %), elapsed time: 46.008147954940796 seconds, estimated time remaining: 1200.191576504588 seconds\n",
      "3000 rows processed (5.5378140402045295 %), elapsed time: 69.48612809181213 seconds, estimated time remaining: 1185.2715322297415 seconds\n",
      "4000 rows processed (7.383752053606039 %), elapsed time: 92.65146589279175 seconds, estimated time remaining: 1162.1505115219354 seconds\n",
      "5000 rows processed (9.22969006700755 %), elapsed time: 115.6754059791565 seconds, estimated time remaining: 1137.621357021618 seconds\n",
      "6000 rows processed (11.075628080409059 %), elapsed time: 138.77773475646973 seconds, estimated time remaining: 1114.223312308351 seconds\n",
      "7000 rows processed (12.921566093810569 %), elapsed time: 162.82808685302734 seconds, estimated time remaining: 1097.2985174701214 seconds\n",
      "8000 rows processed (14.767504107212078 %), elapsed time: 186.07719898223877 seconds, estimated time remaining: 1073.967865866244 seconds\n",
      "9000 rows processed (16.613442120613588 %), elapsed time: 209.4672989845276 seconds, estimated time remaining: 1051.3629362521172 seconds\n",
      "10000 rows processed (18.4593801340151 %), elapsed time: 232.24536275863647 seconds, estimated time remaining: 1025.897446179557 seconds\n",
      "11000 rows processed (20.30531814741661 %), elapsed time: 255.26186990737915 seconds, estimated time remaining: 1001.8564393663839 seconds\n",
      "12000 rows processed (22.151256160818118 %), elapsed time: 278.0063409805298 seconds, estimated time remaining: 977.030120694697 seconds\n",
      "13000 rows processed (23.99719417421963 %), elapsed time: 300.6624000072479 seconds, estimated time remaining: 952.2440795972346 seconds\n",
      "14000 rows processed (25.843132187621137 %), elapsed time: 323.34765696525574 seconds, estimated time remaining: 927.8461071349383 seconds\n",
      "15000 rows processed (27.68907020102265 %), elapsed time: 346.15903091430664 seconds, estimated time remaining: 904.0058534708182 seconds\n",
      "16000 rows processed (29.535008214424156 %), elapsed time: 368.70374584198 seconds, estimated time remaining: 879.658008470729 seconds\n",
      "17000 rows processed (31.380946227825667 %), elapsed time: 391.4004719257355 seconds, estimated time remaining: 855.8546928439 seconds\n",
      "18000 rows processed (33.226884241227175 %), elapsed time: 414.78436398506165 seconds, estimated time remaining: 833.5552843072944 seconds\n",
      "19000 rows processed (35.07282225462869 %), elapsed time: 437.68363904953003 seconds, estimated time remaining: 810.2445633355944 seconds\n",
      "20000 rows processed (36.9187602680302 %), elapsed time: 460.47793793678284 seconds, estimated time remaining: 786.7956302851796 seconds\n",
      "21000 rows processed (38.764698281431706 %), elapsed time: 483.1694118976593 seconds, estimated time remaining: 763.2466162107786 seconds\n",
      "22000 rows processed (40.61063629483322 %), elapsed time: 505.9753336906433 seconds, estimated time remaining: 739.9429309029472 seconds\n",
      "23000 rows processed (42.45657430823473 %), elapsed time: 528.655678987503 seconds, estimated time remaining: 716.5123265567966 seconds\n",
      "24000 rows processed (44.302512321636236 %), elapsed time: 551.3700799942017 seconds, estimated time remaining: 693.1870617173116 seconds\n",
      "25000 rows processed (46.14845033503775 %), elapsed time: 574.150643825531 seconds, estimated time remaining: 669.9878740225506 seconds\n",
      "26000 rows processed (47.99438834843926 %), elapsed time: 596.8870158195496 seconds, estimated time remaining: 646.7729983514181 seconds\n",
      "27000 rows processed (49.840326361840766 %), elapsed time: 619.6925549507141 seconds, estimated time remaining: 623.6631785774055 seconds\n",
      "28000 rows processed (51.686264375242274 %), elapsed time: 642.5171530246735 seconds, estimated time remaining: 600.5929096812606 seconds\n",
      "29000 rows processed (53.53220238864379 %), elapsed time: 665.2690539360046 seconds, estimated time remaining: 577.4764799564789 seconds\n",
      "30000 rows processed (55.3781404020453 %), elapsed time: 688.0590019226074 seconds, estimated time remaining: 554.415009986051 seconds\n",
      "31000 rows processed (57.22407841544681 %), elapsed time: 710.7906427383423 seconds, estimated time remaining: 531.3274721289989 seconds\n",
      "32000 rows processed (59.07001642884831 %), elapsed time: 733.4752957820892 seconds, estimated time remaining: 508.2296174940169 seconds\n",
      "33000 rows processed (60.915954442249834 %), elapsed time: 756.3860898017883 seconds, estimated time remaining: 485.3019001398014 seconds\n",
      "34000 rows processed (62.761892455651335 %), elapsed time: 779.177937746048 seconds, estimated time remaining: 462.3046047705622 seconds\n",
      "35000 rows processed (64.60783046905286 %), elapsed time: 802.0415098667145 seconds, estimated time remaining: 439.358339627409 seconds\n",
      "36000 rows processed (66.45376848245435 %), elapsed time: 824.8511710166931 seconds, estimated time remaining: 416.38945411715247 seconds\n",
      "37000 rows processed (68.29970649585587 %), elapsed time: 847.8169639110565 seconds, estimated time remaining: 393.50164244261947 seconds\n",
      "38000 rows processed (70.14564450925738 %), elapsed time: 870.8629307746887 seconds, estimated time remaining: 370.6438481459555 seconds\n",
      "39000 rows processed (71.99158252265889 %), elapsed time: 893.622683763504 seconds, estimated time remaining: 347.6650516231121 seconds\n",
      "40000 rows processed (73.8375205360604 %), elapsed time: 916.3814649581909 seconds, estimated time remaining: 324.69686290922164 seconds\n",
      "41000 rows processed (75.68345854946192 %), elapsed time: 939.167417049408 seconds, estimated time remaining: 301.74761944767323 seconds\n",
      "42000 rows processed (77.52939656286341 %), elapsed time: 961.8322069644928 seconds, estimated time remaining: 278.7710352046887 seconds\n",
      "43000 rows processed (79.37533457626493 %), elapsed time: 984.7266368865967 seconds, estimated time remaining: 255.86862156402788 seconds\n",
      "44000 rows processed (81.22127258966644 %), elapsed time: 1007.4636940956116 seconds, estimated time remaining: 232.93018606169116 seconds\n",
      "45000 rows processed (83.06721060306795 %), elapsed time: 1030.1760039329529 seconds, estimated time remaining: 209.9956554447121 seconds\n",
      "46000 rows processed (84.91314861646946 %), elapsed time: 1052.981158733368 seconds, estimated time remaining: 187.08728304501722 seconds\n",
      "47000 rows processed (86.75908662987098 %), elapsed time: 1075.807401895523 seconds, estimated time remaining: 164.18652129015518 seconds\n",
      "48000 rows processed (88.60502464327247 %), elapsed time: 1099.0808067321777 seconds, estimated time remaining: 141.34637204632165 seconds\n",
      "49000 rows processed (90.450962656674 %), elapsed time: 1122.647656917572 seconds, estimated time remaining: 118.51951725036757 seconds\n",
      "50000 rows processed (92.2969006700755 %), elapsed time: 1147.1160428524017 seconds, estimated time remaining: 95.7383052747345 seconds\n",
      "51000 rows processed (94.14283868347701 %), elapsed time: 1170.8073289394379 seconds, estimated time remaining: 72.84258183602726 seconds\n",
      "52000 rows processed (95.98877669687852 %), elapsed time: 1194.2307398319244 seconds, estimated time remaining: 49.90506537936742 seconds\n",
      "53000 rows processed (97.83471471028002 %), elapsed time: 1217.1621780395508 seconds, estimated time remaining: 26.938325248988168 seconds\n",
      "54000 rows processed (99.68065272368153 %), elapsed time: 1240.325360774994 seconds, estimated time remaining: 3.9736349551677703 seconds\n"
     ]
    }
   ],
   "source": [
    "# Postprocessing: find the modelCreator\n",
    "\n",
    "df_filtered = models_df[models_df['architecture'].notna()]\n",
    "\n",
    "# Process each row\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for index, row in df_filtered.iterrows():\n",
    "    # Find the row where 'id' matches the 'architecture' of the current row\n",
    "    try:\n",
    "        matching_row = models_df[models_df['id'].astype(str) == str(row['architecture'])]\n",
    "    except ValueError:\n",
    "        break\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        # Get the first developer from the 'developers' list\n",
    "        first_developer = matching_row['developers'].iloc[0][0] if matching_row['developers'].iloc[0] else None\n",
    "        # Set the 'modelCreator' attribute of the original row\n",
    "        models_df.at[index, 'modelCreator'] = first_developer\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} rows processed ({count/len(df_filtered)*100} %), elapsed time: {time.time() - start_time} seconds, estimated time remaining: {(time.time() - start_time) / count * (len(df_filtered) - count)} seconds')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = models_df.drop(columns=['id']).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'models_data.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "    json.dump(models_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all datasets\n",
    "\n",
    "# datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 1000 models\n",
    "\n",
    "# datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "# datasets_df = pd.DataFrame(datasets)\n",
    "# datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset):\n",
    "\n",
    "\tdataset_tags = dataset.tags\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset.id)\n",
    "\tdataset_attributes['size [GB]'] = match_size(dataset_tags)\n",
    "\n",
    "\t# url = \"https://huggingface.co/datasets/\" + dataset.id\n",
    "\t# file_size_str = extract_file_size(url)\n",
    "\t# if file_size_str:\n",
    "\t# \tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t# \tif file_size_gb:\n",
    "\t# \t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence-similarity datasets...\n",
      "Processing summarization datasets...\n",
      "Processing text-classification datasets...\n",
      "Processing question-answering datasets...\n",
      "Processing feature-extraction datasets...\n",
      "Processing zero-shot-classification datasets...\n",
      "Processing token-classification datasets...\n",
      "Processing text-generation datasets...\n",
      "Processing translation datasets...\n",
      "Processing fill-mask datasets...\n",
      "Processing table-question-answering datasets...\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(result_path, 'datasets_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} datasets...')\n",
    "    datasets = api.list_datasets(filter=task, full=True)\n",
    "    for dataset in datasets:\n",
    "        dataset_attributes = extract_datasets_attributes(dataset)\n",
    "        add_to_json_file(dataset_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} datasets processed, {time.time() - start_time} seconds elapsed, estimated time remaining: {(time.time() - start_time) / count * (199642 - count):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'nlp_datasets_data.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "datasets_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before removing duplicates: 773\n",
      "len after removing duplicates: 489\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(f'len before removing duplicates: {  len(datasets_df) }')\n",
    "datasets_df = datasets_df.loc[datasets_df.astype(str).drop_duplicates().index]\n",
    "print(f'len after removing duplicates: {  len(datasets_df) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = datasets_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'nlp_datasets_no_duplicates.json')\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "    json.dump(datasets_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description, # TODO: text2text generation has no description\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "\n",
    "    with open(result_path + '/ChatIMPACT.DownstreamTask.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tasks_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "def scrape_metrics():\n",
    "\turl_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "\tresponse = requests.get(url_metrics)\n",
    "\thtml_content = response.text\n",
    "\n",
    "\tsoup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\th4_tags = soup.find_all('h4')\n",
    "\tmetrics = [h4_tag.get_text(strip=True) for h4_tag in h4_tags]\n",
    "\t# print(metrics)\n",
    "\n",
    "\tp_tags = soup.find_all('p')\n",
    "\tdescriptions = [p_tag.get_text() for p_tag in p_tags]\n",
    "\tdescriptions = descriptions[2:] # drop first lines\n",
    "\t# print(descriptions)\n",
    "\n",
    "\t# remove from the list the metrics withoud description (not useful for our purpose)\n",
    "\tmetrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "\tmetrics.remove('Aye10032/loss_metric')\n",
    "\tmetrics.remove('giulio98/code_eval_outputs')\n",
    "\tmetrics.remove('maysonma/lingo_judge_metric')\n",
    "\tmetrics.remove('lvwerra/test')\n",
    "\tmetrics.remove('sma2023/wil')\n",
    "\n",
    "\t# From the lists, remove the descriptions and then the relative metric in the same index that have in the description 'TODO: add a description here\\n\\t\\t\\t\\t\\t\\t'ArithmeticError\n",
    "\n",
    "\tfor i, description in enumerate(descriptions):\n",
    "\t\tif 'TODO: add a description here' in description:\n",
    "\t\t\tmetrics.pop(i)\n",
    "\t\t\tdescriptions.pop(i)\n",
    "\t\n",
    "\treturn metrics, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json(metrics, descriptions):\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    metrics, descriptions = scrape_metrics()\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['trained'] = None\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "        metrics_data.append(metric_attributes)\n",
    "\n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Metric.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship():\n",
    "\n",
    "    file_path = os.path.join(result_path, 'train_relationship.json')\n",
    "\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        print(f'Processing {task} models...')\n",
    "        models = api.list_models(filter=task, full=True)\n",
    "        for model in models:\n",
    "            model_tags = model.tags\n",
    "            datasets = match_dataset(model_tags)\n",
    "            if len(datasets) != 0:\n",
    "                train_relationship = dict()\n",
    "                train_relationship[\"Models\"] = extract_name(model.id)\n",
    "                train_relationship[\"Datasets\"] = [extract_name(dataset) for dataset in datasets]\n",
    "                add_to_json_file(train_relationship, file_path)\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence-similarity models...\n",
      "Processing translation models...\n",
      "10000 models processed, 42.742920875549316 seconds elapsed\n",
      "Processing question-answering models...\n",
      "20000 models processed, 106.41267013549805 seconds elapsed\n",
      "Processing table-question-answering models...\n",
      "Processing summarization models...\n",
      "Processing fill-mask models...\n",
      "30000 models processed, 146.85001277923584 seconds elapsed\n",
      "Processing text-generation models...\n",
      "40000 models processed, 171.17433261871338 seconds elapsed\n",
      "50000 models processed, 195.7509036064148 seconds elapsed\n",
      "60000 models processed, 241.23480033874512 seconds elapsed\n",
      "70000 models processed, 274.88327288627625 seconds elapsed\n",
      "80000 models processed, 310.5311772823334 seconds elapsed\n",
      "90000 models processed, 351.8119206428528 seconds elapsed\n",
      "100000 models processed, 408.94479608535767 seconds elapsed\n",
      "110000 models processed, 434.0016312599182 seconds elapsed\n",
      "120000 models processed, 459.8774256706238 seconds elapsed\n",
      "130000 models processed, 491.9701542854309 seconds elapsed\n",
      "140000 models processed, 512.1452996730804 seconds elapsed\n",
      "150000 models processed, 529.8206927776337 seconds elapsed\n",
      "160000 models processed, 552.3346121311188 seconds elapsed\n",
      "170000 models processed, 582.0267584323883 seconds elapsed\n",
      "Processing text-classification models...\n",
      "180000 models processed, 618.140261888504 seconds elapsed\n",
      "190000 models processed, 669.2476892471313 seconds elapsed\n",
      "200000 models processed, 712.2804200649261 seconds elapsed\n",
      "210000 models processed, 746.5805397033691 seconds elapsed\n",
      "220000 models processed, 773.559821844101 seconds elapsed\n",
      "230000 models processed, 796.7391798496246 seconds elapsed\n",
      "240000 models processed, 813.4568645954132 seconds elapsed\n",
      "Processing zero-shot-classification models...\n",
      "Processing token-classification models...\n",
      "250000 models processed, 853.5023403167725 seconds elapsed\n",
      "260000 models processed, 889.4887013435364 seconds elapsed\n",
      "Processing feature-extraction models...\n",
      "270000 models processed, 919.088301897049 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "create_train_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship():\n",
    "\n",
    "    file_path = os.path.join(result_path, 'suited_for_relationship.json')\n",
    "\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    models = api.list_models(full=True)\n",
    "    for model in models:\n",
    "        model_tags = model.tags\n",
    "        tasks = []\n",
    "        for t in model_tags:\n",
    "            if t in TAG_DOWNSTREAM_TASK:\n",
    "                tasks.append(t)\n",
    "        if len(tasks) != 0:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model.id)\n",
    "            suited_for_relationship['DownstreamTask'] = tasks\n",
    "            add_to_json_file(suited_for_relationship, file_path)\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship():\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'enable_relationship.json')\n",
    "\n",
    "\tcount = 0\n",
    "\tstart_time = time.time()\n",
    "\tdatasets = api.list_datasets(full=True)\n",
    "\tfor dataset in datasets:\n",
    "\t\tdataset_tags = dataset.tags\n",
    "\t\ttasks = match_tasks(dataset_tags)\n",
    "\t\tif len(tasks) != 0:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset.id)\n",
    "\t\t\tenable_relationship['DownstreamTask'] = tasks\n",
    "\t\t\tadd_to_json_file(enable_relationship, file_path)\n",
    "\t\tcount += 1\n",
    "\t\tif count % 10000 == 0:\n",
    "\t\t\tprint(f'{count} datasets processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 datasets processed, 36.90543603897095 seconds elapsed\n",
      "20000 datasets processed, 61.39656043052673 seconds elapsed\n",
      "30000 datasets processed, 81.8622419834137 seconds elapsed\n",
      "40000 datasets processed, 99.32798981666565 seconds elapsed\n",
      "50000 datasets processed, 128.8480429649353 seconds elapsed\n",
      "60000 datasets processed, 182.50446486473083 seconds elapsed\n",
      "70000 datasets processed, 207.4542465209961 seconds elapsed\n",
      "80000 datasets processed, 229.8976695537567 seconds elapsed\n",
      "90000 datasets processed, 261.43047404289246 seconds elapsed\n",
      "100000 datasets processed, 299.85623478889465 seconds elapsed\n",
      "110000 datasets processed, 335.7912917137146 seconds elapsed\n",
      "120000 datasets processed, 372.2245075702667 seconds elapsed\n",
      "130000 datasets processed, 398.1815721988678 seconds elapsed\n",
      "140000 datasets processed, 418.6915295124054 seconds elapsed\n",
      "150000 datasets processed, 438.64215755462646 seconds elapsed\n",
      "160000 datasets processed, 462.48283767700195 seconds elapsed\n",
      "170000 datasets processed, 490.81813979148865 seconds elapsed\n",
      "180000 datasets processed, 513.5843617916107 seconds elapsed\n",
      "190000 datasets processed, 534.6843166351318 seconds elapsed\n",
      "200000 datasets processed, 553.8104147911072 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "create_enable_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: here https://huggingface.co/tasks some tasks have associated metrics, we could scrape the tasks one by one\n",
    "\n",
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\\n\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "\n",
    "    return assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.AssessRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def create_evaluate_relationship():\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'evaluate_relationship.json')\n",
    "\n",
    "\tcount = 0\n",
    "\tstart_time = time.time()\n",
    "\tfor task in TAG_DOWNSTREAM_TASK:\n",
    "\t\tprint(f'Processing {task} models...')\n",
    "\t\tmodels = api.list_models(filter=task, full=True, cardData=True)\n",
    "\t\tfor model in models:\n",
    "\t\t\tif model.card_data is not None:\n",
    "\t\t\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\t\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\t\tevaluate_relationship = dict()\n",
    "\t\t\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model.id)\n",
    "\t\t\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\t\t\tadd_to_json_file(evaluate_relationship, file_path)\n",
    "\t\tcount += 1\n",
    "\t\tif count % 10000 == 0:\n",
    "\t\t\tprint(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

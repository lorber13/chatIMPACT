{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frabet/miniconda3/envs/hf-api-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub.utils import logging\n",
    "\n",
    "from tags import * # tags.py\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape languages from HF\n",
    "\n",
    "url_languages = 'https://huggingface.co/languages'\n",
    "\n",
    "response = requests.get(url_languages)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "code_tags = soup.find_all('code')\n",
    "tag_language = [code_tag.get_text() for code_tag in code_tags]\n",
    "\n",
    "tag_language.remove('jax') # 'jax' is the ISO for Jambi Malay (present in 3 datasets, 36 models), impossible to distinguish from JAX the library... TODO: better solution?\n",
    "\n",
    "tag_language = set(tag_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching functions\n",
    "\n",
    "def extract_name(full_name):\n",
    "    pattern = re.compile(r'[^/]+/(.+)')\n",
    "    match = re.search(pattern, full_name)\n",
    "    if match:\n",
    "        return match.group(1) # the part after '/' might also contain version and number of parameters (impossible to extract in a uniform way)\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def match_string(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_all_matches(entries, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    matches = []\n",
    "    for entry in entries:\n",
    "        match = pattern.match(entry)\n",
    "        if match:\n",
    "            matches.append(match.group(1))\n",
    "    return matches\n",
    "\n",
    "def match_license(entries):\n",
    "    return match_string(entries, r'license:(\\S+)')\n",
    "\n",
    "def match_dataset(entries):\n",
    "    return find_all_matches(entries, r'dataset:(\\S+)')\n",
    "\n",
    "def match_uri(entries):\n",
    "    uri = match_string(entries, r'arxiv:(\\S+)') # TODO: use DOI instead of arXiv?\n",
    "    if uri is None:\n",
    "        uri = match_string(entries, r'doi:(\\S+)')\n",
    "    return uri\n",
    "\n",
    "def match_language(entries):\n",
    "    return find_all_matches(entries, r'language:(\\S+)')\n",
    "\n",
    "def match_size(entries):\n",
    "    return match_string(entries, r'size_categories:(\\S+)')\n",
    "\n",
    "def match_tasks(entries):\n",
    "    return find_all_matches(entries, r'task_categories:(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_json_file(data, file_path):\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            f.seek(f.tell() - 1, os.SEEK_SET)\n",
    "            f.truncate()\n",
    "            f.write(',\\n')\n",
    "            json.dump(data, f, indent=4)\n",
    "            f.write(']')\n",
    "    else:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([data], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "result_path = os.path.join(parent_path, 'database', 'HF entries', 'hf extracted json')\n",
    "os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all models\n",
    "\n",
    "# models = api.list_models(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 1000 models\n",
    "\n",
    "# model = itertools.islice(models, 0, 1000)\n",
    "# models_df = pd.DataFrame(model)\n",
    "# models_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_attributes(model):\n",
    "\n",
    "\tmodel_tags = model.tags\n",
    "\tif model.card_data is not None:\n",
    "\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\telse:\n",
    "\t\tmodel_card_data = None\n",
    "\tmodel_attributes = dict()\n",
    "\n",
    "\tmodel_attributes['name'] = extract_name(model.id)\n",
    "\tmodel_attributes['id'] = model.id\n",
    "\tmodel_attributes['version'] = None # sometimes in model['id'] but impossible to extract in a uniform way\n",
    "\tmodel_attributes['numberOfParameters'] = None # sometimes in model['id'] or model description but impossible to extract in a uniform way\n",
    "\n",
    "\tmodel_attributes['quantization'] = None\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_quantization:\n",
    "\t\t\tmodel_attributes['quantization'] = t\n",
    "\n",
    "\tmodel_attributes['architecture'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['architecture'] = model_card_data['base_model']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['languages'] = []\n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_language:\n",
    "\t\t\tmodel_attributes['languages'].append(t)\n",
    "\n",
    "\tmodel_attributes['modelCreator'] = None # extracted in a postprocessing step\n",
    "\n",
    "\tmodel_attributes['licenseToUse'] = match_license(model_tags)\n",
    "\n",
    "\tmodel_attributes['libraryFramework'] = [] \n",
    "\tfor t in model_tags:\n",
    "\t\tif t in tag_library:\n",
    "\t\t\tmodel_attributes['libraryFramework'].append(t)\n",
    "\n",
    "\tmodel_attributes['contextLength'] = None\n",
    "\tmodel_attributes['developers'] = [model.author]\n",
    "\tmodel_attributes['openSource'] = True\n",
    "\n",
    "\tmodel_attributes['uri'] = match_uri(model_tags)\n",
    "\n",
    "\tmodel_attributes['fineTuned'] = None # if there is a 'base_model' in card_data, it is fine-tuned\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tif 'base_model' in model_card_data:\n",
    "\t\t\t\tmodel_attributes['fineTuned'] = True\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['carbonEmission [CO2eq tons]'] = None\n",
    "\ttry:\n",
    "\t\tif model_card_data is not None:\n",
    "\t\t\tmodel_attributes['carbonEmission [CO2eq tons]'] = model_card_data['co2_eq_emissions']\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "\n",
    "\tmodel_attributes['tokenizer'] = None\n",
    "\n",
    "\treturn model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(result_path, 'models_data_no_modelCreator.json')\n",
    "\n",
    "# Total: 697,162 models\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} models...')\n",
    "    models = api.list_models(filter=task, full=True, cardData=True)\n",
    "    for model in models:\n",
    "        model_attributes = extract_model_attributes(model)\n",
    "        add_to_json_file(model_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data as a DataFrame\n",
    "\n",
    "file_path = os.path.join(result_path, 'models_data_no_modelCreator.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "models_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing: find the modelCreator\n",
    "\n",
    "df_filtered = models_df[models_df['architecture'].notna()]\n",
    "\n",
    "# Process each row\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for index, row in df_filtered.iterrows():\n",
    "    # Find the row where 'id' matches the 'architecture' of the current row\n",
    "    try:\n",
    "        matching_row = models_df[models_df['id'].astype(str) == str(row['architecture'])]\n",
    "    except ValueError:\n",
    "        break\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        # Get the first developer from the 'developers' list\n",
    "        first_developer = matching_row['developers'].iloc[0][0] if matching_row['developers'].iloc[0] else None\n",
    "        # Set the 'modelCreator' attribute of the original row\n",
    "        models_df.at[index, 'modelCreator'] = first_developer\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} rows processed ({count/len(df_filtered)*100} %), elapsed time: {time.time() - start_time} seconds, estimated time remaining: {(time.time() - start_time) / count * (len(df_filtered) - count)} seconds')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = models_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_postprocessed = os.path.join(result_path, 'models_data.json')\n",
    "\n",
    "# Remove 'id' field if it exists\n",
    "for model in models_list:\n",
    "    model.pop('id', None)\n",
    "\n",
    "with open(file_path_postprocessed, \"w\") as json_file:\n",
    "    json.dump(models_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all datasets\n",
    "\n",
    "# datasets = api.list_datasets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 1000 models\n",
    "\n",
    "# datasets = list(itertools.islice(datasets, 0, 1000))\n",
    "# datasets_df = pd.DataFrame(datasets)\n",
    "# datasets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_size_to_gb(file_size_str):\n",
    "    \"\"\"\n",
    "    Convert the file size string (e.g., '74.6 kB') to gigabytes (GB).\n",
    "    \"\"\"\n",
    "    file_size_parts = file_size_str.split()\n",
    "    file_size = float(file_size_parts[0])\n",
    "    unit = file_size_parts[1]\n",
    "\n",
    "    conversion_factors = {\n",
    "        'B': 1 / (1024 ** 3),\n",
    "        'kB': 1 / (1024 ** 2),\n",
    "        'MB': 1 / 1024,\n",
    "        'GB': 1,\n",
    "        'TB': 1024,\n",
    "    }\n",
    "\n",
    "    if unit in conversion_factors:\n",
    "        return float(file_size * conversion_factors[unit])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_file_size(url):\n",
    "    # Fetch the HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div containing the \"Size of downloaded dataset files:\" text\n",
    "    size_label_div = soup.find('div', string='Size of downloaded dataset files:')\n",
    "\n",
    "    if size_label_div:\n",
    "        # Find the next sibling div containing the file size\n",
    "        size_div = size_label_div.find_next('div')\n",
    "        if size_div:\n",
    "            # Extract the file size text\n",
    "            file_size = size_div.get_text(strip=True)\n",
    "            return file_size\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_attributes(dataset):\n",
    "\n",
    "\tdataset_tags = dataset.tags\n",
    "\tdataset_attributes = dict()\n",
    "\n",
    "\tdataset_attributes['name'] = extract_name(dataset.id)\n",
    "\tdataset_attributes['size [GB]'] = match_size(dataset_tags)\n",
    "\n",
    "\t# url = \"https://huggingface.co/datasets/\" + dataset.id\n",
    "\t# file_size_str = extract_file_size(url)\n",
    "\t# if file_size_str:\n",
    "\t# \tfile_size_gb = convert_file_size_to_gb(file_size_str)\n",
    "\t# \tif file_size_gb:\n",
    "\t# \t\tdataset_attributes['size [GB]'] = file_size_gb\n",
    "\n",
    "\tdataset_attributes['languages'] = match_language(dataset_tags)\n",
    "\n",
    "\t# dataset_attributes['dataset creator'] = dataset['author'] # TODO: add attribute in our model?\n",
    "\n",
    "\tdataset_attributes['licenseToUse'] = match_license(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['domain'] = []\n",
    "\tfor t in dataset_tags:\n",
    "\t\tif t in tag_domain:\n",
    "\t\t\tdataset_attributes['domain'].append(t)\n",
    "\n",
    "\tdataset_attributes['uri'] = match_uri(dataset_tags)\n",
    "\n",
    "\tdataset_attributes['fineTuning'] = None\n",
    "\n",
    "\treturn dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence-similarity datasets...\n",
      "Processing summarization datasets...\n",
      "Processing text-classification datasets...\n",
      "Processing question-answering datasets...\n",
      "Processing feature-extraction datasets...\n",
      "Processing zero-shot-classification datasets...\n",
      "Processing token-classification datasets...\n",
      "Processing text-generation datasets...\n",
      "Processing translation datasets...\n",
      "Processing fill-mask datasets...\n",
      "Processing table-question-answering datasets...\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(result_path, 'datasets_data.json')\n",
    "\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for task in TAG_DOWNSTREAM_TASK:\n",
    "    print(f'Processing {task} datasets...')\n",
    "    datasets = api.list_datasets(filter=task, full=True)\n",
    "    for dataset in datasets:\n",
    "        dataset_attributes = extract_datasets_attributes(dataset)\n",
    "        add_to_json_file(dataset_attributes, file_path)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f'{count} datasets processed, {time.time() - start_time} seconds elapsed, estimated time remaining: {(time.time() - start_time) / count * (199642 - count):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        target_paragraph = soup.find('p', class_='text-[1.2rem] text-gray-500')\n",
    "        \n",
    "        if target_paragraph:\n",
    "            return target_paragraph.get_text().strip()\n",
    "        else:\n",
    "            return \"Target paragraph not found.\"\n",
    "    else:\n",
    "        return f\"Failed to fetch the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "def create_tasks_json():\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    tasks_data = []\n",
    "\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        description = fetch_and_extract_text(url)\n",
    "        \n",
    "        tasks_data.append({\n",
    "            \"name\": task,\n",
    "            \"description\": description, # TODO: text2text generation has no description\n",
    "            \"sub-task\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed: {task}\")\n",
    "        # time.sleep(0.5)  # Be polite to the server\n",
    "\n",
    "    with open(result_path + '/ChatIMPACT.DownstreamTask.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tasks_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape metrics and descriptions from HF\n",
    "\n",
    "def scrape_metrics():\n",
    "\turl_metrics = 'https://huggingface.co/metrics'\n",
    "\n",
    "\tresponse = requests.get(url_metrics)\n",
    "\thtml_content = response.text\n",
    "\n",
    "\tsoup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\th4_tags = soup.find_all('h4')\n",
    "\tmetrics = [h4_tag.get_text(strip=True) for h4_tag in h4_tags]\n",
    "\t# print(metrics)\n",
    "\n",
    "\tp_tags = soup.find_all('p')\n",
    "\tdescriptions = [p_tag.get_text() for p_tag in p_tags]\n",
    "\tdescriptions = descriptions[2:] # drop first lines\n",
    "\t# print(descriptions)\n",
    "\n",
    "\t# remove from the list the metrics withoud description (not useful for our purpose)\n",
    "\tmetrics.remove('AlhitawiMohammed22/CER_Hu-Evaluation-Metrics')\n",
    "\tmetrics.remove('Aye10032/loss_metric')\n",
    "\tmetrics.remove('giulio98/code_eval_outputs')\n",
    "\tmetrics.remove('maysonma/lingo_judge_metric')\n",
    "\tmetrics.remove('lvwerra/test')\n",
    "\tmetrics.remove('sma2023/wil')\n",
    "\n",
    "\t# From the lists, remove the descriptions and then the relative metric in the same index that have in the description 'TODO: add a description here\\n\\t\\t\\t\\t\\t\\t'ArithmeticError\n",
    "\n",
    "\tfor i, description in enumerate(descriptions):\n",
    "\t\tif 'TODO: add a description here' in description:\n",
    "\t\t\tmetrics.pop(i)\n",
    "\t\t\tdescriptions.pop(i)\n",
    "\t\n",
    "\treturn metrics, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_json(metrics, descriptions):\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    current_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    result_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    metrics, descriptions = scrape_metrics()\n",
    "    \n",
    "    for idx in range(len(metrics)):\n",
    "        metric_attributes = dict()\n",
    "\n",
    "        metric_attributes['name'] = metrics[idx]\n",
    "        metric_attributes['description'] = descriptions[idx]\n",
    "        metric_attributes['trained'] = None\n",
    "        metric_attributes['context'] = None\n",
    "        metric_attributes['featureBased/endToEnd'] = None\n",
    "        metric_attributes['granularity'] = None\n",
    "\n",
    "        metrics_data.append(metric_attributes)\n",
    "\n",
    "    with open(os.path.join(result_path, 'ChatIMPACT.Metric.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_relationship():\n",
    "\n",
    "    file_path = os.path.join(result_path, 'train_relationship.json')\n",
    "\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        print(f'Processing {task} models...')\n",
    "        models = api.list_models(filter=task, full=True)\n",
    "        for model in models:\n",
    "            model_tags = model.tags\n",
    "            datasets = match_dataset(model_tags)\n",
    "            if len(datasets) != 0:\n",
    "                train_relationship = dict()\n",
    "                train_relationship[\"Models\"] = extract_name(model.id)\n",
    "                train_relationship[\"Datasets\"] = [extract_name(dataset) for dataset in datasets]\n",
    "                add_to_json_file(train_relationship, file_path)\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence-similarity models...\n",
      "Processing translation models...\n",
      "10000 models processed, 42.742920875549316 seconds elapsed\n",
      "Processing question-answering models...\n",
      "20000 models processed, 106.41267013549805 seconds elapsed\n",
      "Processing table-question-answering models...\n",
      "Processing summarization models...\n",
      "Processing fill-mask models...\n",
      "30000 models processed, 146.85001277923584 seconds elapsed\n",
      "Processing text-generation models...\n",
      "40000 models processed, 171.17433261871338 seconds elapsed\n",
      "50000 models processed, 195.7509036064148 seconds elapsed\n",
      "60000 models processed, 241.23480033874512 seconds elapsed\n",
      "70000 models processed, 274.88327288627625 seconds elapsed\n",
      "80000 models processed, 310.5311772823334 seconds elapsed\n",
      "90000 models processed, 351.8119206428528 seconds elapsed\n",
      "100000 models processed, 408.94479608535767 seconds elapsed\n",
      "110000 models processed, 434.0016312599182 seconds elapsed\n",
      "120000 models processed, 459.8774256706238 seconds elapsed\n",
      "130000 models processed, 491.9701542854309 seconds elapsed\n",
      "140000 models processed, 512.1452996730804 seconds elapsed\n",
      "150000 models processed, 529.8206927776337 seconds elapsed\n",
      "160000 models processed, 552.3346121311188 seconds elapsed\n",
      "170000 models processed, 582.0267584323883 seconds elapsed\n",
      "Processing text-classification models...\n",
      "180000 models processed, 618.140261888504 seconds elapsed\n",
      "190000 models processed, 669.2476892471313 seconds elapsed\n",
      "200000 models processed, 712.2804200649261 seconds elapsed\n",
      "210000 models processed, 746.5805397033691 seconds elapsed\n",
      "220000 models processed, 773.559821844101 seconds elapsed\n",
      "230000 models processed, 796.7391798496246 seconds elapsed\n",
      "240000 models processed, 813.4568645954132 seconds elapsed\n",
      "Processing zero-shot-classification models...\n",
      "Processing token-classification models...\n",
      "250000 models processed, 853.5023403167725 seconds elapsed\n",
      "260000 models processed, 889.4887013435364 seconds elapsed\n",
      "Processing feature-extraction models...\n",
      "270000 models processed, 919.088301897049 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "create_train_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuitedFor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_suited_for_relationship():\n",
    "\n",
    "    file_path = os.path.join(result_path, 'suited_for_relationship.json')\n",
    "\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    models = api.list_models(full=True)\n",
    "    for model in models:\n",
    "        model_tags = model.tags\n",
    "        tasks = []\n",
    "        for t in model_tags:\n",
    "            if t in TAG_DOWNSTREAM_TASK:\n",
    "                tasks.append(t)\n",
    "        if len(tasks) != 0:\n",
    "            suited_for_relationship = dict()\n",
    "            suited_for_relationship['LargeLanguageModel'] = extract_name(model.id)\n",
    "            suited_for_relationship['DownstreamTask'] = tasks\n",
    "            add_to_json_file(suited_for_relationship, file_path)\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_suited_for_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enable_relationship():\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'enable_relationship.json')\n",
    "\n",
    "\tcount = 0\n",
    "\tstart_time = time.time()\n",
    "\tdatasets = api.list_datasets(full=True)\n",
    "\tfor dataset in datasets:\n",
    "\t\tdataset_tags = dataset.tags\n",
    "\t\ttasks = match_tasks(dataset_tags)\n",
    "\t\tif len(tasks) != 0:\n",
    "\t\t\tenable_relationship = dict()\n",
    "\t\t\tenable_relationship['Dataset'] = extract_name(dataset.id)\n",
    "\t\t\tenable_relationship['DownstreamTask'] = tasks\n",
    "\t\t\tadd_to_json_file(enable_relationship, file_path)\n",
    "\t\tcount += 1\n",
    "\t\tif count % 10000 == 0:\n",
    "\t\t\tprint(f'{count} datasets processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 datasets processed, 36.90543603897095 seconds elapsed\n",
      "20000 datasets processed, 61.39656043052673 seconds elapsed\n",
      "30000 datasets processed, 81.8622419834137 seconds elapsed\n",
      "40000 datasets processed, 99.32798981666565 seconds elapsed\n",
      "50000 datasets processed, 128.8480429649353 seconds elapsed\n",
      "60000 datasets processed, 182.50446486473083 seconds elapsed\n",
      "70000 datasets processed, 207.4542465209961 seconds elapsed\n",
      "80000 datasets processed, 229.8976695537567 seconds elapsed\n",
      "90000 datasets processed, 261.43047404289246 seconds elapsed\n",
      "100000 datasets processed, 299.85623478889465 seconds elapsed\n",
      "110000 datasets processed, 335.7912917137146 seconds elapsed\n",
      "120000 datasets processed, 372.2245075702667 seconds elapsed\n",
      "130000 datasets processed, 398.1815721988678 seconds elapsed\n",
      "140000 datasets processed, 418.6915295124054 seconds elapsed\n",
      "150000 datasets processed, 438.64215755462646 seconds elapsed\n",
      "160000 datasets processed, 462.48283767700195 seconds elapsed\n",
      "170000 datasets processed, 490.81813979148865 seconds elapsed\n",
      "180000 datasets processed, 513.5843617916107 seconds elapsed\n",
      "190000 datasets processed, 534.6843166351318 seconds elapsed\n",
      "200000 datasets processed, 553.8104147911072 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "create_enable_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: here https://huggingface.co/tasks some tasks have associated metrics, we could scrape the tasks one by one\n",
    "\n",
    "def extract_assess_relationship():\n",
    "\n",
    "    assess = []\n",
    "    for task in TAG_DOWNSTREAM_TASK:\n",
    "        assess_element = {'Metric': [], 'DownstreamTask': task}\n",
    "        print(f\"Processing task: {task}\\n\")\n",
    "        url = f\"https://huggingface.co/tasks/{task}\"\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all the <dl> elements\n",
    "        dl_elements = soup.find_all('dl', class_='flex items-center rounded-lg border border-gray-100')\n",
    "\n",
    "        # Loop through each <dl> element\n",
    "        for dl in dl_elements:\n",
    "            # Extract the metric name from the <dt> tag inside the <summary>\n",
    "            metric_name = dl.find('dt').get_text(strip=True)\n",
    "\n",
    "            assess_element['Metric'].append(metric_name)\n",
    "\n",
    "        assess.append(assess_element)\n",
    "\n",
    "    return assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_asess_relationship_json():\n",
    "\t\n",
    "\tcurrent_path = os.getcwd()\n",
    "\tparent_path = os.path.dirname(current_path)\n",
    "\tresult_path = os.path.join(parent_path, 'database', 'hf extracted json')\n",
    "\n",
    "\tos.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "\tassess_relationship = extract_assess_relationship()\n",
    "\n",
    "\twith open(os.path.join(result_path, 'ChatIMPACT.AssessRelationship.json'), 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump(assess_relationship, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_asess_relationship_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check that this is correct (the output and the model cards on HF do not seem to be coherent?)\n",
    "# Model card template: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n",
    "\n",
    "def create_evaluate_relationship():\n",
    "\n",
    "\tfile_path = os.path.join(result_path, 'evaluate_relationship.json')\n",
    "\n",
    "\tcount = 0\n",
    "\tstart_time = time.time()\n",
    "\tfor task in TAG_DOWNSTREAM_TASK:\n",
    "\t\tprint(f'Processing {task} models...')\n",
    "\t\tmodels = api.list_models(filter=task, full=True, cardData=True)\n",
    "\t\tfor model in models:\n",
    "\t\t\tif model.card_data is not None:\n",
    "\t\t\t\tmodel_card_data = model.card_data.to_dict()\n",
    "\t\t\t\tif 'metrics' in model_card_data:\n",
    "\t\t\t\t\tmetrics = model_card_data['metrics']\n",
    "\t\t\t\t\tevaluate_relationship = dict()\n",
    "\t\t\t\t\tevaluate_relationship['LargeLanguageModel'] = extract_name(model.id)\n",
    "\t\t\t\t\tevaluate_relationship['Metric'] = metrics\n",
    "\t\t\t\t\tadd_to_json_file(evaluate_relationship, file_path)\n",
    "\t\tcount += 1\n",
    "\t\tif count % 10000 == 0:\n",
    "\t\t\tprint(f'{count} models processed, {time.time() - start_time} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_evaluate_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590aa"
  },
  "name": "PARENT",
  "description ": "Evaluates the quality of data-to-text generation by considering precision, recall, and n-gram overlap while also factoring in entity precision and recall.",
  "context": "Context-dependent",
  "featureBased/endToEnd": null,
  "granularity": "Sentence and entity level"
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a2"
  },
  "name": "BLEU",
  "description ": "Measures overlap between generated text and reference translations.",
  "context": "Context-free",
  "featureBased/endToEnd": null,
  "granularity": "N-gram level"
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a3"
  },
  "name": "ROUGE",
  "description ": "Evaluates text summarization by measuring overlap of n-grams.",
  "context": "Context-free",
  "featureBased/endToEnd": null,
  "granularity": "N-gram level"
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590ad"
  },
  "name": "Human Judgments",
  "description ": "Human evaluators assess text quality based on criteria like relevance.",
  "context": null,
  "featureBased/endToEnd": null,
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a4"
  },
  "name": "METEOR",
  "description ": "Considers synonyms and stemming for translation.",
  "context": "Context-free",
  "featureBased/endToEnd": null,
  "granularity": "N-gram level"
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a8"
  },
  "name": "SHAP",
  "description ": "Explains model predictions based on feature contributions.",
  "context": "Context-free",
  "featureBased/endToEnd": "Feature-based",
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590b1"
  },
  "name": "Winograd Schema Challenge",
  "description ": "Tests model's understanding of sentence disambiguation.",
  "context": null,
  "featureBased/endToEnd": null,
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590ab"
  },
  "name": "MQAG",
  "description ": "Uses multiple-choice QA and generation to assess information consistency.",
  "context": "Context-dependent",
  "featureBased/endToEnd": "End-to-end",
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a5"
  },
  "name": "Perplexity",
  "description ": "Measures how well a probability distribution predicts a sample.",
  "context": "Context-free",
  "featureBased/endToEnd": null,
  "granularity": "Word level"
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a6"
  },
  "name": "BERTScore",
  "description ": "Uses BERT embeddings to compare generated text to reference text.",
  "context": "Context-dependent",
  "featureBased/endToEnd": "Feature-based",
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590ac"
  },
  "name": "mFACT",
  "description ": "A metric to evaluate the faithfulness of summarisation models in multiple languages by detecting hallucinations",
  "context": "Context-dependent",
  "featureBased/endToEnd": "End-to-end",
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590af"
  },
  "name": "Turing Test",
  "description ": "Evaluates if the generated responses are indistinguishable from human ones.",
  "context": null,
  "featureBased/endToEnd": null,
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a9"
  },
  "name": "LIME",
  "description ": "Provides local explanations for model predictions.",
  "context": "Context-free",
  "featureBased/endToEnd": "Feature-based",
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590ae"
  },
  "name": "Pairwise Comparison",
  "description ": "Humans compare outputs from different models to determine superiority.",
  "context": null,
  "featureBased/endToEnd": null,
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590a7"
  },
  "name": "Groot",
  "description ": "Adversarial testing metric that uses tree-based semantic transformation to test the robustness and safety of text-to-image generative models by decomposing prompts into detailed semantic trees.",
  "context": "Context-dependent",
  "featureBased/endToEnd": null,
  "granularity": "High (tree-based decomposition)"
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590b0"
  },
  "name": "User Feedback and Interaction Logs",
  "description ": "Collects feedback from end-users to improve models.",
  "context": null,
  "featureBased/endToEnd": null,
  "granularity": null
},
{
  "_id": {
    "$oid": "66aa7abd2ee22bb2ec7590b2"
  },
  "name": "chrF",
  "description ": "ChrF and ChrF++ are two MT evaluation metrics that use the F-score statistic for character n-gram matches. ChrF++ additionally includes word n-grams, which correlate more strongly with direct assessment. We use the implementation that is already present in sacrebleu.\n\nWhile this metric is included in sacreBLEU, the implementation here is slightly different from sacreBLEU in terms of the required input format. Here, the length of the references and hypotheses lists need to be the same, so you may need to transpose your references compared to sacrebleuâ€™s required input format. See https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\n\nSee the sacreBLEU README.md for more information.",
  "context": null,
  "featureBased/endToEnd": null,
  "granularity": "character"
}]